{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 08. Natural Language Processing with TensorFlow  NLP\n",
    "\n",
    "文本--> 数字-> 模型-> 训练模型-> 寻找模式-> 使用模式（生成预测）\n",
    "\n",
    "![](https://raw.githubusercontent.com/mrdbourke/tensorflow-deep-learning/main/images/08-example-nlp-problems.png)\n",
    "*A handful of example natural language processing (NLP) and natural language understanding (NLU) problems. These are also often referred to as sequence problems (going from one sequence to another).*\n",
    "\n",
    "The main goal of [natural language processing (NLP)](https://becominghuman.ai/a-simple-introduction-to-natural-language-processing-ea66a1747b32) is to derive information from natural language.\n",
    "\n",
    "Natural language is a broad term but you can consider it to cover any of the following:\n",
    "* Text (such as that contained in an email, blog post, book, Tweet)\n",
    "* Speech (a conversation you have with a doctor, voice commands you give to a smart speaker)\n",
    "\n",
    "Under the umbrellas of text and speech there are many different things you might want to do.\n",
    "\n",
    "If you're building an email application, you might want to scan incoming emails to see if they're spam or not spam (classification).\n",
    "\n",
    "If you're trying to analyse customer feedback complaints, you might want to discover which section of your business they're for.\n",
    "\n",
    "> 🔑 **Note:** Both of these types of data are often referred to as *sequences* (a sentence is a sequence of words). So a common term you'll come across in NLP problems is called *seq2seq*, in other words, finding information in one sequence to produce another sequence (e.g. converting a speech command to a sequence of text-based steps).\n",
    "\n",
    "To get hands-on with NLP in TensorFlow, we're going to practice the steps we've used previously but this time with text data:\n",
    "\n",
    "```\n",
    "Text -> turn into numbers -> build a model -> train the model to find patterns -> use patterns (make predictions)\n",
    "```\n",
    "\n",
    "> 📖 **Resource:** For a great overview of NLP and the different problems within it, read the article [*A Simple Introduction to Natural Language Processing*](https://becominghuman.ai/a-simple-introduction-to-natural-language-processing-ea66a1747b32).\n",
    "\n",
    "## What we're going to cover\n",
    "\n",
    "Let's get specific hey?\n",
    "\n",
    "* Downloading a text dataset\n",
    "* Visualizing text data\n",
    "* Converting text into numbers using tokenization\n",
    "* Turning our tokenized text into an embedding\n",
    "* Modelling a text dataset\n",
    "  * Starting with a baseline (TF-IDF)\n",
    "  * Building several deep learning text models\n",
    "    * Dense, LSTM, GRU, Conv1D, Transfer learning\n",
    "* Comparing the performance of each our models\n",
    "* Combining our models into an ensemble\n",
    "* Saving and loading a trained model\n",
    "* Find the most wrong predictions\n",
    "\n",
    "## How you should approach this notebook\n",
    "\n",
    "You can read through the descriptions and the code (it should all run, except for the cells which error on purpose), but there's a better option.\n",
    "\n",
    "Write all of the code yourself.\n",
    "\n",
    "Yes. I'm serious. Create a new notebook, and rewrite each line by yourself. Investigate it, see if you can break it, why does it break?\n",
    "\n",
    "You don't have to write the text descriptions but writing the code yourself is a great way to get hands-on experience.\n",
    "\n",
    "Don't worry if you make mistakes, we all do. The way to get better and make less mistakes is to write more code.\n",
    "\n",
    "> 📖 **Resource:** See the full set of course materials on GitHub: https://github.com/mrdbourke/tensorflow-deep-learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Notebook last run (end-to-end): 2025-01-22 10:27:49.762145\n"
     ]
    }
   ],
   "source": [
    "import datetime\n",
    "print(f\"Notebook last run (end-to-end): {datetime.datetime.now()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check for GPU\n",
    "\n",
    "In order for our deep learning models to run as fast as possible, we'll need access to a GPU.\n",
    "\n",
    "In Google Colab, you can set this up by going to Runtime -> Change runtime type -> Hardware accelerator -> GPU.\n",
    "\n",
    "After selecting GPU, you may have to restart the runtime."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/bin/bash: nvidia-smi: command not found\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi -L"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get helper functions\n",
    "\n",
    "In past modules, we've created a bunch of helper functions to do small tasks required for our notebooks.\n",
    "\n",
    "Rather than rewrite all of these, we can import a script and load them in from there.\n",
    "\n",
    "The script containing our helper functions can be [found on GitHub](https://github.com/mrdbourke/tensorflow-deep-learning/blob/main/extras/helper_functions.py)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "if not os.path.exists('helper_functions.py'):\n",
    "    !wget https://raw.githubusercontent.com/mrdbourke/tensorflow-deep-learning/main/extras/helper_functions.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-01-22 10:27:49.931995: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F AVX512_VNNI AVX512_BF16 AVX_VNNI AMX_TILE AMX_INT8 AMX_BF16 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2025-01-22 10:27:50.041074: I tensorflow/core/util/util.cc:169] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2025-01-22 10:27:50.046171: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory\n",
      "2025-01-22 10:27:50.046185: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n",
      "2025-01-22 10:27:50.070649: E tensorflow/stream_executor/cuda/cuda_blas.cc:2981] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2025-01-22 10:27:50.587465: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory\n",
      "2025-01-22 10:27:50.587514: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory\n",
      "2025-01-22 10:27:50.587520: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n"
     ]
    }
   ],
   "source": [
    "from helper_functions import unzip_data, create_tensorboard_callback, plot_loss_curves, compare_historys"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Download a text dataset\n",
    "\n",
    "Let's start by download a text dataset. We'll be using the [Real or Not?](https://www.kaggle.com/c/nlp-getting-started/data) dataset from Kaggle which contains text-based Tweets about natural disasters. \n",
    "\n",
    "The Real Tweets are actually about disasters, for example:\n",
    "\n",
    "```\n",
    "Jetstar and Virgin forced to cancel Bali flights again because of ash from Mount Raung volcano\n",
    "```\n",
    "\n",
    "The Not Real Tweets are Tweets not about disasters (they can be on anything), for example:\n",
    "\n",
    "```\n",
    "'Education is the most powerful weapon which you can use to change the world.' Nelson #Mandela #quote\n",
    "```\n",
    "\n",
    "For convenience, the dataset has been [downloaded from Kaggle](https://www.kaggle.com/c/nlp-getting-started/data) (doing this requires a Kaggle account) and uploaded as a downloadable zip file. \n",
    "\n",
    "> 🔑 **Note:** The original downloaded data has not been altered to how you would download it from Kaggle."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.exists(\"nlp_getting_started.zip\"):\n",
    "    !wget \"https://storage.googleapis.com/ztm_tf_course/nlp_getting_started.zip\"\n",
    "\n",
    "# Unzip data\n",
    "if not os.path.exists(\"nlp_getting_started\"):\n",
    "    unzip_data(\"nlp_getting_started.zip\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Unzipping `nlp_getting_started.zip` gives the following 3 `.csv` files:\n",
    "\n",
    "三个csv文件，sample_submission.csv 是提交给Kaggle竞赛的样例文件，train.csv 是训练数据，test.csv 是测试数据。\n",
    "\n",
    "* `sample_submission.csv` - an example of the file you'd submit to the Kaggle competition of your model's predictions.\n",
    "* `train.csv` - training samples of real and not real diaster Tweets.\n",
    "* `test.csv` - testing samples of real and not real diaster Tweets."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualizing a text dataset  可视化数据\n",
    "\n",
    "Once you've acquired a new dataset to work with, what should you do first?\n",
    "\n",
    "Explore it? Inspect it? Verify it? Become one with it?\n",
    "\n",
    "All correct.\n",
    "\n",
    "Remember the motto: visualize, visualize, visualize.\n",
    "\n",
    "Right now, our text data samples are in the form of `.csv` files. For an easy way to make them visual, let's turn them into pandas DataFrame's.\n",
    "\n",
    "> 📖 **Reading:** You might come across text datasets in many different formats. Aside from CSV files (what we're working with), you'll probably encounter `.txt` files and `.json` files too. For working with these type of files, I'd recommend reading the two following articles by RealPython:\n",
    "* [How to Read and Write Files in Python](https://realpython.com/read-write-files-python/)\n",
    "* [Working with JSON Data in Python](https://realpython.com/python-json/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>keyword</th>\n",
       "      <th>location</th>\n",
       "      <th>text</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Our Deeds are the Reason of this #earthquake M...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Forest fire near La Ronge Sask. Canada</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>All residents asked to 'shelter in place' are ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>6</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>13,000 people receive #wildfires evacuation or...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>7</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Just got sent this photo from Ruby #Alaska as ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id keyword location                                               text  \\\n",
       "0   1     NaN      NaN  Our Deeds are the Reason of this #earthquake M...   \n",
       "1   4     NaN      NaN             Forest fire near La Ronge Sask. Canada   \n",
       "2   5     NaN      NaN  All residents asked to 'shelter in place' are ...   \n",
       "3   6     NaN      NaN  13,000 people receive #wildfires evacuation or...   \n",
       "4   7     NaN      NaN  Just got sent this photo from Ruby #Alaska as ...   \n",
       "\n",
       "   target  \n",
       "0       1  \n",
       "1       1  \n",
       "2       1  \n",
       "3       1  \n",
       "4       1  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "train_df = pd.read_csv(\"train.csv\")\n",
    "test_df = pd.read_csv(\"test.csv\")\n",
    "train_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>keyword</th>\n",
       "      <th>location</th>\n",
       "      <th>text</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2644</th>\n",
       "      <td>3796</td>\n",
       "      <td>destruction</td>\n",
       "      <td>NaN</td>\n",
       "      <td>So you have a new weapon that can cause un-ima...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2227</th>\n",
       "      <td>3185</td>\n",
       "      <td>deluge</td>\n",
       "      <td>NaN</td>\n",
       "      <td>The f$&amp;amp;@ing things I do for #GISHWHES Just...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5448</th>\n",
       "      <td>7769</td>\n",
       "      <td>police</td>\n",
       "      <td>UK</td>\n",
       "      <td>DT @georgegalloway: RT @Galloway4Mayor: ÛÏThe...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>132</th>\n",
       "      <td>191</td>\n",
       "      <td>aftershock</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Aftershock back to school kick off was great. ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6845</th>\n",
       "      <td>9810</td>\n",
       "      <td>trauma</td>\n",
       "      <td>Montgomery County, MD</td>\n",
       "      <td>in response to trauma Children of Addicts deve...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        id      keyword               location  \\\n",
       "2644  3796  destruction                    NaN   \n",
       "2227  3185       deluge                    NaN   \n",
       "5448  7769       police                     UK   \n",
       "132    191   aftershock                    NaN   \n",
       "6845  9810       trauma  Montgomery County, MD   \n",
       "\n",
       "                                                   text  target  \n",
       "2644  So you have a new weapon that can cause un-ima...       1  \n",
       "2227  The f$&amp;@ing things I do for #GISHWHES Just...       0  \n",
       "5448  DT @georgegalloway: RT @Galloway4Mayor: ÛÏThe...       1  \n",
       "132   Aftershock back to school kick off was great. ...       0  \n",
       "6845  in response to trauma Children of Addicts deve...       0  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# shuffle\n",
    "train_df_shuffled = train_df.sample(frac=1, random_state=42) # shuffle with random_state=42 for reproducibility\n",
    "train_df_shuffled.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice how the training data has a `\"target\"` column.\n",
    "\n",
    "We're going to be writing code to find patterns (e.g. different combinations of words) in the `\"text\"` column of the training dataset to predict the value of the `\"target\"` column.\n",
    "\n",
    "The test dataset doesn't have a `\"target\"` column.\n",
    "\n",
    "```\n",
    "Inputs (text column) -> Machine Learning Algorithm -> Outputs (target column)\n",
    "```\n",
    "\n",
    "![](https://raw.githubusercontent.com/mrdbourke/tensorflow-deep-learning/main/images/08-text-classification-inputs-and-outputs.png)\n",
    "*Example text classification inputs and outputs for the problem of classifying whether a Tweet is about a disaster or not.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>keyword</th>\n",
       "      <th>location</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Just happened a terrible car crash</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Heard about #earthquake is different cities, s...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>there is a forest fire at spot pond, geese are...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>9</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Apocalypse lighting. #Spokane #wildfires</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>11</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Typhoon Soudelor kills 28 in China and Taiwan</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id keyword location                                               text\n",
       "0   0     NaN      NaN                 Just happened a terrible car crash\n",
       "1   2     NaN      NaN  Heard about #earthquake is different cities, s...\n",
       "2   3     NaN      NaN  there is a forest fire at spot pond, geese are...\n",
       "3   9     NaN      NaN           Apocalypse lighting. #Spokane #wildfires\n",
       "4  11     NaN      NaN      Typhoon Soudelor kills 28 in China and Taiwan"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "target\n",
       "0    4342\n",
       "1    3271\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 类别分布\n",
    "train_df.target.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(7613, 3263, 10876)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_df),len(test_df),len(train_df)+len(test_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Alright, seems like we've got a decent amount of training and test data. If anything, we've got an abundance of testing examples, usually a split of 90/10 (90% training, 10% testing) or 80/20 is suffice.\n",
    "\n",
    "Okay, time to visualize, let's write some code to visualize random text samples.\n",
    "\n",
    "> 🤔 **Question:** Why visualize random samples? You could visualize samples in order but this could lead to only seeing a certain subset of data. Better to visualize a substantial quantity (100+) of random samples to get an idea of the different kinds of data you're working with. In machine learning, never underestimate the power of randomness.\n",
    "\n",
    "随机抽取100条数据，要多看随机数据的分布情况"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Target: 1 (real disaster)\n",
      "Text:\n",
      "Militants attack police post in Udhampur; 2 SPOs injured | LiveMint http://t.co/Rptouz2iJs | http://t.co/69mLhfefhr #AllTheNews\n",
      "\n",
      "------------------------------\n",
      "Target: 1 (real disaster)\n",
      "Text:\n",
      "BREAKING: Obama Officials GAVE Muslim Terrorist the Weapon Used in Texas Attack http://t.co/qi8QDw5dFG\n",
      "\n",
      "------------------------------\n",
      "Target: 1 (real disaster)\n",
      "Text:\n",
      "Delhi Government to Provide Free Treatment to Acid Attack Victims in Private Hospitals http://t.co/H6PM1W7elL\n",
      "\n",
      "------------------------------\n",
      "Target: 1 (real disaster)\n",
      "Text:\n",
      "New post from @darkreading http://t.co/8eIJDXApnp New SMB Relay Attack Steals User Credentials Over Internet\n",
      "\n",
      "------------------------------\n",
      "Target: 1 (real disaster)\n",
      "Text:\n",
      "Israeli forces raid home of alleged car attack suspect http://t.co/3GVUS8NPpy #palestine\n",
      "\n",
      "------------------------------\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "\n",
    "rand_num = random.randint(0, len(train_df)-5)\n",
    "for row in train_df.iloc[rand_num:rand_num+5].itertuples():\n",
    "    text = row.text\n",
    "    target = row.target\n",
    "    print(f\"Target: {target}\", \"(real disaster)\" if target > 0 else \"(not real disaster)\")\n",
    "    print(f\"Text:\\n{text}\\n\")\n",
    "    print(\"---\"*10)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split data into training and validation sets  划分训练集和验证集\n",
    "\n",
    "Since the test set has no labels and we need a way to evalaute our trained models, we'll split off some of the training data and create a validation set.\n",
    "\n",
    "When our model trains (tries patterns in the Tweet samples), it'll only see data from the training set and we can see how it performs on unseen data using the validation set.\n",
    "\n",
    "We'll convert our splits from pandas Series datatypes to lists of strings (for the text) and lists of ints (for the labels) for ease of use later.\n",
    "\n",
    "To split our training dataset and create a validation dataset, we'll use Scikit-Learn's [`train_test_split()`](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.train_test_split.html) method and dedicate 10% of the training samples to the validation set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "train_sentences, val_sentences, train_labels, val_labels = train_test_split(train_df_shuffled[\"text\"].to_numpy(),\n",
    "                                                                            train_df_shuffled[\"target\"].to_numpy(),\n",
    "                                                                            test_size=0.1, # dedicate 10% of samples to validation set\n",
    "                                                                            random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(6851, 6851, 762, 762)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_sentences),len(train_labels),len(val_sentences),len(val_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array(['@mogacola @zamtriossu i screamed after hitting tweet',\n",
       "        'Imagine getting flattened by Kurt Zouma',\n",
       "        '@Gurmeetramrahim #MSGDoing111WelfareWorks Green S welfare force ke appx 65000 members har time disaster victim ki help ke liye tyar hai....',\n",
       "        \"@shakjn @C7 @Magnums im shaking in fear he's gonna hack the planet\",\n",
       "        'Somehow find you and I collide http://t.co/Ee8RpOahPk',\n",
       "        '@EvaHanderek @MarleyKnysh great times until the bus driver held us hostage in the mall parking lot lmfao',\n",
       "        'destroy the free fandom honestly',\n",
       "        'Weapons stolen from National Guard Armory in New Albany still missing #Gunsense http://t.co/lKNU8902JE',\n",
       "        '@wfaaweather Pete when will the heat wave pass? Is it really going to be mid month? Frisco Boy Scouts have a canoe trip in Okla.',\n",
       "        'Patient-reported outcomes in long-term survivors of metastatic colorectal cancer - British Journal of Surgery http://t.co/5Yl4DC1Tqt'],\n",
       "       dtype=object),\n",
       " array([0, 0, 1, 0, 0, 1, 1, 0, 1, 1]))"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_sentences[:10],train_labels[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ## Converting text into numbers 文本数字化\n",
    "\n",
    "两种数字化操作:\n",
    "1、tokenization:单词级，字母级，subword级\n",
    "2、embedding: create embeddings，用训练好的模型的embedding层输出，更能表示单词之间的关系\n",
    "\n",
    "Wonderful! We've got a training set and a validation set containing Tweets and labels.\n",
    "\n",
    "Our labels are in numerical form (`0` and `1`) but our Tweets are in string form.\n",
    "\n",
    "> 🤔 **Question:** What do you think we have to do before we can use a machine learning algorithm with our text data? \n",
    "\n",
    "If you answered something along the lines of \"turn it into numbers\", you're correct. A machine learning algorithm requires its inputs to be in numerical form.\n",
    "\n",
    "In NLP, there are two main concepts for turning text into numbers:\n",
    "* **Tokenization** - A straight mapping from word or character or sub-word to a numerical value. There are three main levels of tokenization:\n",
    "  1. Using **word-level tokenization** with the sentence \"I love TensorFlow\" might result in \"I\" being `0`, \"love\" being `1` and \"TensorFlow\" being `2`. In this case, every word in a sequence considered a single **token**.\n",
    "  2. **Character-level tokenization**, such as converting the letters A-Z to values `1-26`. In this case, every character in a sequence considered a single **token**.\n",
    "  3. **Sub-word tokenization** is in between word-level and character-level tokenization. It involves breaking invidual words into smaller parts and then converting those smaller parts into numbers. For example, \"my favourite food is pineapple pizza\" might become \"my, fav, avour, rite, fo, oo, od, is, pin, ine, app, le, piz, za\". After doing this, these sub-words would then be mapped to a numerical value. In this case, every word could be considered multiple **tokens**.\n",
    "* **Embeddings** - An embedding is a representation of natural language which can be learned. Representation comes in the form of a **feature vector**. For example, the word \"dance\" could be represented by the 5-dimensional vector `[-0.8557, 0.5559, -0.3332, 0.9877, 0.1112]`. It's important to note here, the size of the feature vector is tuneable. There are two ways to use embeddings: \n",
    "  1. **Create your own embedding** - Once your text has been turned into numbers (required for an embedding), you can put them through an embedding layer (such as [`tf.keras.layers.Embedding`](https://www.tensorflow.org/api_docs/python/tf/keras/layers/Embedding)) and an embedding representation will be learned during model training.\n",
    "  2. **Reuse a pre-learned embedding** - Many pre-trained embeddings exist online. These pre-trained embeddings have often been learned on large corpuses of text (such as all of Wikipedia) and thus have a good underlying representation of natural language. You can use a pre-trained embedding to initialize your model and fine-tune it to your own specific task.\n",
    "\n",
    "![](https://raw.githubusercontent.com/mrdbourke/tensorflow-deep-learning/main/images/08-tokenization-vs-embedding.png)\n",
    "*Example of **tokenization** (straight mapping from word to number) and **embedding** (richer representation of relationships between tokens).*\n",
    "\n",
    "> 🤔 **Question:** What level of tokenzation should I use? What embedding should should I choose?\n",
    "\n",
    "It depends on your problem. You could try character-level tokenization/embeddings and word-level tokenization/embeddings and see which perform best. You might even want to try stacking them (e.g. combining the outputs of your embedding layers using [`tf.keras.layers.concatenate`](https://www.tensorflow.org/api_docs/python/tf/keras/layers/concatenate)). \n",
    "\n",
    "If you're looking for pre-trained word embeddings, [Word2vec embeddings](http://jalammar.github.io/illustrated-word2vec/), [GloVe embeddings](https://nlp.stanford.edu/projects/glove/) and many of the options available on [TensorFlow Hub](https://tfhub.dev/s?module-type=text-embedding) are great places to start.\n",
    "\n",
    "> 🔑 **Note:** Much like searching for a pre-trained computer vision model, you can search for pre-trained word embeddings to use for your problem. Try searching for something like \"use pre-trained word embeddings in TensorFlow\"."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Text vectorization (tokenization)  TextVectorization将单词token化\n",
    "\n",
    "Enough talking about tokenization and embeddings, let's create some.\n",
    "\n",
    "We'll practice tokenzation (mapping our words to numbers) first.\n",
    "\n",
    "To tokenize our words, we'll use the helpful preprocessing layer [`tf.keras.layers.experimental.preprocessing.TextVectorization`](https://www.tensorflow.org/api_docs/python/tf/keras/layers/experimental/preprocessing/TextVectorization).\n",
    "\n",
    "The `TextVectorization` layer takes the following parameters:\n",
    "* `max_tokens` - The maximum number of words in your vocabulary (e.g. 20000 or the number of unique words in your text), includes a value for OOV (out of vocabulary) tokens. \n",
    "* `standardize` - Method for standardizing text. Default is `\"lower_and_strip_punctuation\"` which lowers text and removes all punctuation marks.\n",
    "* `split` - How to split text, default is `\"whitespace\"` which splits on spaces.\n",
    "* `ngrams` - How many words to contain per token split, for example, `ngrams=2` splits tokens into continuous sequences of 2.\n",
    "* `output_mode` -  How to output tokens, can be `\"int\"` (integer mapping), `\"binary\"` (one-hot encoding), `\"count\"` or `\"tf-idf\"`. See documentation for more.\n",
    "* `output_sequence_length` - Length of tokenized sequence to output. For example, if `output_sequence_length=150`, all tokenized sequences will be 150 tokens long.\n",
    "* `pad_to_max_tokens` - Defaults to `False`, if `True`, the output feature axis will be padded to `max_tokens` even if the number of unique tokens in the vocabulary is less than `max_tokens`. Only valid in certain modes, see docs for more.\n",
    "\n",
    "Let's see it in action."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.10.0\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "print(tf.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import TextVectorization # after TensorFlow 2.6\n",
    "\n",
    "# Before TensorFlow 2.6\n",
    "# from tensorflow.keras.layers.experimental.preprocessing import TextVectorization \n",
    "# Note: in TensorFlow 2.6+, you no longer need \"layers.experimental.preprocessing\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-01-22 10:27:51.865988: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcuda.so.1'; dlerror: libcuda.so.1: cannot open shared object file: No such file or directory\n",
      "2025-01-22 10:27:51.866012: W tensorflow/stream_executor/cuda/cuda_driver.cc:263] failed call to cuInit: UNKNOWN ERROR (303)\n",
      "2025-01-22 10:27:51.866024: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:156] kernel driver does not appear to be running on this host (iv-yddi5wlhj4qc6il2vxqe): /proc/driver/nvidia/version does not exist\n",
      "2025-01-22 10:27:51.866237: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F AVX512_VNNI AVX512_BF16 AVX_VNNI AMX_TILE AMX_INT8 AMX_BF16 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "text_vectorizer = TextVectorization(max_tokens=None, # how many words in the vocabulary (all of the different words in your text)\n",
    "                                    standardize=\"lower_and_strip_punctuation\", # how to process text\n",
    "                                    split=\"whitespace\", # how to split tokens\n",
    "                                    ngrams=None, # create groups of n-words?\n",
    "                                    output_mode=\"int\", # how to map tokens to numbers\n",
    "                                    output_sequence_length=None) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We've initialized a `TextVectorization` object with the default settings but let's customize it a little bit for our own use case.\n",
    "\n",
    "In particular, let's set values for `max_tokens` and `output_sequence_length`.\n",
    "\n",
    "For `max_tokens` (the number of words in the vocabulary), multiples of 10,000 (`10,000`, `20,000`, `30,000`) or the exact number of unique words in your text (e.g. `32,179`) are common values.\n",
    "\n",
    "For our use case, we'll use `10,000`.\n",
    "\n",
    "And for the `output_sequence_length` we'll use the average number of tokens per Tweet in the training set. But first, we'll need to find it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "14.901036345059115"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum([len(i.split()) for i in train_sentences])/len(train_sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_vocab_length = 10000 \n",
    "max_length = 15\n",
    "\n",
    "text_vectorizer = TextVectorization(max_tokens=max_vocab_length,\n",
    "                                    output_mode=\"int\",\n",
    "                                    output_sequence_length=max_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "numpy.ndarray"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(train_sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fit train data\n",
    "text_vectorizer.adapt(train_sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(1, 15), dtype=int64, numpy=\n",
       "array([[264,   3, 232,   4,  13, 698,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0]])>"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample_sentence = \"There's a flood in my street!\"\n",
    "text_vectorizer([sample_sentence])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original text:\n",
      "late night mcdonalds with friends = hilarious although my car is wrecked and there's half a steak pastie in the industrial estate      \n",
      "\n",
      "Vectorized version:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(1, 15), dtype=int64, numpy=\n",
       "array([[1394,  285, 3647,   14,  819, 1519, 4148,   13,  127,    9,  337,\n",
       "           7,  264,  575,    3]])>"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "random_sentence = random.choice(train_sentences)\n",
    "print(f\"Original text:\\n{random_sentence}\\\n",
    "      \\n\\nVectorized version:\")\n",
    "text_vectorizer([random_sentence])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of words in vocab: 10000\n",
      "Top 5 most common words: ['', '[UNK]', 'the', 'a', 'in']\n",
      "Bottom 5 least common words: ['pages', 'paeds', 'pads', 'padres', 'paddytomlinson1']\n"
     ]
    }
   ],
   "source": [
    "words_in_vocab = text_vectorizer.get_vocabulary()\n",
    "top5_words = words_in_vocab[:5]\n",
    "bottom_words = words_in_vocab[-5:]\n",
    "print(f\"Number of words in vocab: {len(words_in_vocab)}\")\n",
    "print(f\"Top 5 most common words: {top5_words}\")\n",
    "print(f\"Bottom 5 least common words: {bottom_words}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating an Embedding using an Embedding Layer  tf.keras.layers.Embedding 实现embedding\n",
    "\n",
    "We've got a way to map our text to numbers. How about we go a step further and turn those numbers into an embedding?\n",
    "\n",
    "The powerful thing about an embedding is it can be learned during training. This means rather than just being static (e.g. `1` = I, `2` = love, `3` = TensorFlow), a word's numeric representation can be improved as a model goes through data samples.\n",
    "\n",
    "We can see what an embedding of a word looks like by using the [`tf.keras.layers.Embedding`](https://www.tensorflow.org/api_docs/python/tf/keras/layers/Embedding) layer. \n",
    "\n",
    "参数：input_dim 表示词汇表大小，output_dim表示输出的embedding向量维度，embeddings_initializer表示embedding矩阵的初始化方式，input_length表示输入的序列长度\n",
    "\n",
    "The main parameters we're concerned about here are:\n",
    "* `input_dim` - The size of the vocabulary (e.g. `len(text_vectorizer.get_vocabulary()`).\n",
    "* `output_dim` - The size of the output embedding vector, for example, a value of `100` outputs a  feature vector of size 100 for each word.\n",
    "* `embeddings_initializer` - How to initialize the embeddings matrix, default is `\"uniform\"` which randomly initalizes embedding matrix with uniform distribution. This can be changed for using pre-learned embeddings.\n",
    "* `input_length` - Length of sequences being passed to embedding layer.\n",
    "\n",
    "Knowing these, let's make an embedding layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<keras.layers.core.embedding.Embedding at 0x7fd7f92e7850>"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.random.set_seed(42)\n",
    "from tensorflow.keras import layers\n",
    "\n",
    "embedding = layers.Embedding(input_dim=max_vocab_length, \n",
    "                             output_dim=64,\n",
    "                             embeddings_initializer=\"uniform\",\n",
    "                             input_length=max_length,\n",
    "                             name=\"embedding_1\")\n",
    "embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original text:\n",
      "7:13pm #MAPLERIDGE Lougheed Hwy EB is closed between 203rd and Dewdney Trunk Rd because of Collision. ETO is between 8:00 PM and 9:00 PM.      \n",
      "\n",
      "Embedded version:\n",
      "random_sentence_int:[[   1    1    1 1838 3901    9  959 1323    1    7    1 4370  765  152\n",
      "     6]]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(1, 15, 64), dtype=float32, numpy=\n",
       "array([[[-1.81695819e-02,  2.90265717e-02,  3.18275727e-02,\n",
       "          3.36486138e-02,  3.00082900e-02, -7.18151405e-03,\n",
       "          4.67654318e-03,  1.02014430e-02, -7.36920908e-03,\n",
       "         -2.08133347e-02, -4.35255878e-02,  4.69939224e-02,\n",
       "          3.21924426e-02, -1.35521889e-02, -3.21945176e-02,\n",
       "         -6.20983541e-04, -1.43146627e-02, -1.57124884e-02,\n",
       "         -3.09010874e-02, -4.87412326e-02, -6.08097389e-03,\n",
       "          4.18156870e-02,  2.72732414e-02,  4.39486988e-02,\n",
       "          2.03671940e-02,  6.57899305e-03, -3.52343917e-02,\n",
       "         -2.96069868e-02,  2.34069116e-02,  2.90972851e-02,\n",
       "          2.38841809e-02, -3.41748372e-02,  7.89452717e-03,\n",
       "          9.39922407e-03,  3.47914211e-02,  3.39226760e-02,\n",
       "         -1.27207115e-03, -8.47246498e-03, -4.10997756e-02,\n",
       "          4.99075912e-02, -3.52857336e-02,  2.74533033e-03,\n",
       "          1.55851282e-02, -3.94850150e-02, -3.44061852e-02,\n",
       "         -3.77968773e-02,  3.42959426e-02,  4.46748473e-02,\n",
       "          2.49046125e-02,  2.41302885e-02,  2.72128098e-02,\n",
       "          2.39529051e-02, -4.98504415e-02,  1.37590431e-02,\n",
       "         -4.78975661e-02,  1.24825016e-02,  2.57855095e-02,\n",
       "         -2.02386864e-02,  3.72343995e-02, -1.60302408e-02,\n",
       "          4.15903665e-02, -2.24876050e-02, -2.45540980e-02,\n",
       "          5.78309223e-03],\n",
       "        [-1.81695819e-02,  2.90265717e-02,  3.18275727e-02,\n",
       "          3.36486138e-02,  3.00082900e-02, -7.18151405e-03,\n",
       "          4.67654318e-03,  1.02014430e-02, -7.36920908e-03,\n",
       "         -2.08133347e-02, -4.35255878e-02,  4.69939224e-02,\n",
       "          3.21924426e-02, -1.35521889e-02, -3.21945176e-02,\n",
       "         -6.20983541e-04, -1.43146627e-02, -1.57124884e-02,\n",
       "         -3.09010874e-02, -4.87412326e-02, -6.08097389e-03,\n",
       "          4.18156870e-02,  2.72732414e-02,  4.39486988e-02,\n",
       "          2.03671940e-02,  6.57899305e-03, -3.52343917e-02,\n",
       "         -2.96069868e-02,  2.34069116e-02,  2.90972851e-02,\n",
       "          2.38841809e-02, -3.41748372e-02,  7.89452717e-03,\n",
       "          9.39922407e-03,  3.47914211e-02,  3.39226760e-02,\n",
       "         -1.27207115e-03, -8.47246498e-03, -4.10997756e-02,\n",
       "          4.99075912e-02, -3.52857336e-02,  2.74533033e-03,\n",
       "          1.55851282e-02, -3.94850150e-02, -3.44061852e-02,\n",
       "         -3.77968773e-02,  3.42959426e-02,  4.46748473e-02,\n",
       "          2.49046125e-02,  2.41302885e-02,  2.72128098e-02,\n",
       "          2.39529051e-02, -4.98504415e-02,  1.37590431e-02,\n",
       "         -4.78975661e-02,  1.24825016e-02,  2.57855095e-02,\n",
       "         -2.02386864e-02,  3.72343995e-02, -1.60302408e-02,\n",
       "          4.15903665e-02, -2.24876050e-02, -2.45540980e-02,\n",
       "          5.78309223e-03],\n",
       "        [-1.81695819e-02,  2.90265717e-02,  3.18275727e-02,\n",
       "          3.36486138e-02,  3.00082900e-02, -7.18151405e-03,\n",
       "          4.67654318e-03,  1.02014430e-02, -7.36920908e-03,\n",
       "         -2.08133347e-02, -4.35255878e-02,  4.69939224e-02,\n",
       "          3.21924426e-02, -1.35521889e-02, -3.21945176e-02,\n",
       "         -6.20983541e-04, -1.43146627e-02, -1.57124884e-02,\n",
       "         -3.09010874e-02, -4.87412326e-02, -6.08097389e-03,\n",
       "          4.18156870e-02,  2.72732414e-02,  4.39486988e-02,\n",
       "          2.03671940e-02,  6.57899305e-03, -3.52343917e-02,\n",
       "         -2.96069868e-02,  2.34069116e-02,  2.90972851e-02,\n",
       "          2.38841809e-02, -3.41748372e-02,  7.89452717e-03,\n",
       "          9.39922407e-03,  3.47914211e-02,  3.39226760e-02,\n",
       "         -1.27207115e-03, -8.47246498e-03, -4.10997756e-02,\n",
       "          4.99075912e-02, -3.52857336e-02,  2.74533033e-03,\n",
       "          1.55851282e-02, -3.94850150e-02, -3.44061852e-02,\n",
       "         -3.77968773e-02,  3.42959426e-02,  4.46748473e-02,\n",
       "          2.49046125e-02,  2.41302885e-02,  2.72128098e-02,\n",
       "          2.39529051e-02, -4.98504415e-02,  1.37590431e-02,\n",
       "         -4.78975661e-02,  1.24825016e-02,  2.57855095e-02,\n",
       "         -2.02386864e-02,  3.72343995e-02, -1.60302408e-02,\n",
       "          4.15903665e-02, -2.24876050e-02, -2.45540980e-02,\n",
       "          5.78309223e-03],\n",
       "        [-1.27219334e-02,  4.76786755e-02, -7.39868730e-03,\n",
       "         -4.12483141e-03,  2.10416205e-02,  3.79959010e-02,\n",
       "         -4.01530415e-03, -1.72975063e-02, -4.89504002e-02,\n",
       "         -2.60671377e-02,  4.34484370e-02, -1.84936039e-02,\n",
       "          4.41238396e-02, -2.99458504e-02,  1.61089636e-02,\n",
       "         -2.92678233e-02, -4.00737636e-02, -1.90549251e-02,\n",
       "          6.94910437e-03, -2.35139206e-03,  3.47995646e-02,\n",
       "         -5.37178665e-03,  4.14893888e-02, -2.22612154e-02,\n",
       "         -4.82838154e-02, -1.08099580e-02, -3.04337498e-02,\n",
       "          2.30618156e-02,  3.26415151e-03,  1.84226781e-04,\n",
       "          4.59740050e-02,  1.21483803e-02,  3.77246030e-02,\n",
       "          2.01018937e-02, -4.72465903e-03,  3.67205180e-02,\n",
       "         -1.62144303e-02, -1.48200169e-02,  4.76061217e-02,\n",
       "          8.90276581e-03,  3.46734188e-02,  1.93553232e-02,\n",
       "         -2.16238983e-02, -3.43797207e-02, -4.39264178e-02,\n",
       "          3.05004232e-02,  1.66672505e-02, -3.26167718e-02,\n",
       "          4.76879366e-02, -1.48330443e-02, -1.98066365e-02,\n",
       "          2.90586241e-02, -1.21268854e-02, -2.79747136e-02,\n",
       "          2.26809271e-02,  1.85524337e-02,  1.85492747e-02,\n",
       "          4.16199602e-02,  3.88752706e-02,  4.20485847e-02,\n",
       "          1.97628178e-02,  3.87320668e-03, -8.70350748e-03,\n",
       "         -1.96591616e-02],\n",
       "        [ 4.03330587e-02, -3.57928984e-02,  3.33852805e-02,\n",
       "          2.86257006e-02, -4.45360541e-02,  2.12770440e-02,\n",
       "         -8.57404619e-03,  2.17841156e-02,  7.21035153e-03,\n",
       "         -2.22922210e-02, -3.57057676e-02,  2.03082301e-02,\n",
       "          3.02762501e-02, -3.13396603e-02, -3.37459929e-02,\n",
       "         -3.64044905e-02,  2.74479873e-02, -5.18046692e-03,\n",
       "          2.12201215e-02,  1.94295086e-02,  2.99878754e-02,\n",
       "          1.69464387e-02, -2.27155220e-02,  2.87392400e-02,\n",
       "         -3.94330174e-03, -3.09806354e-02,  3.98783945e-02,\n",
       "          4.08111326e-02,  9.64989513e-03,  3.32839377e-02,\n",
       "         -1.87542439e-02,  2.28517912e-02, -1.96254253e-02,\n",
       "         -4.38441895e-02, -2.69210227e-02, -2.76154410e-02,\n",
       "         -3.35542187e-02,  2.89892294e-02, -2.13042032e-02,\n",
       "          3.87993716e-02, -4.66827303e-03,  2.25975253e-02,\n",
       "          2.92549618e-02, -5.51016256e-03, -3.30255516e-02,\n",
       "         -2.28459239e-02,  2.14668773e-02, -4.24409620e-02,\n",
       "          1.75014846e-02,  2.02327967e-03,  1.38374902e-02,\n",
       "          3.14484350e-02,  4.08402830e-03, -1.66845322e-02,\n",
       "          4.00742926e-02,  1.42529346e-02, -3.01859528e-03,\n",
       "         -4.83420864e-02,  1.19106993e-02,  3.16834450e-03,\n",
       "         -1.79358833e-02, -2.84773242e-02,  1.79666542e-02,\n",
       "          3.46642621e-02],\n",
       "        [ 2.35886499e-03, -2.36707814e-02,  2.52685584e-02,\n",
       "          1.93221606e-02,  3.09900902e-02, -2.94955261e-02,\n",
       "          2.00555436e-02,  3.03981937e-02, -2.33243592e-02,\n",
       "          2.15903036e-02, -3.90701517e-02,  3.43395807e-02,\n",
       "         -2.28090640e-02, -1.01606958e-02,  4.21810485e-02,\n",
       "          1.32714771e-02, -3.45820189e-03, -4.04623859e-02,\n",
       "         -4.94112261e-02,  3.72485034e-02, -2.31830832e-02,\n",
       "         -1.35534778e-02,  1.78367235e-02, -3.51309776e-03,\n",
       "         -2.64346842e-02, -4.96296063e-02, -4.69856746e-02,\n",
       "          3.15421484e-02,  2.37844624e-02, -3.16051729e-02,\n",
       "          1.24171264e-02, -3.33265662e-02, -4.79251146e-03,\n",
       "         -1.33520365e-03, -3.16916332e-02,  1.05860718e-02,\n",
       "          4.84547280e-02, -5.47078997e-03, -2.56479736e-02,\n",
       "          3.46331261e-02, -2.74325367e-02,  2.23335959e-02,\n",
       "          2.52532400e-02,  3.65914218e-02,  1.57447942e-02,\n",
       "         -4.07491699e-02, -3.80797014e-02, -1.02090947e-02,\n",
       "          1.44611113e-02, -4.77897786e-02, -9.95621085e-05,\n",
       "          2.38388665e-02,  8.79057497e-03,  3.13545205e-02,\n",
       "          3.07085998e-02,  4.68364023e-02,  6.57379627e-03,\n",
       "          3.82763855e-02,  2.44307183e-02,  3.18163745e-02,\n",
       "         -5.93159348e-03,  2.78256088e-03, -2.30755694e-02,\n",
       "          1.67509429e-02],\n",
       "        [-4.69865687e-02,  4.93213646e-02, -3.64674814e-02,\n",
       "          3.25144455e-03,  1.59580149e-02, -2.32012272e-02,\n",
       "         -4.67723124e-02, -3.40081379e-03,  4.68701459e-02,\n",
       "          1.68024935e-02,  2.26354636e-02, -2.23070737e-02,\n",
       "          2.34630220e-02, -3.14607844e-02, -4.70297113e-02,\n",
       "          1.67914294e-02,  3.27306651e-02,  1.12365931e-04,\n",
       "          3.62307914e-02, -1.01500042e-02, -4.81616259e-02,\n",
       "          5.65255806e-03,  1.96123384e-02,  6.11200184e-03,\n",
       "          8.43901560e-03, -2.53363978e-02,  4.62127589e-02,\n",
       "         -4.66657989e-02,  3.62817198e-03,  2.26708688e-02,\n",
       "         -1.04403272e-02, -4.02784124e-02, -1.87303796e-02,\n",
       "         -2.39888672e-02, -3.95687595e-02, -4.78903912e-02,\n",
       "         -2.00436246e-02, -4.11750302e-02,  4.13306616e-02,\n",
       "         -1.31004825e-02,  3.79210599e-02,  4.81367446e-02,\n",
       "         -2.31518988e-02, -2.58208867e-02,  4.83891480e-02,\n",
       "         -2.87414789e-02,  2.04388537e-02, -3.29865962e-02,\n",
       "          4.48358990e-02, -1.51357874e-02,  9.24358517e-03,\n",
       "         -2.78012510e-02, -1.34336948e-03,  4.82369401e-02,\n",
       "          9.22482088e-03,  3.52810510e-02,  5.66259772e-03,\n",
       "          1.95489265e-02, -3.58964130e-03,  4.21543159e-02,\n",
       "          4.76954021e-02, -4.67407703e-02, -3.53630185e-02,\n",
       "          4.54517268e-02],\n",
       "        [-1.36987343e-02,  4.84783165e-02, -9.16049629e-03,\n",
       "          2.86487080e-02,  2.77155377e-02,  3.53272893e-02,\n",
       "          1.30721666e-02, -7.09474087e-03,  1.24749653e-02,\n",
       "         -3.03174984e-02, -2.94615868e-02,  2.14405991e-02,\n",
       "         -4.46850918e-02, -3.53523381e-02, -4.78528999e-02,\n",
       "         -3.22921872e-02,  8.14714283e-03,  4.71958555e-02,\n",
       "         -4.75108027e-02,  1.47733353e-02,  3.71035822e-02,\n",
       "         -3.83772738e-02, -1.81963071e-02, -1.42182037e-03,\n",
       "          4.96901311e-02, -4.32365201e-02, -1.17771998e-02,\n",
       "         -1.16034038e-02,  3.39472927e-02,  4.80570905e-02,\n",
       "          1.85149945e-02,  3.94718386e-02, -2.49238014e-02,\n",
       "         -7.45109469e-03,  2.56392695e-02, -2.74209864e-02,\n",
       "          1.41446628e-02, -3.76462936e-02, -2.87134182e-02,\n",
       "         -2.96638254e-02, -8.61965492e-03, -4.68796492e-02,\n",
       "          3.69699709e-02, -4.81987260e-02,  1.55248530e-02,\n",
       "          4.41384651e-02, -1.79240853e-03, -8.84951279e-03,\n",
       "         -7.88145140e-03, -4.95217219e-02,  4.46128845e-03,\n",
       "          3.13153006e-02,  4.64888923e-02,  4.61352579e-02,\n",
       "          3.06870677e-02, -8.46797228e-03,  4.76454981e-02,\n",
       "         -4.62389588e-02,  1.60988010e-02,  1.30782016e-02,\n",
       "          3.75157259e-02,  2.10965313e-02,  7.15054572e-04,\n",
       "         -1.14829317e-02],\n",
       "        [-1.81695819e-02,  2.90265717e-02,  3.18275727e-02,\n",
       "          3.36486138e-02,  3.00082900e-02, -7.18151405e-03,\n",
       "          4.67654318e-03,  1.02014430e-02, -7.36920908e-03,\n",
       "         -2.08133347e-02, -4.35255878e-02,  4.69939224e-02,\n",
       "          3.21924426e-02, -1.35521889e-02, -3.21945176e-02,\n",
       "         -6.20983541e-04, -1.43146627e-02, -1.57124884e-02,\n",
       "         -3.09010874e-02, -4.87412326e-02, -6.08097389e-03,\n",
       "          4.18156870e-02,  2.72732414e-02,  4.39486988e-02,\n",
       "          2.03671940e-02,  6.57899305e-03, -3.52343917e-02,\n",
       "         -2.96069868e-02,  2.34069116e-02,  2.90972851e-02,\n",
       "          2.38841809e-02, -3.41748372e-02,  7.89452717e-03,\n",
       "          9.39922407e-03,  3.47914211e-02,  3.39226760e-02,\n",
       "         -1.27207115e-03, -8.47246498e-03, -4.10997756e-02,\n",
       "          4.99075912e-02, -3.52857336e-02,  2.74533033e-03,\n",
       "          1.55851282e-02, -3.94850150e-02, -3.44061852e-02,\n",
       "         -3.77968773e-02,  3.42959426e-02,  4.46748473e-02,\n",
       "          2.49046125e-02,  2.41302885e-02,  2.72128098e-02,\n",
       "          2.39529051e-02, -4.98504415e-02,  1.37590431e-02,\n",
       "         -4.78975661e-02,  1.24825016e-02,  2.57855095e-02,\n",
       "         -2.02386864e-02,  3.72343995e-02, -1.60302408e-02,\n",
       "          4.15903665e-02, -2.24876050e-02, -2.45540980e-02,\n",
       "          5.78309223e-03],\n",
       "        [-2.80240532e-02, -1.27814636e-02,  4.53674793e-03,\n",
       "          3.38082574e-02, -3.63766328e-02, -3.23687568e-02,\n",
       "          1.12727508e-02,  1.14207268e-02, -2.00343374e-02,\n",
       "          2.28126310e-02, -1.42692812e-02, -3.67569104e-02,\n",
       "         -3.10529396e-03,  3.33414562e-02,  3.22444327e-02,\n",
       "          2.82935239e-02, -3.22223678e-02, -2.24311240e-02,\n",
       "         -3.54218371e-02,  5.34292310e-03,  3.71543318e-03,\n",
       "         -5.42066246e-03, -1.82333812e-02, -1.83935277e-02,\n",
       "         -1.63962953e-02,  3.14859487e-02,  3.76975425e-02,\n",
       "          3.58963273e-02,  4.89501394e-02, -2.85201073e-02,\n",
       "         -2.55905986e-02, -2.02060863e-03, -1.09726787e-02,\n",
       "          2.35958956e-02,  2.43705399e-02,  3.10335271e-02,\n",
       "          2.95119360e-03, -3.33034620e-02,  2.61515379e-03,\n",
       "         -3.36156115e-02, -2.57214904e-02,  4.84541543e-02,\n",
       "         -2.45058183e-02, -2.29111668e-02, -2.08878871e-02,\n",
       "          4.34135906e-02,  1.01911910e-02, -1.27412379e-04,\n",
       "          1.62071846e-02, -4.38181274e-02,  4.34393920e-02,\n",
       "         -3.52947339e-02, -2.61463653e-02,  1.27961151e-02,\n",
       "          5.56430966e-03,  2.11235769e-02, -4.68645096e-02,\n",
       "         -1.48621090e-02,  2.87401676e-03, -4.89517115e-02,\n",
       "          3.46837305e-02,  4.02936824e-02,  4.94448878e-02,\n",
       "          1.95443369e-02],\n",
       "        [-1.81695819e-02,  2.90265717e-02,  3.18275727e-02,\n",
       "          3.36486138e-02,  3.00082900e-02, -7.18151405e-03,\n",
       "          4.67654318e-03,  1.02014430e-02, -7.36920908e-03,\n",
       "         -2.08133347e-02, -4.35255878e-02,  4.69939224e-02,\n",
       "          3.21924426e-02, -1.35521889e-02, -3.21945176e-02,\n",
       "         -6.20983541e-04, -1.43146627e-02, -1.57124884e-02,\n",
       "         -3.09010874e-02, -4.87412326e-02, -6.08097389e-03,\n",
       "          4.18156870e-02,  2.72732414e-02,  4.39486988e-02,\n",
       "          2.03671940e-02,  6.57899305e-03, -3.52343917e-02,\n",
       "         -2.96069868e-02,  2.34069116e-02,  2.90972851e-02,\n",
       "          2.38841809e-02, -3.41748372e-02,  7.89452717e-03,\n",
       "          9.39922407e-03,  3.47914211e-02,  3.39226760e-02,\n",
       "         -1.27207115e-03, -8.47246498e-03, -4.10997756e-02,\n",
       "          4.99075912e-02, -3.52857336e-02,  2.74533033e-03,\n",
       "          1.55851282e-02, -3.94850150e-02, -3.44061852e-02,\n",
       "         -3.77968773e-02,  3.42959426e-02,  4.46748473e-02,\n",
       "          2.49046125e-02,  2.41302885e-02,  2.72128098e-02,\n",
       "          2.39529051e-02, -4.98504415e-02,  1.37590431e-02,\n",
       "         -4.78975661e-02,  1.24825016e-02,  2.57855095e-02,\n",
       "         -2.02386864e-02,  3.72343995e-02, -1.60302408e-02,\n",
       "          4.15903665e-02, -2.24876050e-02, -2.45540980e-02,\n",
       "          5.78309223e-03],\n",
       "        [ 3.15114893e-02, -3.06233298e-02,  6.32984564e-03,\n",
       "         -5.31203672e-03, -4.28216234e-02,  5.82946464e-03,\n",
       "          2.11207941e-03,  1.94398053e-02,  2.31879838e-02,\n",
       "         -4.47881930e-02,  4.35747989e-02,  3.56225483e-02,\n",
       "          1.86170600e-02, -8.90662521e-03, -1.36676058e-02,\n",
       "          2.13102140e-02,  4.63886596e-02,  2.39128582e-02,\n",
       "          4.66168188e-02,  1.84294842e-02,  3.26615684e-02,\n",
       "          3.63404416e-02,  1.61826350e-02,  3.84939834e-03,\n",
       "         -3.33061963e-02, -4.03975621e-02, -3.16486508e-02,\n",
       "          4.66439612e-02,  2.49161571e-03, -1.65725835e-02,\n",
       "          2.64343657e-02,  2.14686245e-03,  1.57066323e-02,\n",
       "         -2.98995897e-03, -1.62567124e-02, -4.26706448e-02,\n",
       "          4.35610898e-02, -1.41542330e-02, -1.96530465e-02,\n",
       "         -3.40466388e-02, -4.34294716e-02, -9.50330496e-03,\n",
       "         -2.11814530e-02,  7.47300312e-03,  2.70168819e-02,\n",
       "         -3.25119123e-02, -3.37404013e-02, -4.74851727e-02,\n",
       "          4.69620340e-02,  1.18322000e-02, -1.86963789e-02,\n",
       "         -4.57473844e-03,  2.65512578e-02,  3.33089493e-02,\n",
       "          1.20769516e-02, -2.93846261e-02,  3.20344083e-02,\n",
       "          1.12299100e-02, -4.46605459e-02, -1.31312758e-03,\n",
       "          6.96403906e-03,  4.95500602e-02,  2.26183869e-02,\n",
       "         -3.39855440e-02],\n",
       "        [ 1.11485720e-02,  3.50956991e-03, -1.84872039e-02,\n",
       "          3.62291224e-02, -4.57645059e-02,  1.35193132e-02,\n",
       "          3.56057771e-02,  1.01599842e-03, -7.58697838e-03,\n",
       "          2.46542357e-02,  1.95416100e-02,  8.40876251e-03,\n",
       "          2.51064636e-02,  3.18767913e-02, -2.12820768e-02,\n",
       "          1.19014606e-02,  3.58151272e-03, -3.38093638e-02,\n",
       "          2.82793306e-02, -2.54833233e-02,  4.42412756e-02,\n",
       "         -3.95999663e-02, -2.58176811e-02,  1.62329525e-03,\n",
       "         -1.85465813e-02, -3.27833444e-02,  1.37347840e-02,\n",
       "          1.38862468e-02, -3.94420139e-02, -7.90281221e-03,\n",
       "          2.85297632e-03,  1.41676702e-02,  4.70138527e-02,\n",
       "         -1.49937645e-02,  4.29281704e-02, -4.73130457e-02,\n",
       "         -2.74076462e-02, -1.15701444e-02,  4.16471027e-02,\n",
       "         -3.33445221e-02,  1.60093233e-03,  2.16095112e-02,\n",
       "         -3.93438227e-02,  2.46395133e-02, -4.47099470e-02,\n",
       "          2.49377973e-02, -4.93979454e-02, -3.20217013e-02,\n",
       "         -1.23795383e-02, -9.10208374e-03,  2.64961012e-02,\n",
       "         -4.95129116e-02, -7.19069317e-03, -1.43416412e-02,\n",
       "          3.15792896e-02,  1.71246789e-02, -2.80888565e-02,\n",
       "          1.54449604e-02,  4.44235466e-02,  1.10764280e-02,\n",
       "          1.87838413e-02, -2.15461496e-02, -1.29797831e-02,\n",
       "          2.17112340e-02],\n",
       "        [-3.06319129e-02,  1.02347359e-02, -1.70175321e-02,\n",
       "          1.85104720e-02,  3.97824086e-02,  1.89664029e-02,\n",
       "         -7.89577886e-03,  4.84436639e-02,  1.38464905e-02,\n",
       "         -3.41610797e-02, -1.31202117e-02,  1.07548945e-02,\n",
       "          1.26733445e-02, -4.37744260e-02,  1.45491995e-02,\n",
       "          3.20539512e-02,  2.88447030e-02,  2.90423743e-02,\n",
       "          1.52316578e-02, -4.38504815e-02,  4.58531715e-02,\n",
       "         -3.40863839e-02,  4.32403348e-02, -1.95696205e-03,\n",
       "          4.70166914e-02,  1.52908899e-02,  9.26259905e-03,\n",
       "          4.07245494e-02,  3.77929546e-02,  1.86335705e-02,\n",
       "          4.17029001e-02,  3.50543000e-02, -2.73705367e-02,\n",
       "          2.43378319e-02,  4.80593778e-02,  3.87888812e-02,\n",
       "          3.29840891e-02,  6.90697506e-03, -2.63776425e-02,\n",
       "          4.16526943e-03, -4.41549197e-02,  4.17250879e-02,\n",
       "         -7.24754483e-03, -4.02585492e-02,  5.41716814e-03,\n",
       "         -4.10001054e-02,  1.01484433e-02,  3.35072614e-02,\n",
       "         -1.49811134e-02,  5.79599291e-03, -9.39900801e-03,\n",
       "          1.50730722e-02,  1.26348399e-02, -7.87016004e-03,\n",
       "         -4.69683073e-02, -2.52906233e-03, -2.40615495e-02,\n",
       "          2.65575387e-02,  3.28391455e-02, -3.83611917e-02,\n",
       "          4.31718715e-02, -2.92210821e-02,  1.54899992e-02,\n",
       "          3.68221290e-02],\n",
       "        [ 1.17142200e-02,  2.31670178e-02, -1.39112957e-02,\n",
       "          6.76236302e-03, -4.62041385e-02, -3.67739424e-02,\n",
       "         -3.88539545e-02, -2.57140994e-02,  2.75770687e-02,\n",
       "          2.37995870e-02, -5.45381382e-03,  3.28114145e-02,\n",
       "          9.61109251e-03,  4.46872748e-02,  2.27243043e-02,\n",
       "          4.74137552e-02, -2.80990005e-02, -3.45478170e-02,\n",
       "          4.07367013e-02,  3.79623212e-02,  1.04695447e-02,\n",
       "         -1.32077932e-02, -2.05726027e-02,  5.64731658e-04,\n",
       "          9.66690481e-04, -2.52784621e-02,  2.81298421e-02,\n",
       "          2.48920172e-04,  3.56365368e-03,  4.01025750e-02,\n",
       "         -1.53482780e-02,  4.83330004e-02,  1.03183277e-02,\n",
       "         -1.28002279e-02, -5.48604876e-03, -2.21084952e-02,\n",
       "         -3.40190753e-02, -9.59044695e-03, -3.92731428e-02,\n",
       "          3.06475274e-02,  4.00447957e-02, -1.19687803e-02,\n",
       "         -2.61777993e-02, -1.19342692e-02, -2.35181935e-02,\n",
       "         -1.02843270e-02,  4.84511890e-02,  2.02788971e-02,\n",
       "          2.39904411e-02, -4.01025414e-02,  3.64273228e-02,\n",
       "          4.66428138e-02,  3.58998813e-02,  1.53315403e-02,\n",
       "          2.33331434e-02, -2.14228164e-02, -4.95425574e-02,\n",
       "          2.99651511e-02,  9.98758152e-03,  3.73988412e-02,\n",
       "         -2.16890704e-02,  1.82389282e-02, -1.84773430e-02,\n",
       "          3.83790396e-02]]], dtype=float32)>"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 案例\n",
    "random_sentence = random.choice(train_sentences)  \n",
    "print(f\"Original text:\\n{random_sentence}\\\n",
    "      \\n\\nEmbedded version:\")\n",
    "\n",
    "random_sentence_int = text_vectorizer([random_sentence])\n",
    "print(f\"random_sentence_int:{random_sentence_int}\")\n",
    "sample_embed = embedding(random_sentence_int)\n",
    "sample_embed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(64,), dtype=float32, numpy=\n",
       "array([-0.01816958,  0.02902657,  0.03182757,  0.03364861,  0.03000829,\n",
       "       -0.00718151,  0.00467654,  0.01020144, -0.00736921, -0.02081333,\n",
       "       -0.04352559,  0.04699392,  0.03219244, -0.01355219, -0.03219452,\n",
       "       -0.00062098, -0.01431466, -0.01571249, -0.03090109, -0.04874123,\n",
       "       -0.00608097,  0.04181569,  0.02727324,  0.0439487 ,  0.02036719,\n",
       "        0.00657899, -0.03523439, -0.02960699,  0.02340691,  0.02909729,\n",
       "        0.02388418, -0.03417484,  0.00789453,  0.00939922,  0.03479142,\n",
       "        0.03392268, -0.00127207, -0.00847246, -0.04109978,  0.04990759,\n",
       "       -0.03528573,  0.00274533,  0.01558513, -0.03948501, -0.03440619,\n",
       "       -0.03779688,  0.03429594,  0.04467485,  0.02490461,  0.02413029,\n",
       "        0.02721281,  0.02395291, -0.04985044,  0.01375904, -0.04789757,\n",
       "        0.0124825 ,  0.02578551, -0.02023869,  0.0372344 , -0.01603024,\n",
       "        0.04159037, -0.0224876 , -0.0245541 ,  0.00578309], dtype=float32)>"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 单个词映射出的embedding 向量\n",
    "sample_embed[0][0] "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These values might not mean much to us but they're what our computer sees each word as. When our model looks for patterns in different samples, these values will be updated as necessary."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modelling a text dataset  模型化\n",
    "\n",
    "![](https://raw.githubusercontent.com/mrdbourke/tensorflow-deep-learning/main/images/08-inputs-and-outputs-with-shapes-and-models-were-going-to-build.png)\n",
    "*Once you've got your inputs and outputs prepared, it's a matter of figuring out which machine learning model to build in between them to bridge the gap.*\n",
    "\n",
    "Now that we've got a way to turn our text data into numbers, we can start to build machine learning models to model it.\n",
    "\n",
    "To get plenty of practice, we're going to build a series of different models, each as its own experiment. We'll then compare the results of each model and see which one performed best.\n",
    "\n",
    "More specifically, we'll be building the following:\n",
    "* **Model 0**: Naive Bayes (baseline)\n",
    "* **Model 1**: Feed-forward neural network (dense model)\n",
    "* **Model 2**: LSTM model\n",
    "* **Model 3**: GRU model\n",
    "* **Model 4**: Bidirectional-LSTM model\n",
    "* **Model 5**: 1D Convolutional Neural Network\n",
    "* **Model 6**: TensorFlow Hub Pretrained Feature Extractor\n",
    "* **Model 7**: Same as model 6 with 10% of training data\n",
    "\n",
    "Model 0 is the simplest to acquire a baseline which we'll expect each other of the other deeper models to beat.\n",
    "\n",
    "Each experiment will go through the following steps:\n",
    "* Construct the model\n",
    "* Train the model\n",
    "* Make predictions with the model\n",
    "* Track prediction evaluation metrics for later comparison\n",
    "\n",
    "Let's get started."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model 0: Getting a baseline  基准模型 TF-IDF\n",
    "\n",
    "As with all machine learning modelling experiments, it's important to create a baseline model so you've got a benchmark for future experiments to build upon.\n",
    "\n",
    "To create our baseline, we'll create a Scikit-Learn Pipeline using the TF-IDF (term frequency-inverse document frequency) formula to convert our words to numbers and then model them with the [Multinomial Naive Bayes algorithm](https://scikit-learn.org/stable/modules/generated/sklearn.naive_bayes.MultinomialNB.html#sklearn.naive_bayes.MultinomialNB). This was chosen via referring to the [Scikit-Learn machine learning map](https://scikit-learn.org/stable/tutorial/machine_learning_map/index.html).\n",
    "\n",
    "> 📖 **Reading:** The ins and outs of TF-IDF algorithm is beyond the scope of this notebook, however, the curious reader is encouraged to check out the [Scikit-Learn documentation for more](https://scikit-learn.org/stable/modules/feature_extraction.html#tfidf-term-weighting)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-1 {\n",
       "  /* Definition of color scheme common for light and dark mode */\n",
       "  --sklearn-color-text: black;\n",
       "  --sklearn-color-line: gray;\n",
       "  /* Definition of color scheme for unfitted estimators */\n",
       "  --sklearn-color-unfitted-level-0: #fff5e6;\n",
       "  --sklearn-color-unfitted-level-1: #f6e4d2;\n",
       "  --sklearn-color-unfitted-level-2: #ffe0b3;\n",
       "  --sklearn-color-unfitted-level-3: chocolate;\n",
       "  /* Definition of color scheme for fitted estimators */\n",
       "  --sklearn-color-fitted-level-0: #f0f8ff;\n",
       "  --sklearn-color-fitted-level-1: #d4ebff;\n",
       "  --sklearn-color-fitted-level-2: #b3dbfd;\n",
       "  --sklearn-color-fitted-level-3: cornflowerblue;\n",
       "\n",
       "  /* Specific color for light theme */\n",
       "  --sklearn-color-text-on-default-background: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, black)));\n",
       "  --sklearn-color-background: var(--sg-background-color, var(--theme-background, var(--jp-layout-color0, white)));\n",
       "  --sklearn-color-border-box: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, black)));\n",
       "  --sklearn-color-icon: #696969;\n",
       "\n",
       "  @media (prefers-color-scheme: dark) {\n",
       "    /* Redefinition of color scheme for dark theme */\n",
       "    --sklearn-color-text-on-default-background: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, white)));\n",
       "    --sklearn-color-background: var(--sg-background-color, var(--theme-background, var(--jp-layout-color0, #111)));\n",
       "    --sklearn-color-border-box: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, white)));\n",
       "    --sklearn-color-icon: #878787;\n",
       "  }\n",
       "}\n",
       "\n",
       "#sk-container-id-1 {\n",
       "  color: var(--sklearn-color-text);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 pre {\n",
       "  padding: 0;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 input.sk-hidden--visually {\n",
       "  border: 0;\n",
       "  clip: rect(1px 1px 1px 1px);\n",
       "  clip: rect(1px, 1px, 1px, 1px);\n",
       "  height: 1px;\n",
       "  margin: -1px;\n",
       "  overflow: hidden;\n",
       "  padding: 0;\n",
       "  position: absolute;\n",
       "  width: 1px;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-dashed-wrapped {\n",
       "  border: 1px dashed var(--sklearn-color-line);\n",
       "  margin: 0 0.4em 0.5em 0.4em;\n",
       "  box-sizing: border-box;\n",
       "  padding-bottom: 0.4em;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-container {\n",
       "  /* jupyter's `normalize.less` sets `[hidden] { display: none; }`\n",
       "     but bootstrap.min.css set `[hidden] { display: none !important; }`\n",
       "     so we also need the `!important` here to be able to override the\n",
       "     default hidden behavior on the sphinx rendered scikit-learn.org.\n",
       "     See: https://github.com/scikit-learn/scikit-learn/issues/21755 */\n",
       "  display: inline-block !important;\n",
       "  position: relative;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-text-repr-fallback {\n",
       "  display: none;\n",
       "}\n",
       "\n",
       "div.sk-parallel-item,\n",
       "div.sk-serial,\n",
       "div.sk-item {\n",
       "  /* draw centered vertical line to link estimators */\n",
       "  background-image: linear-gradient(var(--sklearn-color-text-on-default-background), var(--sklearn-color-text-on-default-background));\n",
       "  background-size: 2px 100%;\n",
       "  background-repeat: no-repeat;\n",
       "  background-position: center center;\n",
       "}\n",
       "\n",
       "/* Parallel-specific style estimator block */\n",
       "\n",
       "#sk-container-id-1 div.sk-parallel-item::after {\n",
       "  content: \"\";\n",
       "  width: 100%;\n",
       "  border-bottom: 2px solid var(--sklearn-color-text-on-default-background);\n",
       "  flex-grow: 1;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-parallel {\n",
       "  display: flex;\n",
       "  align-items: stretch;\n",
       "  justify-content: center;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "  position: relative;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-parallel-item {\n",
       "  display: flex;\n",
       "  flex-direction: column;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-parallel-item:first-child::after {\n",
       "  align-self: flex-end;\n",
       "  width: 50%;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-parallel-item:last-child::after {\n",
       "  align-self: flex-start;\n",
       "  width: 50%;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-parallel-item:only-child::after {\n",
       "  width: 0;\n",
       "}\n",
       "\n",
       "/* Serial-specific style estimator block */\n",
       "\n",
       "#sk-container-id-1 div.sk-serial {\n",
       "  display: flex;\n",
       "  flex-direction: column;\n",
       "  align-items: center;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "  padding-right: 1em;\n",
       "  padding-left: 1em;\n",
       "}\n",
       "\n",
       "\n",
       "/* Toggleable style: style used for estimator/Pipeline/ColumnTransformer box that is\n",
       "clickable and can be expanded/collapsed.\n",
       "- Pipeline and ColumnTransformer use this feature and define the default style\n",
       "- Estimators will overwrite some part of the style using the `sk-estimator` class\n",
       "*/\n",
       "\n",
       "/* Pipeline and ColumnTransformer style (default) */\n",
       "\n",
       "#sk-container-id-1 div.sk-toggleable {\n",
       "  /* Default theme specific background. It is overwritten whether we have a\n",
       "  specific estimator or a Pipeline/ColumnTransformer */\n",
       "  background-color: var(--sklearn-color-background);\n",
       "}\n",
       "\n",
       "/* Toggleable label */\n",
       "#sk-container-id-1 label.sk-toggleable__label {\n",
       "  cursor: pointer;\n",
       "  display: block;\n",
       "  width: 100%;\n",
       "  margin-bottom: 0;\n",
       "  padding: 0.5em;\n",
       "  box-sizing: border-box;\n",
       "  text-align: center;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 label.sk-toggleable__label-arrow:before {\n",
       "  /* Arrow on the left of the label */\n",
       "  content: \"▸\";\n",
       "  float: left;\n",
       "  margin-right: 0.25em;\n",
       "  color: var(--sklearn-color-icon);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {\n",
       "  color: var(--sklearn-color-text);\n",
       "}\n",
       "\n",
       "/* Toggleable content - dropdown */\n",
       "\n",
       "#sk-container-id-1 div.sk-toggleable__content {\n",
       "  max-height: 0;\n",
       "  max-width: 0;\n",
       "  overflow: hidden;\n",
       "  text-align: left;\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-toggleable__content.fitted {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-toggleable__content pre {\n",
       "  margin: 0.2em;\n",
       "  border-radius: 0.25em;\n",
       "  color: var(--sklearn-color-text);\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-toggleable__content.fitted pre {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {\n",
       "  /* Expand drop-down */\n",
       "  max-height: 200px;\n",
       "  max-width: 100%;\n",
       "  overflow: auto;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {\n",
       "  content: \"▾\";\n",
       "}\n",
       "\n",
       "/* Pipeline/ColumnTransformer-specific style */\n",
       "\n",
       "#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
       "  color: var(--sklearn-color-text);\n",
       "  background-color: var(--sklearn-color-unfitted-level-2);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-label.fitted input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
       "  background-color: var(--sklearn-color-fitted-level-2);\n",
       "}\n",
       "\n",
       "/* Estimator-specific style */\n",
       "\n",
       "/* Colorize estimator box */\n",
       "#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-2);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-estimator.fitted input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-2);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-label label.sk-toggleable__label,\n",
       "#sk-container-id-1 div.sk-label label {\n",
       "  /* The background is the default theme color */\n",
       "  color: var(--sklearn-color-text-on-default-background);\n",
       "}\n",
       "\n",
       "/* On hover, darken the color of the background */\n",
       "#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {\n",
       "  color: var(--sklearn-color-text);\n",
       "  background-color: var(--sklearn-color-unfitted-level-2);\n",
       "}\n",
       "\n",
       "/* Label box, darken color on hover, fitted */\n",
       "#sk-container-id-1 div.sk-label.fitted:hover label.sk-toggleable__label.fitted {\n",
       "  color: var(--sklearn-color-text);\n",
       "  background-color: var(--sklearn-color-fitted-level-2);\n",
       "}\n",
       "\n",
       "/* Estimator label */\n",
       "\n",
       "#sk-container-id-1 div.sk-label label {\n",
       "  font-family: monospace;\n",
       "  font-weight: bold;\n",
       "  display: inline-block;\n",
       "  line-height: 1.2em;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-label-container {\n",
       "  text-align: center;\n",
       "}\n",
       "\n",
       "/* Estimator-specific */\n",
       "#sk-container-id-1 div.sk-estimator {\n",
       "  font-family: monospace;\n",
       "  border: 1px dotted var(--sklearn-color-border-box);\n",
       "  border-radius: 0.25em;\n",
       "  box-sizing: border-box;\n",
       "  margin-bottom: 0.5em;\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-estimator.fitted {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-0);\n",
       "}\n",
       "\n",
       "/* on hover */\n",
       "#sk-container-id-1 div.sk-estimator:hover {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-2);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-estimator.fitted:hover {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-2);\n",
       "}\n",
       "\n",
       "/* Specification for estimator info (e.g. \"i\" and \"?\") */\n",
       "\n",
       "/* Common style for \"i\" and \"?\" */\n",
       "\n",
       ".sk-estimator-doc-link,\n",
       "a:link.sk-estimator-doc-link,\n",
       "a:visited.sk-estimator-doc-link {\n",
       "  float: right;\n",
       "  font-size: smaller;\n",
       "  line-height: 1em;\n",
       "  font-family: monospace;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "  border-radius: 1em;\n",
       "  height: 1em;\n",
       "  width: 1em;\n",
       "  text-decoration: none !important;\n",
       "  margin-left: 1ex;\n",
       "  /* unfitted */\n",
       "  border: var(--sklearn-color-unfitted-level-1) 1pt solid;\n",
       "  color: var(--sklearn-color-unfitted-level-1);\n",
       "}\n",
       "\n",
       ".sk-estimator-doc-link.fitted,\n",
       "a:link.sk-estimator-doc-link.fitted,\n",
       "a:visited.sk-estimator-doc-link.fitted {\n",
       "  /* fitted */\n",
       "  border: var(--sklearn-color-fitted-level-1) 1pt solid;\n",
       "  color: var(--sklearn-color-fitted-level-1);\n",
       "}\n",
       "\n",
       "/* On hover */\n",
       "div.sk-estimator:hover .sk-estimator-doc-link:hover,\n",
       ".sk-estimator-doc-link:hover,\n",
       "div.sk-label-container:hover .sk-estimator-doc-link:hover,\n",
       ".sk-estimator-doc-link:hover {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-3);\n",
       "  color: var(--sklearn-color-background);\n",
       "  text-decoration: none;\n",
       "}\n",
       "\n",
       "div.sk-estimator.fitted:hover .sk-estimator-doc-link.fitted:hover,\n",
       ".sk-estimator-doc-link.fitted:hover,\n",
       "div.sk-label-container:hover .sk-estimator-doc-link.fitted:hover,\n",
       ".sk-estimator-doc-link.fitted:hover {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-3);\n",
       "  color: var(--sklearn-color-background);\n",
       "  text-decoration: none;\n",
       "}\n",
       "\n",
       "/* Span, style for the box shown on hovering the info icon */\n",
       ".sk-estimator-doc-link span {\n",
       "  display: none;\n",
       "  z-index: 9999;\n",
       "  position: relative;\n",
       "  font-weight: normal;\n",
       "  right: .2ex;\n",
       "  padding: .5ex;\n",
       "  margin: .5ex;\n",
       "  width: min-content;\n",
       "  min-width: 20ex;\n",
       "  max-width: 50ex;\n",
       "  color: var(--sklearn-color-text);\n",
       "  box-shadow: 2pt 2pt 4pt #999;\n",
       "  /* unfitted */\n",
       "  background: var(--sklearn-color-unfitted-level-0);\n",
       "  border: .5pt solid var(--sklearn-color-unfitted-level-3);\n",
       "}\n",
       "\n",
       ".sk-estimator-doc-link.fitted span {\n",
       "  /* fitted */\n",
       "  background: var(--sklearn-color-fitted-level-0);\n",
       "  border: var(--sklearn-color-fitted-level-3);\n",
       "}\n",
       "\n",
       ".sk-estimator-doc-link:hover span {\n",
       "  display: block;\n",
       "}\n",
       "\n",
       "/* \"?\"-specific style due to the `<a>` HTML tag */\n",
       "\n",
       "#sk-container-id-1 a.estimator_doc_link {\n",
       "  float: right;\n",
       "  font-size: 1rem;\n",
       "  line-height: 1em;\n",
       "  font-family: monospace;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "  border-radius: 1rem;\n",
       "  height: 1rem;\n",
       "  width: 1rem;\n",
       "  text-decoration: none;\n",
       "  /* unfitted */\n",
       "  color: var(--sklearn-color-unfitted-level-1);\n",
       "  border: var(--sklearn-color-unfitted-level-1) 1pt solid;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 a.estimator_doc_link.fitted {\n",
       "  /* fitted */\n",
       "  border: var(--sklearn-color-fitted-level-1) 1pt solid;\n",
       "  color: var(--sklearn-color-fitted-level-1);\n",
       "}\n",
       "\n",
       "/* On hover */\n",
       "#sk-container-id-1 a.estimator_doc_link:hover {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-3);\n",
       "  color: var(--sklearn-color-background);\n",
       "  text-decoration: none;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 a.estimator_doc_link.fitted:hover {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-3);\n",
       "}\n",
       "</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>Pipeline(steps=[(&#x27;tfidf&#x27;, TfidfVectorizer()), (&#x27;clf&#x27;, MultinomialNB())])</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item sk-dashed-wrapped\"><div class=\"sk-label-container\"><div class=\"sk-label fitted sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" ><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label fitted sk-toggleable__label-arrow fitted\">&nbsp;&nbsp;Pipeline<a class=\"sk-estimator-doc-link fitted\" rel=\"noreferrer\" target=\"_blank\" href=\"https://scikit-learn.org/1.5/modules/generated/sklearn.pipeline.Pipeline.html\">?<span>Documentation for Pipeline</span></a><span class=\"sk-estimator-doc-link fitted\">i<span>Fitted</span></span></label><div class=\"sk-toggleable__content fitted\"><pre>Pipeline(steps=[(&#x27;tfidf&#x27;, TfidfVectorizer()), (&#x27;clf&#x27;, MultinomialNB())])</pre></div> </div></div><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-estimator fitted sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-2\" type=\"checkbox\" ><label for=\"sk-estimator-id-2\" class=\"sk-toggleable__label fitted sk-toggleable__label-arrow fitted\">&nbsp;TfidfVectorizer<a class=\"sk-estimator-doc-link fitted\" rel=\"noreferrer\" target=\"_blank\" href=\"https://scikit-learn.org/1.5/modules/generated/sklearn.feature_extraction.text.TfidfVectorizer.html\">?<span>Documentation for TfidfVectorizer</span></a></label><div class=\"sk-toggleable__content fitted\"><pre>TfidfVectorizer()</pre></div> </div></div><div class=\"sk-item\"><div class=\"sk-estimator fitted sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-3\" type=\"checkbox\" ><label for=\"sk-estimator-id-3\" class=\"sk-toggleable__label fitted sk-toggleable__label-arrow fitted\">&nbsp;MultinomialNB<a class=\"sk-estimator-doc-link fitted\" rel=\"noreferrer\" target=\"_blank\" href=\"https://scikit-learn.org/1.5/modules/generated/sklearn.naive_bayes.MultinomialNB.html\">?<span>Documentation for MultinomialNB</span></a></label><div class=\"sk-toggleable__content fitted\"><pre>MultinomialNB()</pre></div> </div></div></div></div></div></div>"
      ],
      "text/plain": [
       "Pipeline(steps=[('tfidf', TfidfVectorizer()), ('clf', MultinomialNB())])"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "model0 = Pipeline([\n",
    "    ('tfidf', TfidfVectorizer()),\n",
    "    ('clf', MultinomialNB())\n",
    "])\n",
    "\n",
    "model0.fit(train_sentences, train_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Our baseline model achieves an accuracy of 79.27%\n"
     ]
    }
   ],
   "source": [
    "baseline_score = model0.score(val_sentences, val_labels)\n",
    "print(f\"Our baseline model achieves an accuracy of {baseline_score*100:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 1, 1, 0, 0, 1, 1, 1, 1, 0])"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "baseline_preds = model0.predict(val_sentences)\n",
    "baseline_preds[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating an evaluation function for our model experiments  评估函数\n",
    "\n",
    "We could evaluate these as they are but since we're going to be evaluating several models in the same way going forward, let's create a helper function which takes an array of predictions and ground truth labels and computes the following:\n",
    "* Accuracy\n",
    "* Precision\n",
    "* Recall\n",
    "* F1-score\n",
    "\n",
    "> 🔑 **Note:** Since we're dealing with a classification problem, the above metrics are the most appropriate. If we were working with a regression problem, other metrics such as MAE (mean absolute error) would be a better choice."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score, precision_recall_fscore_support,roc_auc_score\n",
    "\n",
    "def calculate_results(y_true, y_pred):\n",
    "  \"\"\"\n",
    "  Calculates model accuracy, precision, recall and f1 score of a binary classification model.\n",
    "\n",
    "  Args:\n",
    "  -----\n",
    "  y_true = true labels in the form of a 1D array\n",
    "  y_pred = predicted labels in the form of a 1D array\n",
    "\n",
    "  Returns a dictionary of accuracy, precision, recall, f1-score.\n",
    "  \"\"\"\n",
    "  # Calculate model accuracy\n",
    "  model_accuracy = accuracy_score(y_true, y_pred) \n",
    "  # Calculate model precision, recall and f1 score using \"weighted\" average\n",
    "  model_precision, model_recall, model_f1, _ = precision_recall_fscore_support(y_true, y_pred, average=\"weighted\")\n",
    "  auc = roc_auc_score(y_true, y_pred)\n",
    "  model_results = {\"accuracy\": model_accuracy,\n",
    "                  \"precision\": model_precision,\n",
    "                  \"recall\": model_recall,\n",
    "                  \"f1\": model_f1,\n",
    "                  \"auc\":auc}\n",
    "  return model_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'accuracy': 0.7926509186351706,\n",
       " 'precision': 0.8111390004213173,\n",
       " 'recall': 0.7926509186351706,\n",
       " 'f1': 0.7862189758049549,\n",
       " 'auc': 0.7794019656838247}"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "baseline_results = calculate_results(val_labels,baseline_preds)\n",
    "baseline_results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model 1: A simple dense model  简单的dnn模型\n",
    "\n",
    "The first \"deep\" model we're going to build is a single layer dense model. In fact, it's barely going to have a single layer. \n",
    "\n",
    "It'll take our text and labels as input, tokenize the text, create an embedding, find the average of the embedding (using Global Average Pooling) and then pass the average through a fully connected layer with one output unit and a sigmoid activation function.\n",
    "\n",
    "If the previous sentence sounds like a mouthful, it'll make sense when we code it out (remember, if in doubt, code it out).\n",
    "\n",
    "And since we're going to be building a number of TensorFlow deep learning models, we'll import our `create_tensorboard_callback()` function from `helper_functions.py` to keep track of the results of each. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "from helper_functions import create_tensorboard_callback\n",
    "\n",
    "SAVE_DIR = \"model_log\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model1\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_1 (InputLayer)        [(None, 1)]               0         \n",
      "                                                                 \n",
      " text_vectorization_1 (TextV  (None, 15)               0         \n",
      " ectorization)                                                   \n",
      "                                                                 \n",
      " embedding_1 (Embedding)     (None, 15, 64)            640000    \n",
      "                                                                 \n",
      " global_average_pooling1d (G  (None, 64)               0         \n",
      " lobalAveragePooling1D)                                          \n",
      "                                                                 \n",
      " dense (Dense)               (None, 1)                 65        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 640,065\n",
      "Trainable params: 640,065\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Build model with the Functional API\n",
    "from tensorflow.keras import layers\n",
    "inputs = layers.Input(shape=(1,),dtype='string') # 输入是一维的 strings\n",
    "x = text_vectorizer(inputs) # 数字化\n",
    "x = embedding(x) # embedding\n",
    "x = layers.GlobalAveragePooling1D()(x)  # 全局平均池化\n",
    "outputs = layers.Dense(1,activation='sigmoid')(x) # 输出\n",
    "model1 = tf.keras.Model(inputs,outputs,name='model1')\n",
    "\n",
    "model1.compile(loss='binary_crossentropy',\n",
    "              optimizer=tf.keras.optimizers.Adam(),\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "model1.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving TensorBoard log files to: model_log/simple_dense_model/20250122-102752\n",
      "Epoch 1/5\n",
      "215/215 [==============================] - 1s 3ms/step - loss: 0.6321 - accuracy: 0.6697 - val_loss: 0.5771 - val_accuracy: 0.7257\n",
      "Epoch 2/5\n",
      "215/215 [==============================] - 0s 2ms/step - loss: 0.4888 - accuracy: 0.7980 - val_loss: 0.4907 - val_accuracy: 0.7835\n",
      "Epoch 3/5\n",
      "215/215 [==============================] - 0s 2ms/step - loss: 0.3933 - accuracy: 0.8428 - val_loss: 0.4677 - val_accuracy: 0.7848\n",
      "Epoch 4/5\n",
      "215/215 [==============================] - 0s 2ms/step - loss: 0.3326 - accuracy: 0.8685 - val_loss: 0.4589 - val_accuracy: 0.7900\n",
      "Epoch 5/5\n",
      "215/215 [==============================] - 0s 2ms/step - loss: 0.2861 - accuracy: 0.8913 - val_loss: 0.4594 - val_accuracy: 0.7913\n"
     ]
    }
   ],
   "source": [
    "# 大部分的参数都在embedding层，64维度，10000个词，参数是640000\n",
    "model1_history = model1.fit(train_sentences,train_labels,epochs=5,validation_data=(val_sentences,val_labels),\n",
    "                            callbacks=[create_tensorboard_callback(dir_name=SAVE_DIR, \n",
    "                                                                     experiment_name=\"simple_dense_model\")])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 1/24 [>.............................] - ETA: 0s - loss: 0.5675 - accuracy: 0.7500"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "24/24 [==============================] - 0s 785us/step - loss: 0.4594 - accuracy: 0.7913\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.4594421088695526, 0.7913385629653931]"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model1.evaluate(val_sentences,val_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "keras.layers.core.embedding.Embedding"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(embedding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<tf.Variable 'embedding_1/embeddings:0' shape=(10000, 64) dtype=float32, numpy=\n",
       " array([[-0.01238398,  0.03976315,  0.05967141, ...,  0.00147118,\n",
       "         -0.00120488,  0.00906573],\n",
       "        [-0.02461476,  0.03500758,  0.03809715, ..., -0.01444725,\n",
       "         -0.01653738, -0.00166482],\n",
       "        [-0.01028796, -0.01531043,  0.01261931, ..., -0.0136078 ,\n",
       "          0.01689295, -0.03572265],\n",
       "        ...,\n",
       "        [-0.02546743,  0.01498115,  0.01955494, ...,  0.0299913 ,\n",
       "          0.00061687,  0.01210845],\n",
       "        [-0.03546415,  0.03735868,  0.01723779, ...,  0.00926579,\n",
       "          0.01255319, -0.02697073],\n",
       "        [-0.06960812,  0.08935145,  0.07606605, ...,  0.09076706,\n",
       "          0.09401847, -0.05873762]], dtype=float32)>]"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embedding.weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([[-0.01238398,  0.03976315,  0.05967141, ...,  0.00147118,\n",
       "         -0.00120488,  0.00906573],\n",
       "        [-0.02461476,  0.03500758,  0.03809715, ..., -0.01444725,\n",
       "         -0.01653738, -0.00166482],\n",
       "        [-0.01028796, -0.01531043,  0.01261931, ..., -0.0136078 ,\n",
       "          0.01689295, -0.03572265],\n",
       "        ...,\n",
       "        [-0.02546743,  0.01498115,  0.01955494, ...,  0.0299913 ,\n",
       "          0.00061687,  0.01210845],\n",
       "        [-0.03546415,  0.03735868,  0.01723779, ...,  0.00926579,\n",
       "          0.01255319, -0.02697073],\n",
       "        [-0.06960812,  0.08935145,  0.07606605, ...,  0.09076706,\n",
       "          0.09401847, -0.05873762]], dtype=float32),\n",
       " (10000, 64))"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embed_weights = model1.get_layer('embedding_1').get_weights()[0]\n",
    "embed_weights,embed_weights.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "24/24 [==============================] - 0s 741us/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[0.3801634 ],\n",
       "       [0.76140225],\n",
       "       [0.99346286],\n",
       "       [0.13641238],\n",
       "       [0.17425583],\n",
       "       [0.9051495 ],\n",
       "       [0.9056347 ],\n",
       "       [0.98097146],\n",
       "       [0.9245294 ],\n",
       "       [0.22286959]], dtype=float32)"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model1_pred_probs = model1.predict(val_sentences)\n",
    "model1_pred_probs[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(762,), dtype=float32, numpy=\n",
       "array([0., 1., 1., 0., 0., 1., 1., 1., 1., 0., 0., 1., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 1., 0., 0., 0., 1., 1., 0., 0., 0., 0., 1., 0., 1., 0.,\n",
       "       0., 0., 1., 0., 0., 1., 0., 0., 0., 0., 1., 1., 0., 1., 0., 1., 0.,\n",
       "       1., 0., 0., 0., 0., 0., 1., 0., 0., 0., 1., 1., 0., 1., 1., 1., 1.,\n",
       "       0., 0., 0., 0., 1., 0., 0., 0., 0., 1., 0., 1., 1., 0., 1., 0., 0.,\n",
       "       0., 0., 0., 1., 1., 1., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 0.,\n",
       "       0., 1., 1., 0., 1., 1., 0., 0., 1., 0., 0., 0., 0., 0., 0., 1., 1.,\n",
       "       0., 1., 0., 0., 1., 0., 0., 1., 0., 0., 1., 0., 1., 1., 1., 1., 0.,\n",
       "       1., 0., 0., 0., 1., 0., 0., 1., 0., 1., 0., 0., 0., 1., 0., 0., 0.,\n",
       "       0., 0., 1., 1., 0., 1., 0., 1., 0., 0., 1., 0., 0., 1., 0., 1., 0.,\n",
       "       1., 1., 1., 0., 1., 0., 0., 0., 1., 1., 0., 1., 1., 1., 1., 1., 0.,\n",
       "       1., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 1., 0., 0., 1., 1., 0.,\n",
       "       0., 1., 0., 1., 1., 1., 0., 1., 0., 0., 0., 0., 1., 1., 1., 0., 1.,\n",
       "       0., 0., 0., 1., 0., 1., 1., 1., 1., 0., 0., 1., 0., 0., 0., 1., 0.,\n",
       "       0., 0., 0., 0., 1., 1., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 1.,\n",
       "       0., 1., 1., 0., 0., 1., 0., 0., 1., 1., 1., 1., 0., 0., 0., 0., 0.,\n",
       "       1., 0., 0., 1., 0., 0., 1., 0., 0., 0., 1., 0., 1., 1., 0., 0., 1.,\n",
       "       0., 1., 0., 0., 0., 0., 1., 0., 1., 0., 1., 0., 0., 0., 1., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 1., 0., 0., 1., 0., 1., 0., 0., 0., 0., 0., 1.,\n",
       "       0., 1., 0., 0., 1., 0., 1., 0., 0., 1., 0., 0., 1., 0., 0., 1., 1.,\n",
       "       0., 0., 1., 1., 1., 0., 1., 0., 0., 0., 0., 1., 0., 0., 0., 0., 1.,\n",
       "       1., 1., 0., 0., 0., 1., 1., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 1., 1., 0., 1., 0., 0., 1., 1., 0., 1., 0., 1.,\n",
       "       0., 0., 1., 0., 1., 0., 0., 0., 0., 1., 0., 1., 1., 0., 1., 0., 0.,\n",
       "       0., 1., 1., 0., 0., 0., 1., 1., 0., 1., 1., 0., 1., 1., 1., 1., 0.,\n",
       "       0., 0., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 0., 1.,\n",
       "       0., 0., 0., 1., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 0., 0., 1.,\n",
       "       1., 1., 0., 0., 0., 1., 1., 0., 0., 1., 0., 0., 1., 0., 0., 1., 0.,\n",
       "       0., 1., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 1., 0.,\n",
       "       0., 0., 0., 0., 1., 0., 1., 1., 1., 1., 1., 0., 0., 0., 1., 0., 0.,\n",
       "       0., 0., 0., 0., 1., 0., 1., 1., 1., 1., 0., 0., 1., 0., 0., 0., 0.,\n",
       "       0., 1., 1., 1., 1., 0., 0., 1., 0., 0., 0., 0., 1., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 1., 1., 0., 1., 1., 0., 1., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 1., 0., 1., 0., 1., 1., 0., 0., 1., 1., 1., 0., 0., 0., 0., 0.,\n",
       "       1., 0., 0., 0., 1., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 1., 1., 0., 0., 0., 1., 1., 1., 0., 1., 0., 1., 1., 0., 1.,\n",
       "       0., 0., 1., 0., 0., 0., 0., 0., 1., 0., 1., 0., 1., 0., 0., 0., 1.,\n",
       "       0., 0., 1., 1., 1., 1., 0., 0., 0., 0., 0., 1., 0., 1., 1., 0., 0.,\n",
       "       0., 0., 0., 1., 1., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 1.,\n",
       "       0., 0., 0., 0., 0., 1., 1., 0., 0., 0., 0., 1., 1., 1., 0., 0., 1.,\n",
       "       0., 0., 1., 0., 0., 1., 0., 0., 0., 0., 1., 1., 1., 0., 1., 0., 1.,\n",
       "       0., 1., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0.,\n",
       "       0., 0., 0., 0., 1., 0., 1., 0., 0., 1., 0., 0., 0., 0., 1., 0., 0.,\n",
       "       1., 1., 1., 1., 1., 0., 0., 0., 0., 1., 0., 0., 1., 1., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 1., 0., 1., 1., 0., 0., 0., 1., 0., 0.],\n",
       "      dtype=float32)>"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 将预测值改为0，1\n",
    "model1_preds =  tf.round(tf.squeeze(model1_pred_probs))\n",
    "model1_preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'accuracy': 0.7913385826771654,\n",
       " 'precision': 0.7976012509315953,\n",
       " 'recall': 0.7913385826771654,\n",
       " 'f1': 0.78803568654912,\n",
       " 'auc': 0.782088122605364}"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model1_results = calculate_results(val_labels,model1_preds)\n",
    "model1_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Baseline accuracy: 0.79, New accuracy: 0.79, Difference: -0.00\n",
      "Baseline precision: 0.81, New precision: 0.80, Difference: -0.01\n",
      "Baseline recall: 0.79, New recall: 0.79, Difference: -0.00\n",
      "Baseline f1: 0.79, New f1: 0.79, Difference: 0.00\n",
      "Baseline auc: 0.78, New auc: 0.78, Difference: 0.00\n"
     ]
    }
   ],
   "source": [
    "# Create a helper function to compare our baseline results to new model results\n",
    "def compare_baseline_to_new_results(baseline_results, new_model_results):\n",
    "  for key, value in baseline_results.items():\n",
    "    print(f\"Baseline {key}: {value:.2f}, New {key}: {new_model_results[key]:.2f}, Difference: {new_model_results[key]-value:.2f}\")\n",
    "\n",
    "compare_baseline_to_new_results(baseline_results=baseline_results, \n",
    "                                new_model_results=model1_results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualizing learned embeddings\n",
    "\n",
    "Our first model (`model_1`) contained an embedding layer (`embedding`) which learned a way of representing words as feature vectors by passing over the training data.\n",
    "\n",
    "Hearing this for the first few times may sound confusing.\n",
    "\n",
    "So to further help understand what a text embedding is, let's visualize the embedding our model learned.\n",
    "\n",
    "To do so, let's remind ourselves of the words in our vocabulary.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10000, ['', '[UNK]', 'the', 'a', 'in', 'to', 'of', 'and', 'i', 'is'])"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "words_in_vocab = text_vectorizer.get_vocabulary()\n",
    "len(words_in_vocab),words_in_vocab[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model1\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_1 (InputLayer)        [(None, 1)]               0         \n",
      "                                                                 \n",
      " text_vectorization_1 (TextV  (None, 15)               0         \n",
      " ectorization)                                                   \n",
      "                                                                 \n",
      " embedding_1 (Embedding)     (None, 15, 64)            640000    \n",
      "                                                                 \n",
      " global_average_pooling1d (G  (None, 64)               0         \n",
      " lobalAveragePooling1D)                                          \n",
      "                                                                 \n",
      " dense (Dense)               (None, 1)                 65        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 640,065\n",
      "Trainable params: 640,065\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model1.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([[-0.01238398,  0.03976315,  0.05967141, ...,  0.00147118,\n",
       "         -0.00120488,  0.00906573],\n",
       "        [-0.02461476,  0.03500758,  0.03809715, ..., -0.01444725,\n",
       "         -0.01653738, -0.00166482],\n",
       "        [-0.01028796, -0.01531043,  0.01261931, ..., -0.0136078 ,\n",
       "          0.01689295, -0.03572265],\n",
       "        ...,\n",
       "        [-0.02546743,  0.01498115,  0.01955494, ...,  0.0299913 ,\n",
       "          0.00061687,  0.01210845],\n",
       "        [-0.03546415,  0.03735868,  0.01723779, ...,  0.00926579,\n",
       "          0.01255319, -0.02697073],\n",
       "        [-0.06960812,  0.08935145,  0.07606605, ...,  0.09076706,\n",
       "          0.09401847, -0.05873762]], dtype=float32),\n",
       " (10000, 64))"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embed_weights = model1.get_layer('embedding_1').get_weights()[0]\n",
    "embed_weights,embed_weights.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we've got these two objects, we can use the [Embedding Projector tool](http://projector.tensorflow.org/_) to visualize our embedding. \n",
    "\n",
    "To use the Embedding Projector tool, we need two files:\n",
    "* The embedding vectors (same as embedding weights).\n",
    "* The meta data of the embedding vectors (the words they represent - our vocabulary).\n",
    "\n",
    "Right now, we've got of these files as Python objects. To download them to file, we're going to [use the code example available on the TensorFlow word embeddings tutorial page](https://www.tensorflow.org/tutorials/text/word_embeddings#retrieve_the_trained_word_embeddings_and_save_them_to_disk).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Code below is adapted from: https://www.tensorflow.org/tutorials/text/word_embeddings#retrieve_the_trained_word_embeddings_and_save_them_to_disk\n",
    "# import io\n",
    "\n",
    "# # Create output writers\n",
    "# out_v = io.open(\"embedding_vectors.tsv\", \"w\", encoding=\"utf-8\")\n",
    "# out_m = io.open(\"embedding_metadata.tsv\", \"w\", encoding=\"utf-8\")\n",
    "\n",
    "# # Write embedding vectors and words to file\n",
    "# for num, word in enumerate(words_in_vocab):\n",
    "#   if num == 0: \n",
    "#      continue # skip padding token\n",
    "#   vec = embed_weights[num]\n",
    "#   out_m.write(word + \"\\n\") # write words to file\n",
    "#   out_v.write(\"\\t\".join([str(x) for x in vec]) + \"\\n\") # write corresponding word vector to file\n",
    "# out_v.close()\n",
    "# out_m.close()\n",
    "\n",
    "# # Download files locally to upload to Embedding Projector\n",
    "# try:\n",
    "#   from google.colab import files\n",
    "# except ImportError:\n",
    "#   pass\n",
    "# else:\n",
    "#   files.download(\"embedding_vectors.tsv\")\n",
    "#   files.download(\"embedding_metadata.tsv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once you've downloaded the embedding vectors and metadata, you can visualize them using Embedding Vector tool:\n",
    "1. Go to  http://projector.tensorflow.org/  查看embedding的分布\n",
    "2. Click on \"Load data\"\n",
    "3. Upload the two files you downloaded (`embedding_vectors.tsv` and `embedding_metadata.tsv`)\n",
    "4. Explore\n",
    "5. Optional: You can share the data you've created by clicking \"Publish\"\n",
    "\n",
    "What do you find?\n",
    "\n",
    "Are words with similar meanings close together?\n",
    "\n",
    "Remember, they might not be. The embeddings we downloaded are how our model interprets words, not necessarily how we interpret them. \n",
    "\n",
    "Also, since the embedding has been learned purely from Tweets, it may contain some strange values as Tweets are a very unique style of natural language.\n",
    "\n",
    "> 🤔 **Question:** Do you have to visualize embeddings every time?\n",
    "\n",
    "No. Although helpful for gaining an intuition of what natural language embeddings are, it's not completely necessary. Especially as the dimensions of your vocabulary and embeddings grow, trying to comprehend them would become an increasingly difficult task."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Recurrent Neural Networks (RNN's) RNN系列模型\n",
    "\n",
    "For our next series of modelling experiments we're going to be using a special kind of neural network called a **Recurrent Neural Network (RNN)**.\n",
    "\n",
    "The premise of an RNN is simple: use information from the past to help you with the future (this is where the term recurrent comes from). In other words, take an input (`X`) and compute an output (`y`) based on all previous inputs.\n",
    "\n",
    "This concept is especially helpful when dealing with sequences such as passages of natural language text (such as our Tweets).\n",
    "\n",
    "For example, when you read this sentence, you take into context the previous words when deciphering the meaning of the current word dog. \n",
    "\n",
    "See what happened there? \n",
    "\n",
    "I put the word \"dog\" at the end which is a valid word but it doesn't make sense in the context of the rest of the sentence.\n",
    "\n",
    "When an RNN looks at a sequence of text (already in numerical form), the patterns it learns are continually updated based on the order of the sequence. \n",
    "\n",
    "For a simple example, take two sentences:\n",
    "1. Massive earthquake last week, no?\n",
    "2. No massive earthquake last week.\n",
    "\n",
    "Both contain exactly the same words but have different meaning. The order of the words determines the meaning (one could argue punctuation marks also dictate the meaning but for simplicity sake, let's stay focused on the words).\n",
    "\n",
    "Recurrent neural networks can be used for a number of sequence-based problems:\n",
    "* **One to one:** one input, one output, such as image classification.\n",
    "* **One to many:** one input, many outputs, such as image captioning (image input, a sequence of text as caption output).\n",
    "* **Many to one:** many inputs, one outputs, such as text classification (classifying a Tweet as real diaster or not real diaster).\n",
    "* **Many to many:** many inputs, many outputs, such as machine translation (translating English to Spanish) or speech to text (audio wave as input, text as output).\n",
    "\n",
    "LSTM、GRU、Bidirectional RNN 等模型\n",
    "\n",
    "When you come across RNN's in the wild, you'll most likely come across variants of the following:\n",
    "* Long short-term memory cells (LSTMs).\n",
    "* Gated recurrent units (GRUs).\n",
    "* Bidirectional RNN's (passes forward and backward along a sequence, left to right and right to left).\n",
    "\n",
    "Going into the details of each these is beyond the scope of this notebook (we're going to focus on using them instead), the main thing you should know for now is that they've proven very effective at modelling sequences.\n",
    "\n",
    "For a deeper understanding of what's happening behind the scenes of the code we're about to write, I'd recommend the following resources:\n",
    "\n",
    "> 📖 **Resources:**\n",
    "> * [MIT Deep Learning Lecture on Recurrent Neural Networks](https://youtu.be/SEnXr6v2ifU) - explains the background of recurrent neural networks and introduces LSTMs.\n",
    "> * [The Unreasonable Effectiveness of Recurrent Neural Networks](http://karpathy.github.io/2015/05/21/rnn-effectiveness/) by Andrej Karpathy - demonstrates the power of RNN's with examples generating various sequences.\n",
    "> * [Understanding LSTMs](https://colah.github.io/posts/2015-08-Understanding-LSTMs/) by Chris Olah - an in-depth (and technical) look at the mechanics of the LSTM cell, possibly the most popular RNN building block.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model 2: LSTM\n",
    "\n",
    "With all this talk of what RNN's are and what they're good for, I'm sure you're eager to build one.\n",
    "\n",
    "We're going to start with an LSTM-powered RNN.\n",
    "\n",
    "To harness the power of the LSTM cell (LSTM cell and LSTM layer are often used interchangably) in TensorFlow, we'll use [`tensorflow.keras.layers.LSTM()`](https://www.tensorflow.org/api_docs/python/tf/keras/layers/LSTM).\n",
    "\n",
    "![](https://raw.githubusercontent.com/mrdbourke/tensorflow-deep-learning/main/images/08-RNN-architecture-coloured-block-edition.png)\n",
    "*Coloured block example of the structure of an recurrent neural network.*\n",
    "\n",
    "Our model is going to take on a very similar structure to `model_1`:\n",
    "\n",
    "```\n",
    "Input (text) -> Tokenize -> Embedding -> Layers -> Output (label probability)\n",
    "```\n",
    "\n",
    "The main difference will be that we're going to add an LSTM layer between our embedding and output.\n",
    "\n",
    "And to make sure we're not getting reusing trained embeddings (this would involve data leakage between models, leading to an uneven comparison later on), we'll create another embedding layer (`model_2_embedding`) for our model. The `text_vectorizer` layer can be reused since it doesn't get updated during training.\n",
    "\n",
    "> 🔑 **Note:** The reason we use a new embedding layer for each model is since the embedding layer is a *learned* representation of words (as numbers), if we were to use the same embedding layer (`embedding_1`) for each model, we'd be mixing what one model learned with the next. And because we want to compare our models later on, starting them with their own embedding layer each time is a better idea."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(None, 15, 128)\n",
      "(None, 64)\n"
     ]
    }
   ],
   "source": [
    "tf.random.set_seed(42)\n",
    "model2_embedding = layers.Embedding(input_dim=max_vocab_length, output_dim=128,embeddings_initializer=\"uniform\", input_length=max_length,name=\"embedding_2\")\n",
    "inputs = layers.Input(shape=(1,),dtype=\"string\")\n",
    "x = text_vectorizer(inputs)\n",
    "x = model2_embedding(x)\n",
    "print(x.shape)\n",
    "# LSTM\n",
    "x = layers.LSTM(64)(x)\n",
    "print(x.shape)\n",
    "outputs = layers.Dense(1,activation=\"sigmoid\")(x)\n",
    "model2 = tf.keras.Model(inputs,outputs,name=\"model_2_LSTM\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_2_LSTM\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_2 (InputLayer)        [(None, 1)]               0         \n",
      "                                                                 \n",
      " text_vectorization_1 (TextV  (None, 15)               0         \n",
      " ectorization)                                                   \n",
      "                                                                 \n",
      " embedding_2 (Embedding)     (None, 15, 128)           1280000   \n",
      "                                                                 \n",
      " lstm (LSTM)                 (None, 64)                49408     \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 1)                 65        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 1,329,473\n",
      "Trainable params: 1,329,473\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model2.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "model2.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looking good! You'll notice a fair few more trainable parameters within our LSTM layer than `model_1`. \n",
    "\n",
    "If you'd like to know where this number comes from, I recommend going through the above resources as well the following on calculating the number of parameters in an LSTM cell:\n",
    "* [Stack Overflow answer to calculate the number of parameters in an LSTM cell](https://stackoverflow.com/questions/38080035/how-to-calculate-the-number-of-parameters-of-an-lstm-network) by Marcin Możejko\n",
    "* [Calculating number of parameters in a LSTM unit and layer](https://medium.com/@priyadarshi.cse/calculating-number-of-parameters-in-a-lstm-unit-layer-7e491978e1e4) by Shridhar Priyadarshi\n",
    "\n",
    "Now our first RNN model's compiled let's fit it to our training data, validating it on the validation data and tracking its training parameters using our TensorBoard callback."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving TensorBoard log files to: model_log/LSTM/20250122-102756\n",
      "Epoch 1/10\n",
      "215/215 [==============================] - 3s 9ms/step - loss: 0.0000e+00 - accuracy: 0.5728 - val_loss: 0.0000e+00 - val_accuracy: 0.5433\n",
      "Epoch 2/10\n",
      "215/215 [==============================] - 2s 8ms/step - loss: 0.0000e+00 - accuracy: 0.5733 - val_loss: 0.0000e+00 - val_accuracy: 0.5433\n",
      "Epoch 3/10\n",
      "215/215 [==============================] - 2s 8ms/step - loss: 0.0000e+00 - accuracy: 0.5733 - val_loss: 0.0000e+00 - val_accuracy: 0.5433\n",
      "Epoch 4/10\n",
      "215/215 [==============================] - 2s 8ms/step - loss: 0.0000e+00 - accuracy: 0.5733 - val_loss: 0.0000e+00 - val_accuracy: 0.5433\n",
      "Epoch 5/10\n",
      "215/215 [==============================] - 2s 8ms/step - loss: 0.0000e+00 - accuracy: 0.5733 - val_loss: 0.0000e+00 - val_accuracy: 0.5433\n",
      "Epoch 6/10\n",
      "215/215 [==============================] - 2s 8ms/step - loss: 0.0000e+00 - accuracy: 0.5733 - val_loss: 0.0000e+00 - val_accuracy: 0.5433\n",
      "Epoch 7/10\n",
      "215/215 [==============================] - 2s 8ms/step - loss: 0.0000e+00 - accuracy: 0.5733 - val_loss: 0.0000e+00 - val_accuracy: 0.5433\n",
      "Epoch 8/10\n",
      "215/215 [==============================] - 2s 8ms/step - loss: 0.0000e+00 - accuracy: 0.5733 - val_loss: 0.0000e+00 - val_accuracy: 0.5433\n",
      "Epoch 9/10\n",
      "215/215 [==============================] - 2s 8ms/step - loss: 0.0000e+00 - accuracy: 0.5733 - val_loss: 0.0000e+00 - val_accuracy: 0.5433\n",
      "Epoch 10/10\n",
      "215/215 [==============================] - 2s 8ms/step - loss: 0.0000e+00 - accuracy: 0.5733 - val_loss: 0.0000e+00 - val_accuracy: 0.5433\n"
     ]
    }
   ],
   "source": [
    "# 大部分的参数都在embedding层，64维度，10000个词，参数是640000\n",
    "model2_history = model2.fit(train_sentences,train_labels,epochs=10,validation_data=(val_sentences,val_labels),\n",
    "                            callbacks=[create_tensorboard_callback(dir_name=SAVE_DIR, \n",
    "                                                                     experiment_name=\"LSTM\")])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "24/24 [==============================] - 0s 2ms/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.]], dtype=float32)"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model2_pred_probs = model2.predict(val_sentences)\n",
    "model2_pred_probs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(762, 1)"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model2_pred_probs.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.]], dtype=float32)"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model2_pred_probs[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(10,), dtype=float32, numpy=array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.], dtype=float32)>"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model2_preds = tf.round(tf.squeeze(model2_pred_probs))\n",
    "model2_preds[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/miniconda3/envs/torchai/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'accuracy': 0.5433070866141733,\n",
       " 'precision': 0.29518259036518074,\n",
       " 'recall': 0.5433070866141733,\n",
       " 'f1': 0.38253254057528524,\n",
       " 'auc': 0.5}"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model2_results = calculate_results(val_labels,model2_preds)\n",
    "model2_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Baseline accuracy: 0.79, New accuracy: 0.54, Difference: -0.25\n",
      "Baseline precision: 0.81, New precision: 0.30, Difference: -0.52\n",
      "Baseline recall: 0.79, New recall: 0.54, Difference: -0.25\n",
      "Baseline f1: 0.79, New f1: 0.38, Difference: -0.40\n",
      "Baseline auc: 0.78, New auc: 0.50, Difference: -0.28\n"
     ]
    }
   ],
   "source": [
    "compare_baseline_to_new_results(baseline_results,model2_results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model 3: GRU\n",
    "\n",
    "Another popular and effective RNN component is the GRU or gated recurrent unit.\n",
    "\n",
    "The GRU cell has similar features to an LSTM cell but has less parameters.\n",
    "\n",
    "> 📖 **Resource:** A full explanation of the GRU cell is beyond the scope of this noteook but I'd suggest the following resources to learn more:\n",
    "* [Gated Recurrent Unit](https://en.wikipedia.org/wiki/Gated_recurrent_unit) Wikipedia page\n",
    "* [Understanding GRU networks](https://towardsdatascience.com/understanding-gru-networks-2ef37df6c9be) by Simeon Kostadinov\n",
    "\n",
    "To use the GRU cell in TensorFlow, we can call the [`tensorflow.keras.layers.GRU()`](https://www.tensorflow.org/api_docs/python/tf/keras/layers/GRU) class.\n",
    "\n",
    "The architecture of the GRU-powered model will follow the same structure we've been using:\n",
    "\n",
    "```\n",
    "Input (text) -> Tokenize -> Embedding -> Layers -> Output (label probability)\n",
    "```\n",
    "\n",
    "Again, the only difference will be the layer(s) we use between the embedding and the output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(None, 15, 128)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(None, 64)\n"
     ]
    }
   ],
   "source": [
    "tf.random.set_seed(42)\n",
    "model3_embedding = layers.Embedding(input_dim=max_vocab_length, output_dim=128,embeddings_initializer=\"uniform\", input_length=max_length,name=\"embedding_3\")\n",
    "inputs = layers.Input(shape=(1,),dtype=\"string\")\n",
    "x = text_vectorizer(inputs)\n",
    "x = model3_embedding(x)\n",
    "print(x.shape)\n",
    "# GRU\n",
    "x = layers.GRU(64)(x)\n",
    "print(x.shape)\n",
    "outputs = layers.Dense(1,activation=\"sigmoid\")(x)\n",
    "model3 = tf.keras.Model(inputs,outputs,name=\"model_3_GRU\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_3_GRU\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_3 (InputLayer)        [(None, 1)]               0         \n",
      "                                                                 \n",
      " text_vectorization_1 (TextV  (None, 15)               0         \n",
      " ectorization)                                                   \n",
      "                                                                 \n",
      " embedding_3 (Embedding)     (None, 15, 128)           1280000   \n",
      "                                                                 \n",
      " gru (GRU)                   (None, 64)                37248     \n",
      "                                                                 \n",
      " dense_2 (Dense)             (None, 1)                 65        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 1,317,313\n",
      "Trainable params: 1,317,313\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model3.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "model3.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving TensorBoard log files to: model_log/LSTM/20250122-102814\n",
      "Epoch 1/10\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "215/215 [==============================] - 3s 10ms/step - loss: 0.0000e+00 - accuracy: 0.5723 - val_loss: 0.0000e+00 - val_accuracy: 0.5433\n",
      "Epoch 2/10\n",
      "215/215 [==============================] - 2s 8ms/step - loss: 0.0000e+00 - accuracy: 0.5733 - val_loss: 0.0000e+00 - val_accuracy: 0.5433\n",
      "Epoch 3/10\n",
      "215/215 [==============================] - 2s 8ms/step - loss: 0.0000e+00 - accuracy: 0.5733 - val_loss: 0.0000e+00 - val_accuracy: 0.5433\n",
      "Epoch 4/10\n",
      "215/215 [==============================] - 2s 8ms/step - loss: 0.0000e+00 - accuracy: 0.5733 - val_loss: 0.0000e+00 - val_accuracy: 0.5433\n",
      "Epoch 5/10\n",
      "215/215 [==============================] - 2s 8ms/step - loss: 0.0000e+00 - accuracy: 0.5733 - val_loss: 0.0000e+00 - val_accuracy: 0.5433\n",
      "Epoch 6/10\n",
      "215/215 [==============================] - 2s 8ms/step - loss: 0.0000e+00 - accuracy: 0.5733 - val_loss: 0.0000e+00 - val_accuracy: 0.5433\n",
      "Epoch 7/10\n",
      "215/215 [==============================] - 2s 8ms/step - loss: 0.0000e+00 - accuracy: 0.5733 - val_loss: 0.0000e+00 - val_accuracy: 0.5433\n",
      "Epoch 8/10\n",
      "215/215 [==============================] - 2s 8ms/step - loss: 0.0000e+00 - accuracy: 0.5733 - val_loss: 0.0000e+00 - val_accuracy: 0.5433\n",
      "Epoch 9/10\n",
      "215/215 [==============================] - 2s 8ms/step - loss: 0.0000e+00 - accuracy: 0.5733 - val_loss: 0.0000e+00 - val_accuracy: 0.5433\n",
      "Epoch 10/10\n",
      "215/215 [==============================] - 2s 8ms/step - loss: 0.0000e+00 - accuracy: 0.5733 - val_loss: 0.0000e+00 - val_accuracy: 0.5433\n"
     ]
    }
   ],
   "source": [
    "model3_history = model3.fit(train_sentences,train_labels,epochs=10,validation_data=(val_sentences,val_labels),\n",
    "                            callbacks=[create_tensorboard_callback(dir_name=SAVE_DIR, \n",
    "                                                                     experiment_name=\"LSTM\")])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "24/24 [==============================] - 0s 2ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/miniconda3/envs/torchai/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'accuracy': 0.5433070866141733,\n",
       " 'precision': 0.29518259036518074,\n",
       " 'recall': 0.5433070866141733,\n",
       " 'f1': 0.38253254057528524,\n",
       " 'auc': 0.5}"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model3_pred_probs = model3.predict(val_sentences)\n",
    "model3_preds = tf.squeeze(tf.round(model3_pred_probs))\n",
    "model3_results = calculate_results(val_labels,\n",
    "                                  model3_preds)\n",
    "model3_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Baseline accuracy: 0.79, New accuracy: 0.54, Difference: -0.25\n",
      "Baseline precision: 0.81, New precision: 0.30, Difference: -0.52\n",
      "Baseline recall: 0.79, New recall: 0.54, Difference: -0.25\n",
      "Baseline f1: 0.79, New f1: 0.38, Difference: -0.40\n",
      "Baseline auc: 0.78, New auc: 0.50, Difference: -0.28\n"
     ]
    }
   ],
   "source": [
    "compare_baseline_to_new_results(baseline_results,model3_results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model 4: Bidirectonal RNN model \n",
    "\n",
    "Look at us go! We've already built two RNN's with GRU and LSTM cells. Now we're going to look into another kind of RNN, the bidirectional RNN.\n",
    "\n",
    "A standard RNN will process a sequence from left to right, where as a bidirectional RNN will process the sequence from left to right and then again from right to left.\n",
    "\n",
    "Intuitively, this can be thought of as if you were reading a sentence for the first time in the normal fashion (left to right) but for some reason it didn't make sense so you traverse back through the words and go back over them again (right to left).\n",
    "\n",
    "In practice, many sequence models often see and improvement in performance when using bidirectional RNN's.\n",
    "\n",
    "However, this improvement in performance often comes at the cost of longer training times and increased model parameters (since the model goes left to right and right to left, the number of trainable parameters doubles).\n",
    "\n",
    "Okay enough talk, let's build a bidirectional RNN.\n",
    "\n",
    "Once again, TensorFlow helps us out by providing the [`tensorflow.keras.layers.Bidirectional`](https://www.tensorflow.org/api_docs/python/tf/keras/layers/Bidirectional) class. We can use the `Bidirectional` class to wrap our existing RNNs, instantly making them bidirectional."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_4_Bidirectional\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_4 (InputLayer)        [(None, 1)]               0         \n",
      "                                                                 \n",
      " text_vectorization_1 (TextV  (None, 15)               0         \n",
      " ectorization)                                                   \n",
      "                                                                 \n",
      " embedding_4 (Embedding)     (None, 15, 128)           1280000   \n",
      "                                                                 \n",
      " bidirectional (Bidirectiona  (None, 128)              98816     \n",
      " l)                                                              \n",
      "                                                                 \n",
      " dense_3 (Dense)             (None, 1)                 129       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 1,378,945\n",
      "Trainable params: 1,378,945\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "tf.random.set_seed(42)\n",
    "model4_embedding = layers.Embedding(input_dim=max_vocab_length, output_dim=128,embeddings_initializer=\"uniform\", input_length=max_length,name=\"embedding_4\")\n",
    "inputs = layers.Input(shape=(1,),dtype=\"string\")\n",
    "x = text_vectorizer(inputs)\n",
    "x = model4_embedding(x)\n",
    "# GRU\n",
    "x = layers.Bidirectional(layers.LSTM(64))(x)\n",
    "outputs = layers.Dense(1,activation=\"sigmoid\")(x)\n",
    "model4 = tf.keras.Model(inputs,outputs,name=\"model_4_Bidirectional\")\n",
    "\n",
    "model4.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "model4.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving TensorBoard log files to: model_log/Bidirectional/20250122-102834\n",
      "Epoch 1/10\n",
      "215/215 [==============================] - 5s 12ms/step - loss: 0.0000e+00 - accuracy: 0.5735 - val_loss: 0.0000e+00 - val_accuracy: 0.5433\n",
      "Epoch 2/10\n",
      "215/215 [==============================] - 2s 9ms/step - loss: 0.0000e+00 - accuracy: 0.5733 - val_loss: 0.0000e+00 - val_accuracy: 0.5433\n",
      "Epoch 3/10\n",
      "215/215 [==============================] - 2s 9ms/step - loss: 0.0000e+00 - accuracy: 0.5733 - val_loss: 0.0000e+00 - val_accuracy: 0.5433\n",
      "Epoch 4/10\n",
      "215/215 [==============================] - 2s 9ms/step - loss: 0.0000e+00 - accuracy: 0.5733 - val_loss: 0.0000e+00 - val_accuracy: 0.5433\n",
      "Epoch 5/10\n",
      "215/215 [==============================] - 2s 9ms/step - loss: 0.0000e+00 - accuracy: 0.5733 - val_loss: 0.0000e+00 - val_accuracy: 0.5433\n",
      "Epoch 6/10\n",
      "215/215 [==============================] - 2s 9ms/step - loss: 0.0000e+00 - accuracy: 0.5733 - val_loss: 0.0000e+00 - val_accuracy: 0.5433\n",
      "Epoch 7/10\n",
      "215/215 [==============================] - 2s 9ms/step - loss: 0.0000e+00 - accuracy: 0.5733 - val_loss: 0.0000e+00 - val_accuracy: 0.5433\n",
      "Epoch 8/10\n",
      "215/215 [==============================] - 2s 9ms/step - loss: 0.0000e+00 - accuracy: 0.5733 - val_loss: 0.0000e+00 - val_accuracy: 0.5433\n",
      "Epoch 9/10\n",
      "215/215 [==============================] - 2s 9ms/step - loss: 0.0000e+00 - accuracy: 0.5733 - val_loss: 0.0000e+00 - val_accuracy: 0.5433\n",
      "Epoch 10/10\n",
      "215/215 [==============================] - 2s 9ms/step - loss: 0.0000e+00 - accuracy: 0.5733 - val_loss: 0.0000e+00 - val_accuracy: 0.5433\n"
     ]
    }
   ],
   "source": [
    "model4_history = model4.fit(train_sentences,train_labels,epochs=10,validation_data=(val_sentences,val_labels),\n",
    "                            callbacks=[create_tensorboard_callback(dir_name=SAVE_DIR, \n",
    "                                                                     experiment_name=\"Bidirectional\")])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "24/24 [==============================] - 1s 3ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/miniconda3/envs/torchai/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'accuracy': 0.5433070866141733,\n",
       " 'precision': 0.29518259036518074,\n",
       " 'recall': 0.5433070866141733,\n",
       " 'f1': 0.38253254057528524,\n",
       " 'auc': 0.5}"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model4_pred_probs = model4.predict(val_sentences)\n",
    "model4_preds = tf.squeeze(tf.round(model4_pred_probs))\n",
    "model4_results = calculate_results(val_labels,\n",
    "                                  model4_preds)\n",
    "model4_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Baseline accuracy: 0.79, New accuracy: 0.54, Difference: -0.25\n",
      "Baseline precision: 0.81, New precision: 0.30, Difference: -0.52\n",
      "Baseline recall: 0.79, New recall: 0.54, Difference: -0.25\n",
      "Baseline f1: 0.79, New f1: 0.38, Difference: -0.40\n",
      "Baseline auc: 0.78, New auc: 0.50, Difference: -0.28\n"
     ]
    }
   ],
   "source": [
    "compare_baseline_to_new_results(baseline_results,model4_results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convolutional Neural Networks for Text  CNN 应用在文本\n",
    "\n",
    "You might've used convolutional neural networks (CNNs) for images before but they can also be used for sequences.\n",
    "\n",
    "The main difference between using CNNs for images and sequences is the shape of the data. Images come in 2-dimensions (height x width) where as sequences are often 1-dimensional (a string of text).\n",
    "\n",
    "So to use CNNs with sequences, we use a 1-dimensional convolution instead of a 2-dimensional convolution.\n",
    "\n",
    "A typical CNN architecture for sequences will look like the following: \n",
    "\n",
    "```\n",
    "Inputs (text) -> Tokenization -> Embedding -> Layers -> Outputs (class probabilities)\n",
    "```\n",
    "\n",
    "You might be thinking \"that just looks like the architecture layout we've been using for the other models...\"\n",
    "\n",
    "And you'd be right.\n",
    "\n",
    "The difference again is in the layers component. Instead of using an LSTM or GRU cell, we're going to use a [`tensorflow.keras.layers.Conv1D()`](https://www.tensorflow.org/api_docs/python/tf/keras/layers/Conv1D) layer followed by a [`tensorflow.keras.layers.GlobablMaxPool1D()`](https://www.tensorflow.org/api_docs/python/tf/keras/layers/GlobalMaxPool1D) layer.\n",
    "\n",
    "> 📖 **Resource:** The intuition here is explained succinctly in the paper [*Understanding Convolutional Neural Networks for Text Classification*](https://www.aclweb.org/anthology/W18-5408.pdf), where they state that CNNs classify text through the following steps:\n",
    "1. 1-dimensional convolving filters are used as ngram detectors, each filter specializing in a closely-related family of ngrams (an ngram is a collection of n-words, for example, an ngram of 5 might result in \"hello, my name is Daniel\").\n",
    "2. Max-pooling over time extracts the relevant ngrams for making a decision.\n",
    "3. The rest of the network classifies the text based on this information.\n",
    "\n",
    "> \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model 5: Conv1D\n",
    "\n",
    "Before we build a full 1-dimensional CNN model, let's see a 1-dimensional convolutional layer (also called a **temporal convolution**) in action.\n",
    "\n",
    "We'll first create an embedding of a sample of text and experiment passing it through a `Conv1D()` layer and `GlobalMaxPool1D()` layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(TensorShape([1, 15, 64]), TensorShape([1, 11, 32]), TensorShape([1, 32]))"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embedding_test = embedding(text_vectorizer([\"Hello world, how are you doing?\"]))\n",
    "conv1d = layers.Conv1D(filters=32, kernel_size=5, activation=\"relu\")\n",
    "conv1d_output = conv1d(embedding_test)\n",
    "max_pool = layers.GlobalMaxPool1D()\n",
    "max_pool_output = max_pool(conv1d_output)\n",
    "embedding_test.shape, conv1d_output.shape, max_pool_output.shape    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(<tf.Tensor: shape=(1, 15, 64), dtype=float32, numpy=\n",
       " array([[[ 0.04927063,  0.0455859 , -0.03696341, -0.02716763,\n",
       "          -0.04393656, -0.02666598, -0.02628189, -0.02066438,\n",
       "          -0.00862695, -0.02994786,  0.01083384,  0.00841267,\n",
       "          -0.04106066, -0.00559476, -0.01404158,  0.02035085,\n",
       "          -0.0247621 , -0.01744848, -0.01688452,  0.04168949,\n",
       "          -0.00381372,  0.02718107,  0.02218331,  0.02043347,\n",
       "          -0.02060524, -0.0454086 , -0.03620398,  0.04254222,\n",
       "          -0.01027236,  0.0498293 , -0.05122475,  0.01015221,\n",
       "          -0.04058774,  0.02962366, -0.03387579, -0.01199133,\n",
       "          -0.04449604,  0.00624861, -0.05035713,  0.03105379,\n",
       "          -0.04941126, -0.0392956 , -0.02791278, -0.03757041,\n",
       "           0.02001321,  0.02441015,  0.04983385,  0.00505213,\n",
       "           0.03252229, -0.02273783,  0.02550583, -0.04352991,\n",
       "           0.03043619,  0.00896313, -0.03817467, -0.00248338,\n",
       "          -0.02504817,  0.02785878,  0.0046658 , -0.041079  ,\n",
       "          -0.00084155, -0.00683904, -0.04560264, -0.01841895],\n",
       "         [-0.0501423 ,  0.03019001,  0.0633471 ,  0.01169866,\n",
       "           0.03061692,  0.04656729,  0.03675785, -0.06736621,\n",
       "          -0.00419796,  0.05971726,  0.09424177, -0.04283668,\n",
       "           0.02853829, -0.07273112, -0.06925429, -0.05550537,\n",
       "           0.04630151,  0.10262983, -0.08959711, -0.01010133,\n",
       "          -0.08549505, -0.07410501, -0.03423049,  0.10246175,\n",
       "          -0.08327053, -0.02648197, -0.00483097, -0.00062758,\n",
       "           0.06675572, -0.09538288,  0.06859855, -0.07741411,\n",
       "           0.06360269, -0.00777825,  0.06824356,  0.05240895,\n",
       "           0.02701472,  0.09887393,  0.01604744,  0.02354276,\n",
       "          -0.10371221,  0.07955129, -0.07544108,  0.01834613,\n",
       "          -0.05326008,  0.0396131 , -0.07613777,  0.07508805,\n",
       "          -0.07914715,  0.04435478,  0.03557305, -0.04050968,\n",
       "          -0.03518785, -0.05143929,  0.00733971,  0.03377681,\n",
       "          -0.02497379,  0.05666274, -0.04817802, -0.07699569,\n",
       "          -0.04647508,  0.04282086,  0.06069626, -0.02836328],\n",
       "         [-0.06608857,  0.04833829,  0.05975079,  0.08511666,\n",
       "           0.01538939,  0.02905951,  0.05721397, -0.04223058,\n",
       "          -0.03728984,  0.02015097,  0.04021926, -0.07585999,\n",
       "           0.0456101 , -0.09379292, -0.01695416, -0.04459071,\n",
       "           0.08852336,  0.06158165, -0.03893588, -0.04166451,\n",
       "          -0.07416344, -0.05544537, -0.05555613,  0.05924354,\n",
       "          -0.03625574, -0.04571952, -0.04127111, -0.02197336,\n",
       "           0.00631908, -0.10798879,  0.02785105, -0.0181415 ,\n",
       "           0.03489139, -0.03810434,  0.02429722,  0.09099648,\n",
       "           0.05309036,  0.01882072,  0.05678574,  0.01605315,\n",
       "          -0.04160702,  0.02794815, -0.06292722,  0.08427227,\n",
       "          -0.05816145,  0.09832815, -0.0308572 ,  0.09217211,\n",
       "          -0.09695692,  0.01029115,  0.0967782 , -0.08229979,\n",
       "          -0.04966782, -0.07119102,  0.02772382,  0.03243698,\n",
       "          -0.07917204,  0.00994168, -0.08491247, -0.080271  ,\n",
       "          -0.00096332,  0.01507586,  0.08468387, -0.08258237],\n",
       "         [ 0.08685861, -0.07108815, -0.01069177, -0.06388761,\n",
       "          -0.03401541, -0.03632584, -0.05354851,  0.0693936 ,\n",
       "           0.07452063, -0.07702962, -0.04094275,  0.04354735,\n",
       "          -0.0505449 ,  0.04971958,  0.08241434,  0.06871824,\n",
       "          -0.0521414 , -0.05765101,  0.06907185,  0.05356424,\n",
       "           0.06185675,  0.01128061,  0.01246401, -0.03091753,\n",
       "           0.02979126,  0.03773411,  0.05225956,  0.04873374,\n",
       "          -0.02574352,  0.06048571, -0.03209682,  0.02879446,\n",
       "          -0.01731685,  0.05584861, -0.04581894, -0.05169837,\n",
       "          -0.05487807, -0.0183157 , -0.0299748 , -0.03954571,\n",
       "           0.09242117, -0.06954374, -0.00634026, -0.0898154 ,\n",
       "          -0.0042913 , -0.04268746,  0.00706874, -0.07351863,\n",
       "           0.08096942, -0.08495413, -0.04215693,  0.01694866,\n",
       "           0.01935824,  0.02956953,  0.00548653, -0.08232521,\n",
       "           0.09232419,  0.00312534,  0.0004148 ,  0.09000874,\n",
       "           0.08470949, -0.0686337 , -0.08043369,  0.0296853 ],\n",
       "         [-0.24599786,  0.22188127,  0.25386748,  0.1983497 ,\n",
       "           0.1909409 ,  0.25459948,  0.2533496 , -0.23188576,\n",
       "          -0.14915025,  0.22797143,  0.30611277, -0.16457847,\n",
       "           0.21105607, -0.19784951, -0.17970009, -0.25259003,\n",
       "           0.23473206,  0.20461111, -0.24622859, -0.23023842,\n",
       "          -0.19921798, -0.2166129 , -0.23424165,  0.2087573 ,\n",
       "          -0.16066447, -0.23376617, -0.1611376 , -0.23023799,\n",
       "           0.26306728, -0.24289404,  0.21815394, -0.19795707,\n",
       "           0.15775762, -0.16644554,  0.1921959 ,  0.14747168,\n",
       "           0.16044809,  0.18171906,  0.20018338,  0.22321106,\n",
       "          -0.25522077,  0.18508929, -0.19932237,  0.19075269,\n",
       "          -0.22574973,  0.23645113, -0.28806993,  0.2386013 ,\n",
       "          -0.14635661,  0.18780315,  0.23326388, -0.19793415,\n",
       "          -0.19294322, -0.2222499 ,  0.17438874,  0.17015922,\n",
       "          -0.17519352,  0.25260356, -0.23719248, -0.25678658,\n",
       "          -0.14218934,  0.19929509,  0.21008399, -0.16330215],\n",
       "         [-0.08883369,  0.07944579,  0.06711036,  0.1268568 ,\n",
       "           0.01834131,  0.10364398,  0.06935211, -0.06875733,\n",
       "          -0.05688768,  0.05117618,  0.11315591, -0.06274462,\n",
       "           0.08237036, -0.02963974, -0.04368579, -0.07439536,\n",
       "           0.09830509,  0.11026343, -0.04866317, -0.09650579,\n",
       "          -0.04997461, -0.02572115, -0.08828952,  0.10878482,\n",
       "          -0.05257735, -0.04168154, -0.03163655, -0.06715421,\n",
       "           0.05016777, -0.05101183,  0.05466929, -0.0763237 ,\n",
       "           0.05024358, -0.1015909 ,  0.03864252,  0.11401629,\n",
       "           0.07601546,  0.08753467,  0.04369689,  0.08562659,\n",
       "          -0.05664855,  0.06359611, -0.10232902,  0.03922981,\n",
       "          -0.07020108,  0.11079656, -0.07401514,  0.05948496,\n",
       "          -0.10426619,  0.08541197,  0.11988153, -0.07177594,\n",
       "          -0.04918267, -0.03376859,  0.05610056,  0.09916133,\n",
       "          -0.05542071,  0.11190199, -0.089178  , -0.05159251,\n",
       "          -0.08061475,  0.07563035,  0.10958522, -0.02597179],\n",
       "         [-0.01238398,  0.03976315,  0.05967141, -0.02696273,\n",
       "           0.06098639, -0.01741344,  0.01368975, -0.05788472,\n",
       "          -0.01750314,  0.00552021,  0.05821462, -0.04472128,\n",
       "           0.07289322, -0.01087582,  0.01526838, -0.05243922,\n",
       "           0.05566694,  0.06361009,  0.02286058,  0.02274077,\n",
       "          -0.03174298, -0.03287387,  0.00369497, -0.00540758,\n",
       "          -0.00559367, -0.03582225,  0.00117227, -0.02324406,\n",
       "           0.03628455,  0.00424401,  0.06826916, -0.07131534,\n",
       "           0.06160784,  0.01796041,  0.07056159,  0.04898191,\n",
       "          -0.01856244,  0.04474765, -0.00035899,  0.06050973,\n",
       "          -0.01510662, -0.00526253,  0.01619131,  0.06646373,\n",
       "           0.00275263,  0.00050844,  0.00198431,  0.06140959,\n",
       "          -0.01247452,  0.06644641,  0.05833625, -0.04112704,\n",
       "          -0.03432414, -0.02272543, -0.01147988,  0.0692389 ,\n",
       "          -0.06855379,  0.02698756, -0.05621039,  0.01445469,\n",
       "          -0.04717525,  0.00147118, -0.00120488,  0.00906573],\n",
       "         [-0.01238398,  0.03976315,  0.05967141, -0.02696273,\n",
       "           0.06098639, -0.01741344,  0.01368975, -0.05788472,\n",
       "          -0.01750314,  0.00552021,  0.05821462, -0.04472128,\n",
       "           0.07289322, -0.01087582,  0.01526838, -0.05243922,\n",
       "           0.05566694,  0.06361009,  0.02286058,  0.02274077,\n",
       "          -0.03174298, -0.03287387,  0.00369497, -0.00540758,\n",
       "          -0.00559367, -0.03582225,  0.00117227, -0.02324406,\n",
       "           0.03628455,  0.00424401,  0.06826916, -0.07131534,\n",
       "           0.06160784,  0.01796041,  0.07056159,  0.04898191,\n",
       "          -0.01856244,  0.04474765, -0.00035899,  0.06050973,\n",
       "          -0.01510662, -0.00526253,  0.01619131,  0.06646373,\n",
       "           0.00275263,  0.00050844,  0.00198431,  0.06140959,\n",
       "          -0.01247452,  0.06644641,  0.05833625, -0.04112704,\n",
       "          -0.03432414, -0.02272543, -0.01147988,  0.0692389 ,\n",
       "          -0.06855379,  0.02698756, -0.05621039,  0.01445469,\n",
       "          -0.04717525,  0.00147118, -0.00120488,  0.00906573],\n",
       "         [-0.01238398,  0.03976315,  0.05967141, -0.02696273,\n",
       "           0.06098639, -0.01741344,  0.01368975, -0.05788472,\n",
       "          -0.01750314,  0.00552021,  0.05821462, -0.04472128,\n",
       "           0.07289322, -0.01087582,  0.01526838, -0.05243922,\n",
       "           0.05566694,  0.06361009,  0.02286058,  0.02274077,\n",
       "          -0.03174298, -0.03287387,  0.00369497, -0.00540758,\n",
       "          -0.00559367, -0.03582225,  0.00117227, -0.02324406,\n",
       "           0.03628455,  0.00424401,  0.06826916, -0.07131534,\n",
       "           0.06160784,  0.01796041,  0.07056159,  0.04898191,\n",
       "          -0.01856244,  0.04474765, -0.00035899,  0.06050973,\n",
       "          -0.01510662, -0.00526253,  0.01619131,  0.06646373,\n",
       "           0.00275263,  0.00050844,  0.00198431,  0.06140959,\n",
       "          -0.01247452,  0.06644641,  0.05833625, -0.04112704,\n",
       "          -0.03432414, -0.02272543, -0.01147988,  0.0692389 ,\n",
       "          -0.06855379,  0.02698756, -0.05621039,  0.01445469,\n",
       "          -0.04717525,  0.00147118, -0.00120488,  0.00906573],\n",
       "         [-0.01238398,  0.03976315,  0.05967141, -0.02696273,\n",
       "           0.06098639, -0.01741344,  0.01368975, -0.05788472,\n",
       "          -0.01750314,  0.00552021,  0.05821462, -0.04472128,\n",
       "           0.07289322, -0.01087582,  0.01526838, -0.05243922,\n",
       "           0.05566694,  0.06361009,  0.02286058,  0.02274077,\n",
       "          -0.03174298, -0.03287387,  0.00369497, -0.00540758,\n",
       "          -0.00559367, -0.03582225,  0.00117227, -0.02324406,\n",
       "           0.03628455,  0.00424401,  0.06826916, -0.07131534,\n",
       "           0.06160784,  0.01796041,  0.07056159,  0.04898191,\n",
       "          -0.01856244,  0.04474765, -0.00035899,  0.06050973,\n",
       "          -0.01510662, -0.00526253,  0.01619131,  0.06646373,\n",
       "           0.00275263,  0.00050844,  0.00198431,  0.06140959,\n",
       "          -0.01247452,  0.06644641,  0.05833625, -0.04112704,\n",
       "          -0.03432414, -0.02272543, -0.01147988,  0.0692389 ,\n",
       "          -0.06855379,  0.02698756, -0.05621039,  0.01445469,\n",
       "          -0.04717525,  0.00147118, -0.00120488,  0.00906573],\n",
       "         [-0.01238398,  0.03976315,  0.05967141, -0.02696273,\n",
       "           0.06098639, -0.01741344,  0.01368975, -0.05788472,\n",
       "          -0.01750314,  0.00552021,  0.05821462, -0.04472128,\n",
       "           0.07289322, -0.01087582,  0.01526838, -0.05243922,\n",
       "           0.05566694,  0.06361009,  0.02286058,  0.02274077,\n",
       "          -0.03174298, -0.03287387,  0.00369497, -0.00540758,\n",
       "          -0.00559367, -0.03582225,  0.00117227, -0.02324406,\n",
       "           0.03628455,  0.00424401,  0.06826916, -0.07131534,\n",
       "           0.06160784,  0.01796041,  0.07056159,  0.04898191,\n",
       "          -0.01856244,  0.04474765, -0.00035899,  0.06050973,\n",
       "          -0.01510662, -0.00526253,  0.01619131,  0.06646373,\n",
       "           0.00275263,  0.00050844,  0.00198431,  0.06140959,\n",
       "          -0.01247452,  0.06644641,  0.05833625, -0.04112704,\n",
       "          -0.03432414, -0.02272543, -0.01147988,  0.0692389 ,\n",
       "          -0.06855379,  0.02698756, -0.05621039,  0.01445469,\n",
       "          -0.04717525,  0.00147118, -0.00120488,  0.00906573],\n",
       "         [-0.01238398,  0.03976315,  0.05967141, -0.02696273,\n",
       "           0.06098639, -0.01741344,  0.01368975, -0.05788472,\n",
       "          -0.01750314,  0.00552021,  0.05821462, -0.04472128,\n",
       "           0.07289322, -0.01087582,  0.01526838, -0.05243922,\n",
       "           0.05566694,  0.06361009,  0.02286058,  0.02274077,\n",
       "          -0.03174298, -0.03287387,  0.00369497, -0.00540758,\n",
       "          -0.00559367, -0.03582225,  0.00117227, -0.02324406,\n",
       "           0.03628455,  0.00424401,  0.06826916, -0.07131534,\n",
       "           0.06160784,  0.01796041,  0.07056159,  0.04898191,\n",
       "          -0.01856244,  0.04474765, -0.00035899,  0.06050973,\n",
       "          -0.01510662, -0.00526253,  0.01619131,  0.06646373,\n",
       "           0.00275263,  0.00050844,  0.00198431,  0.06140959,\n",
       "          -0.01247452,  0.06644641,  0.05833625, -0.04112704,\n",
       "          -0.03432414, -0.02272543, -0.01147988,  0.0692389 ,\n",
       "          -0.06855379,  0.02698756, -0.05621039,  0.01445469,\n",
       "          -0.04717525,  0.00147118, -0.00120488,  0.00906573],\n",
       "         [-0.01238398,  0.03976315,  0.05967141, -0.02696273,\n",
       "           0.06098639, -0.01741344,  0.01368975, -0.05788472,\n",
       "          -0.01750314,  0.00552021,  0.05821462, -0.04472128,\n",
       "           0.07289322, -0.01087582,  0.01526838, -0.05243922,\n",
       "           0.05566694,  0.06361009,  0.02286058,  0.02274077,\n",
       "          -0.03174298, -0.03287387,  0.00369497, -0.00540758,\n",
       "          -0.00559367, -0.03582225,  0.00117227, -0.02324406,\n",
       "           0.03628455,  0.00424401,  0.06826916, -0.07131534,\n",
       "           0.06160784,  0.01796041,  0.07056159,  0.04898191,\n",
       "          -0.01856244,  0.04474765, -0.00035899,  0.06050973,\n",
       "          -0.01510662, -0.00526253,  0.01619131,  0.06646373,\n",
       "           0.00275263,  0.00050844,  0.00198431,  0.06140959,\n",
       "          -0.01247452,  0.06644641,  0.05833625, -0.04112704,\n",
       "          -0.03432414, -0.02272543, -0.01147988,  0.0692389 ,\n",
       "          -0.06855379,  0.02698756, -0.05621039,  0.01445469,\n",
       "          -0.04717525,  0.00147118, -0.00120488,  0.00906573],\n",
       "         [-0.01238398,  0.03976315,  0.05967141, -0.02696273,\n",
       "           0.06098639, -0.01741344,  0.01368975, -0.05788472,\n",
       "          -0.01750314,  0.00552021,  0.05821462, -0.04472128,\n",
       "           0.07289322, -0.01087582,  0.01526838, -0.05243922,\n",
       "           0.05566694,  0.06361009,  0.02286058,  0.02274077,\n",
       "          -0.03174298, -0.03287387,  0.00369497, -0.00540758,\n",
       "          -0.00559367, -0.03582225,  0.00117227, -0.02324406,\n",
       "           0.03628455,  0.00424401,  0.06826916, -0.07131534,\n",
       "           0.06160784,  0.01796041,  0.07056159,  0.04898191,\n",
       "          -0.01856244,  0.04474765, -0.00035899,  0.06050973,\n",
       "          -0.01510662, -0.00526253,  0.01619131,  0.06646373,\n",
       "           0.00275263,  0.00050844,  0.00198431,  0.06140959,\n",
       "          -0.01247452,  0.06644641,  0.05833625, -0.04112704,\n",
       "          -0.03432414, -0.02272543, -0.01147988,  0.0692389 ,\n",
       "          -0.06855379,  0.02698756, -0.05621039,  0.01445469,\n",
       "          -0.04717525,  0.00147118, -0.00120488,  0.00906573],\n",
       "         [-0.01238398,  0.03976315,  0.05967141, -0.02696273,\n",
       "           0.06098639, -0.01741344,  0.01368975, -0.05788472,\n",
       "          -0.01750314,  0.00552021,  0.05821462, -0.04472128,\n",
       "           0.07289322, -0.01087582,  0.01526838, -0.05243922,\n",
       "           0.05566694,  0.06361009,  0.02286058,  0.02274077,\n",
       "          -0.03174298, -0.03287387,  0.00369497, -0.00540758,\n",
       "          -0.00559367, -0.03582225,  0.00117227, -0.02324406,\n",
       "           0.03628455,  0.00424401,  0.06826916, -0.07131534,\n",
       "           0.06160784,  0.01796041,  0.07056159,  0.04898191,\n",
       "          -0.01856244,  0.04474765, -0.00035899,  0.06050973,\n",
       "          -0.01510662, -0.00526253,  0.01619131,  0.06646373,\n",
       "           0.00275263,  0.00050844,  0.00198431,  0.06140959,\n",
       "          -0.01247452,  0.06644641,  0.05833625, -0.04112704,\n",
       "          -0.03432414, -0.02272543, -0.01147988,  0.0692389 ,\n",
       "          -0.06855379,  0.02698756, -0.05621039,  0.01445469,\n",
       "          -0.04717525,  0.00147118, -0.00120488,  0.00906573]]],\n",
       "       dtype=float32)>,\n",
       " <tf.Tensor: shape=(1, 11, 32), dtype=float32, numpy=\n",
       " array([[[0.10656257, 0.        , 0.16529739, 0.        , 0.        ,\n",
       "          0.        , 0.10975615, 0.05663275, 0.        , 0.00456921,\n",
       "          0.04470031, 0.        , 0.        , 0.08659957, 0.07581019,\n",
       "          0.        , 0.06656463, 0.        , 0.09920129, 0.17659087,\n",
       "          0.1477437 , 0.19946031, 0.20765842, 0.        , 0.        ,\n",
       "          0.18428068, 0.09486813, 0.        , 0.07964838, 0.13551165,\n",
       "          0.05447463, 0.        ],\n",
       "         [0.01269607, 0.        , 0.        , 0.        , 0.0040381 ,\n",
       "          0.02708118, 0.        , 0.22535428, 0.00424695, 0.04828433,\n",
       "          0.        , 0.        , 0.03814455, 0.        , 0.14670545,\n",
       "          0.03654398, 0.        , 0.06342448, 0.12459818, 0.        ,\n",
       "          0.        , 0.        , 0.        , 0.04231742, 0.12183037,\n",
       "          0.        , 0.        , 0.13833636, 0.        , 0.00513832,\n",
       "          0.        , 0.03152736],\n",
       "         [0.11853225, 0.        , 0.        , 0.07686109, 0.        ,\n",
       "          0.        , 0.        , 0.075513  , 0.18268938, 0.00564187,\n",
       "          0.        , 0.        , 0.00458082, 0.        , 0.        ,\n",
       "          0.        , 0.        , 0.        , 0.05012145, 0.        ,\n",
       "          0.11562674, 0.        , 0.        , 0.        , 0.10725743,\n",
       "          0.        , 0.08524875, 0.        , 0.        , 0.        ,\n",
       "          0.        , 0.        ],\n",
       "         [0.        , 0.        , 0.        , 0.1657491 , 0.        ,\n",
       "          0.        , 0.        , 0.41200864, 0.        , 0.20144065,\n",
       "          0.08636961, 0.        , 0.1429001 , 0.17692982, 0.        ,\n",
       "          0.        , 0.01746073, 0.        , 0.17520806, 0.03414904,\n",
       "          0.        , 0.06257293, 0.02290597, 0.        , 0.12633531,\n",
       "          0.        , 0.07127059, 0.        , 0.12245344, 0.        ,\n",
       "          0.14450394, 0.0191681 ],\n",
       "         [0.06284633, 0.08809745, 0.        , 0.        , 0.        ,\n",
       "          0.        , 0.        , 0.27420706, 0.        , 0.        ,\n",
       "          0.        , 0.        , 0.17699243, 0.01451011, 0.11587269,\n",
       "          0.        , 0.        , 0.        , 0.14923802, 0.        ,\n",
       "          0.        , 0.        , 0.        , 0.        , 0.11583232,\n",
       "          0.05761969, 0.05178833, 0.06601198, 0.18407002, 0.        ,\n",
       "          0.        , 0.11511374],\n",
       "         [0.02792178, 0.04241316, 0.        , 0.        , 0.        ,\n",
       "          0.        , 0.        , 0.13321412, 0.        , 0.        ,\n",
       "          0.        , 0.        , 0.08211318, 0.01219402, 0.        ,\n",
       "          0.        , 0.        , 0.        , 0.058261  , 0.        ,\n",
       "          0.        , 0.00661299, 0.        , 0.        , 0.02815942,\n",
       "          0.01756688, 0.02677479, 0.01145679, 0.        , 0.        ,\n",
       "          0.        , 0.06099168],\n",
       "         [0.        , 0.        , 0.        , 0.02179445, 0.        ,\n",
       "          0.        , 0.        , 0.08029231, 0.        , 0.02195058,\n",
       "          0.        , 0.        , 0.05389513, 0.04268925, 0.        ,\n",
       "          0.        , 0.        , 0.        , 0.04827936, 0.        ,\n",
       "          0.        , 0.00948853, 0.        , 0.        , 0.04212833,\n",
       "          0.        , 0.05541012, 0.        , 0.        , 0.        ,\n",
       "          0.        , 0.02252548],\n",
       "         [0.        , 0.        , 0.        , 0.02179445, 0.        ,\n",
       "          0.        , 0.        , 0.08029231, 0.        , 0.02195058,\n",
       "          0.        , 0.        , 0.05389513, 0.04268925, 0.        ,\n",
       "          0.        , 0.        , 0.        , 0.04827936, 0.        ,\n",
       "          0.        , 0.00948853, 0.        , 0.        , 0.04212833,\n",
       "          0.        , 0.05541012, 0.        , 0.        , 0.        ,\n",
       "          0.        , 0.02252548],\n",
       "         [0.        , 0.        , 0.        , 0.02179445, 0.        ,\n",
       "          0.        , 0.        , 0.08029231, 0.        , 0.02195058,\n",
       "          0.        , 0.        , 0.05389513, 0.04268925, 0.        ,\n",
       "          0.        , 0.        , 0.        , 0.04827936, 0.        ,\n",
       "          0.        , 0.00948853, 0.        , 0.        , 0.04212833,\n",
       "          0.        , 0.05541012, 0.        , 0.        , 0.        ,\n",
       "          0.        , 0.02252548],\n",
       "         [0.        , 0.        , 0.        , 0.02179445, 0.        ,\n",
       "          0.        , 0.        , 0.08029231, 0.        , 0.02195058,\n",
       "          0.        , 0.        , 0.05389513, 0.04268925, 0.        ,\n",
       "          0.        , 0.        , 0.        , 0.04827936, 0.        ,\n",
       "          0.        , 0.00948853, 0.        , 0.        , 0.04212833,\n",
       "          0.        , 0.05541012, 0.        , 0.        , 0.        ,\n",
       "          0.        , 0.02252548],\n",
       "         [0.        , 0.        , 0.        , 0.02179445, 0.        ,\n",
       "          0.        , 0.        , 0.08029231, 0.        , 0.02195058,\n",
       "          0.        , 0.        , 0.05389513, 0.04268925, 0.        ,\n",
       "          0.        , 0.        , 0.        , 0.04827936, 0.        ,\n",
       "          0.        , 0.00948853, 0.        , 0.        , 0.04212833,\n",
       "          0.        , 0.05541012, 0.        , 0.        , 0.        ,\n",
       "          0.        , 0.02252548]]], dtype=float32)>,\n",
       " <tf.Tensor: shape=(1, 32), dtype=float32, numpy=\n",
       " array([[0.11853225, 0.08809745, 0.16529739, 0.1657491 , 0.0040381 ,\n",
       "         0.02708118, 0.10975615, 0.41200864, 0.18268938, 0.20144065,\n",
       "         0.08636961, 0.        , 0.17699243, 0.17692982, 0.14670545,\n",
       "         0.03654398, 0.06656463, 0.06342448, 0.17520806, 0.17659087,\n",
       "         0.1477437 , 0.19946031, 0.20765842, 0.04231742, 0.12633531,\n",
       "         0.18428068, 0.09486813, 0.13833636, 0.18407002, 0.13551165,\n",
       "         0.14450394, 0.11511374]], dtype=float32)>)"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embedding_test[:1], conv1d_output[:1], max_pool_output[:1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_5_conv1D\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_5 (InputLayer)        [(None, 1)]               0         \n",
      "                                                                 \n",
      " text_vectorization_1 (TextV  (None, 15)               0         \n",
      " ectorization)                                                   \n",
      "                                                                 \n",
      " embedding_5 (Embedding)     (None, 15, 128)           1280000   \n",
      "                                                                 \n",
      " conv1d_1 (Conv1D)           (None, 11, 32)            20512     \n",
      "                                                                 \n",
      " global_max_pooling1d_1 (Glo  (None, 32)               0         \n",
      " balMaxPooling1D)                                                \n",
      "                                                                 \n",
      " dense_4 (Dense)             (None, 1)                 33        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 1,300,545\n",
      "Trainable params: 1,300,545\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "tf.random.set_seed(42)\n",
    "model5_embedding = layers.Embedding(input_dim=max_vocab_length, output_dim=128,embeddings_initializer=\"uniform\", input_length=max_length,name=\"embedding_5\")\n",
    "inputs = layers.Input(shape=(1,),dtype=\"string\")\n",
    "x = text_vectorizer(inputs)\n",
    "x = model5_embedding(x)\n",
    "# CNN\n",
    "x = layers.Conv1D(filters=32,kernel_size=5,activation=\"relu\")(x)\n",
    "x = layers.GlobalMaxPool1D()(x)\n",
    "outputs = layers.Dense(1,activation=\"sigmoid\")(x)\n",
    "model5 = tf.keras.Model(inputs,outputs,name=\"model_5_conv1D\")\n",
    "\n",
    "model5.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "model5.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving TensorBoard log files to: model_log/Bidirectional/20250122-102857\n",
      "Epoch 1/10\n",
      "215/215 [==============================] - 1s 4ms/step - loss: 0.0000e+00 - accuracy: 0.5733 - val_loss: 0.0000e+00 - val_accuracy: 0.5433\n",
      "Epoch 2/10\n",
      "215/215 [==============================] - 1s 3ms/step - loss: 0.0000e+00 - accuracy: 0.5733 - val_loss: 0.0000e+00 - val_accuracy: 0.5433\n",
      "Epoch 3/10\n",
      "215/215 [==============================] - 1s 3ms/step - loss: 0.0000e+00 - accuracy: 0.5733 - val_loss: 0.0000e+00 - val_accuracy: 0.5433\n",
      "Epoch 4/10\n",
      "215/215 [==============================] - 1s 3ms/step - loss: 0.0000e+00 - accuracy: 0.5733 - val_loss: 0.0000e+00 - val_accuracy: 0.5433\n",
      "Epoch 5/10\n",
      "215/215 [==============================] - 1s 3ms/step - loss: 0.0000e+00 - accuracy: 0.5733 - val_loss: 0.0000e+00 - val_accuracy: 0.5433\n",
      "Epoch 6/10\n",
      "215/215 [==============================] - 1s 3ms/step - loss: 0.0000e+00 - accuracy: 0.5733 - val_loss: 0.0000e+00 - val_accuracy: 0.5433\n",
      "Epoch 7/10\n",
      "215/215 [==============================] - 1s 3ms/step - loss: 0.0000e+00 - accuracy: 0.5733 - val_loss: 0.0000e+00 - val_accuracy: 0.5433\n",
      "Epoch 8/10\n",
      "215/215 [==============================] - 1s 3ms/step - loss: 0.0000e+00 - accuracy: 0.5733 - val_loss: 0.0000e+00 - val_accuracy: 0.5433\n",
      "Epoch 9/10\n",
      "215/215 [==============================] - 1s 3ms/step - loss: 0.0000e+00 - accuracy: 0.5733 - val_loss: 0.0000e+00 - val_accuracy: 0.5433\n",
      "Epoch 10/10\n",
      "215/215 [==============================] - 1s 3ms/step - loss: 0.0000e+00 - accuracy: 0.5733 - val_loss: 0.0000e+00 - val_accuracy: 0.5433\n"
     ]
    }
   ],
   "source": [
    "model5_history = model5.fit(train_sentences,train_labels,epochs=10,validation_data=(val_sentences,val_labels),\n",
    "                            callbacks=[create_tensorboard_callback(dir_name=SAVE_DIR, \n",
    "                                                                     experiment_name=\"Bidirectional\")])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "24/24 [==============================] - 0s 908us/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/miniconda3/envs/torchai/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'accuracy': 0.5433070866141733,\n",
       " 'precision': 0.29518259036518074,\n",
       " 'recall': 0.5433070866141733,\n",
       " 'f1': 0.38253254057528524,\n",
       " 'auc': 0.5}"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model5_pred_probs = model5.predict(val_sentences)\n",
    "model5_preds = tf.squeeze(tf.round(model5_pred_probs))\n",
    "model5_results = calculate_results(val_labels,\n",
    "                                model5_preds)\n",
    "model5_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Baseline accuracy: 0.79, New accuracy: 0.54, Difference: -0.25\n",
      "Baseline precision: 0.81, New precision: 0.30, Difference: -0.52\n",
      "Baseline recall: 0.79, New recall: 0.54, Difference: -0.25\n",
      "Baseline f1: 0.79, New f1: 0.38, Difference: -0.40\n",
      "Baseline auc: 0.78, New auc: 0.50, Difference: -0.28\n"
     ]
    }
   ],
   "source": [
    "compare_baseline_to_new_results(baseline_results,model5_results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using Pretrained Embeddings (transfer learning for NLP)\n",
    "\n",
    "For all of the previous deep learning models we've built and trained, we've created and used our own embeddings from scratch each time.\n",
    "\n",
    "However, a common practice is to leverage pretrained embeddings through **transfer learning**. This is one of the main benefits of using deep models: being able to take what one (often larger) model has learned (often on a large amount of data) and adjust it for our own use case.\n",
    "\n",
    "For our next model, instead of using our own embedding layer, we're going to replace it with a pretrained embedding layer.\n",
    "\n",
    "More specifically, we're going to be using the [Universal Sentence Encoder](https://www.aclweb.org/anthology/D18-2029.pdf) from [TensorFlow Hub](https://tfhub.dev/google/universal-sentence-encoder/4) (a great resource containing a plethora of pretrained model resources for a variety of tasks).\n",
    "\n",
    "> 🔑 **Note:** There are many different pretrained text embedding options on TensorFlow Hub, however, some require different levels of text preprocessing than others. Best to experiment with a few and see which best suits your use case.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model 6: TensorFlow Hub Pretrained Sentence Encoder  预训练encoder模型\n",
    "\n",
    "The main difference between the embedding layer we created and the Universal Sentence Encoder is that rather than create a word-level embedding, the Universal Sentence Encoder, as you might've guessed, creates a whole sentence-level embedding.\n",
    "\n",
    "Our embedding layer also outputs an a 128 dimensional vector for each word, where as, the Universal Sentence Encoder outputs a 512 dimensional vector for each sentence.\n",
    "\n",
    "![](https://raw.githubusercontent.com/mrdbourke/tensorflow-deep-learning/main/images/08-USE-tensorflow-hub-encoder-decoder-model.png)\n",
    "*The feature extractor model we're building through the eyes of an **encoder/decoder** model.*\n",
    "\n",
    "> 🔑 **Note:** An **encoder** is the name for a model which converts raw data such as text into a numerical representation (feature vector), a **decoder** converts the numerical representation to a desired output.\n",
    "\n",
    "As usual, this is best demonstrated with an example.\n",
    "\n",
    "We can load in a TensorFlow Hub module using the [`hub.load()`](https://www.tensorflow.org/hub/api_docs/python/hub/load) method and passing it the target URL of the module we'd like to use, in our case, it's \"https://tfhub.dev/google/universal-sentence-encoder/4\".\n",
    "\n",
    "Let's load the Universal Sentence Encoder model and test it on a couple of sentences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(\n",
      "[-0.01157022  0.02485909  0.02878049 -0.01271502  0.03971541  0.0882776\n",
      "  0.02680985  0.05589838 -0.01068732 -0.00597293  0.00639324 -0.01819521\n",
      "  0.00030815  0.0910589   0.05874645 -0.03180628  0.01512474 -0.05162928\n",
      "  0.00991369 -0.06865346 -0.04209307  0.0267898   0.0301101   0.00321071\n",
      " -0.00337968 -0.04787361  0.02266718 -0.00985926 -0.04063615 -0.01292094\n",
      " -0.04666382  0.05630298 -0.03949255  0.00517684  0.02495828 -0.0701444\n",
      "  0.02871509  0.04947679 -0.00633974 -0.08960192  0.02807122 -0.00808364\n",
      " -0.01360597  0.05998648 -0.10361788 -0.05195372  0.00232956 -0.02332528\n",
      " -0.03758107  0.03327729], shape=(50,), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "import tensorflow_hub as hub\n",
    "embed = hub.load(\"https://tfhub.dev/google/universal-sentence-encoder/4\") # load Universal Sentence Encoder\n",
    "embed_samples = embed([sample_sentence,\n",
    "                      \"When you call the universal sentence encoder on a sentence, it turns it into numbers.\"])\n",
    "\n",
    "print(embed_samples[0][:50])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorShape([512])"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embed_samples[0].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Passing our sentences to the Universal Sentence Encoder (USE) encodes them from strings to 512 dimensional vectors, which make no sense to us but hopefully make sense to our machine learning models.\n",
    "\n",
    "Speaking of models, let's build one with the USE as our embedding layer.\n",
    "\n",
    "We can convert the TensorFlow Hub USE module into a Keras layer using the [`hub.KerasLayer`](https://www.tensorflow.org/hub/api_docs/python/hub/KerasLayer) class.\n",
    "\n",
    "> 🔑 **Note:** Due to the size of the USE TensorFlow Hub module, it may take a little while to download. Once it's downloaded though, it'll be cached and ready to use. And as with many TensorFlow Hub modules, there is a [\"lite\" version of the USE](https://tfhub.dev/google/universal-sentence-encoder-lite/2) which takes up less space but sacrifices some performance and requires more preprocessing steps. However, depending on your available compute power, the lite version may be better for your application use case."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence_encoder_layer = hub.KerasLayer(\"https://tfhub.dev/google/universal-sentence-encoder/4\",\n",
    "                                        dtype=tf.string, # data type of inputs coming to the USE layer\n",
    "                                        trainable=False, # keep the pretrained weights (we'll create a feature extractor)\n",
    "                                        name=\"USE\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_6_USE\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " USE (KerasLayer)            (None, 512)               256797824 \n",
      "                                                                 \n",
      " dense_5 (Dense)             (None, 64)                32832     \n",
      "                                                                 \n",
      " dense_6 (Dense)             (None, 1)                 65        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 256,830,721\n",
      "Trainable params: 32,897\n",
      "Non-trainable params: 256,797,824\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# sentence encoder layer s使用报错，pip install tensorflow==2.10.0,使用该版本\n",
    "sentence_encoder_layer = hub.KerasLayer(\"https://tfhub.dev/google/universal-sentence-encoder/4\",\n",
    "                                        input_shape=[], # shape of inputs coming to our model \n",
    "                                        dtype=tf.string, # data type of inputs coming to the USE layer\n",
    "                                        trainable=False, # keep the pretrained weights (we'll create a feature extractor)\n",
    "                                        name=\"USE\") \n",
    "\n",
    "model6 = tf.keras.Sequential([\n",
    "  sentence_encoder_layer, # take in sentences and then encode them into an embedding\n",
    "  layers.Dense(64, activation=\"relu\"),\n",
    "  layers.Dense(1, activation=\"sigmoid\")\n",
    "], name=\"model_6_USE\")\n",
    "\n",
    "model6.compile(loss='binary_crossentropy',\n",
    "              optimizer='adam',\n",
    "              metrics=['accuracy'])\n",
    "model6.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving TensorBoard log files to: model_log/tf_hub_sentence_encoder/20250122-102915\n",
      "Epoch 1/10\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "215/215 [==============================] - 3s 5ms/step - loss: 0.5063 - accuracy: 0.7805 - val_loss: 0.4475 - val_accuracy: 0.7966\n",
      "Epoch 2/10\n",
      "215/215 [==============================] - 1s 4ms/step - loss: 0.4144 - accuracy: 0.8146 - val_loss: 0.4368 - val_accuracy: 0.8136\n",
      "Epoch 3/10\n",
      "215/215 [==============================] - 1s 4ms/step - loss: 0.3999 - accuracy: 0.8224 - val_loss: 0.4328 - val_accuracy: 0.8136\n",
      "Epoch 4/10\n",
      "215/215 [==============================] - 1s 4ms/step - loss: 0.3926 - accuracy: 0.8276 - val_loss: 0.4287 - val_accuracy: 0.8123\n",
      "Epoch 5/10\n",
      "215/215 [==============================] - 1s 4ms/step - loss: 0.3864 - accuracy: 0.8286 - val_loss: 0.4307 - val_accuracy: 0.8110\n",
      "Epoch 6/10\n",
      "215/215 [==============================] - 1s 4ms/step - loss: 0.3797 - accuracy: 0.8342 - val_loss: 0.4261 - val_accuracy: 0.8176\n",
      "Epoch 7/10\n",
      "215/215 [==============================] - 1s 4ms/step - loss: 0.3739 - accuracy: 0.8336 - val_loss: 0.4247 - val_accuracy: 0.8163\n",
      "Epoch 8/10\n",
      "215/215 [==============================] - 1s 4ms/step - loss: 0.3681 - accuracy: 0.8409 - val_loss: 0.4248 - val_accuracy: 0.8189\n",
      "Epoch 9/10\n",
      "215/215 [==============================] - 1s 4ms/step - loss: 0.3618 - accuracy: 0.8409 - val_loss: 0.4284 - val_accuracy: 0.8163\n",
      "Epoch 10/10\n",
      "215/215 [==============================] - 1s 4ms/step - loss: 0.3562 - accuracy: 0.8432 - val_loss: 0.4289 - val_accuracy: 0.8215\n"
     ]
    }
   ],
   "source": [
    "model6_history = model6.fit(train_sentences,train_labels,epochs=10,validation_data=(val_sentences,val_labels),\n",
    "                            callbacks=[create_tensorboard_callback(dir_name=SAVE_DIR, \n",
    "                                                                     experiment_name=\"tf_hub_sentence_encoder\")])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "24/24 [==============================] - 0s 3ms/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'accuracy': 0.821522309711286,\n",
       " 'precision': 0.8260248375546463,\n",
       " 'recall': 0.821522309711286,\n",
       " 'f1': 0.8194798413883178,\n",
       " 'auc': 0.8139888389138764}"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model6_pred_probs = model6.predict(val_sentences)\n",
    "model6_preds = tf.squeeze(tf.round(model6_pred_probs))\n",
    "model6_results = calculate_results(val_labels,\n",
    "                                model6_preds)\n",
    "model6_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Baseline accuracy: 0.79, New accuracy: 0.82, Difference: 0.03\n",
      "Baseline precision: 0.81, New precision: 0.83, Difference: 0.01\n",
      "Baseline recall: 0.79, New recall: 0.82, Difference: 0.03\n",
      "Baseline f1: 0.79, New f1: 0.82, Difference: 0.03\n",
      "Baseline auc: 0.78, New auc: 0.81, Difference: 0.03\n"
     ]
    }
   ],
   "source": [
    "# tensorflow hub 中的encoder模型效果明显比较好\n",
    "compare_baseline_to_new_results(baseline_results,model6_results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model 7: TensorFlow Hub Pretrained Sentence Encoder 10% of the training data  10%的数据 encoder模型\n",
    "\n",
    "One of the benefits of using transfer learning methods, such as, the pretrained embeddings within the USE is the ability to get great results on a small amount of data (the USE paper even mentions this in the abstract).\n",
    "\n",
    "To put this to the test, we're going to make a small subset of the training data (10%), train a model and evaluate it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "### NOTE: Making splits like this will lead to data leakage ###\n",
    "### (some of the training examples in the validation set) ###\n",
    "\n",
    "# 因为train_df_shuffled 已经被切分了，所以 train_df_shuffled[[\"text\", \"target\"]].sample(frac=0.1, random_state=42)，可能和验证集有交集\n",
    "\n",
    "\n",
    "### WRONG WAY TO MAKE SPLITS (train_df_shuffled has already been split) ### \n",
    "\n",
    "# # Create subsets of 10% of the training data\n",
    "# train_10_percent = train_df_shuffled[[\"text\", \"target\"]].sample(frac=0.1, random_state=42)\n",
    "# train_sentences_10_percent = train_10_percent[\"text\"].to_list()\n",
    "# train_labels_10_percent = train_10_percent[\"target\"].to_list()\n",
    "# len(train_sentences_10_percent), len(train_labels_10_percent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "# 正确的分割数据\n",
    "train_sentences_90_percent, train_sentences_10_percent, train_labels_90_percent, train_labels_10_percent = train_test_split(np.array(train_sentences),\n",
    "                                                                                                                            train_labels,\n",
    "                                                                                                                            test_size=0.1,\n",
    "                                                                                                                            random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total training examples: 6851\n",
      "Length of 10% training examples: 686\n"
     ]
    }
   ],
   "source": [
    "print(f\"Total training examples: {len(train_sentences)}\")\n",
    "print(f\"Length of 10% training examples: {len(train_sentences_10_percent)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_labels_10_percent.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    415\n",
       "1    271\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.Series(train_labels_10_percent).value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_6_USE\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " USE (KerasLayer)            (None, 512)               256797824 \n",
      "                                                                 \n",
      " dense_5 (Dense)             (None, 64)                32832     \n",
      "                                                                 \n",
      " dense_6 (Dense)             (None, 1)                 65        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 256,830,721\n",
      "Trainable params: 32,897\n",
      "Non-trainable params: 256,797,824\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# 从model6中复制model7\n",
    "\n",
    "model7 = tf.keras.models.clone_model(model6)\n",
    "\n",
    "model7.compile(loss=\"binary_crossentropy\",\n",
    "                optimizer=tf.keras.optimizers.Adam(),\n",
    "                metrics=[\"accuracy\"]\n",
    ")\n",
    "\n",
    "model7.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving TensorBoard log files to: model_log/10_percent_tf_hub_sentence_encoder/20250122-102928\n",
      "Epoch 1/5\n",
      "22/22 [==============================] - 2s 22ms/step - loss: 0.6751 - accuracy: 0.6531 - val_loss: 0.6549 - val_accuracy: 0.7073\n",
      "Epoch 2/5\n",
      "22/22 [==============================] - 0s 8ms/step - loss: 0.6024 - accuracy: 0.8309 - val_loss: 0.5951 - val_accuracy: 0.7598\n",
      "Epoch 3/5\n",
      "22/22 [==============================] - 0s 8ms/step - loss: 0.5222 - accuracy: 0.8382 - val_loss: 0.5384 - val_accuracy: 0.7651\n",
      "Epoch 4/5\n",
      "22/22 [==============================] - 0s 8ms/step - loss: 0.4544 - accuracy: 0.8382 - val_loss: 0.5049 - val_accuracy: 0.7717\n",
      "Epoch 5/5\n",
      "22/22 [==============================] - 0s 8ms/step - loss: 0.4096 - accuracy: 0.8440 - val_loss: 0.4877 - val_accuracy: 0.7808\n"
     ]
    }
   ],
   "source": [
    "model7_history = model7.fit(x=train_sentences_10_percent,\n",
    "                              y=train_labels_10_percent,\n",
    "                              epochs=5,\n",
    "                              validation_data=(val_sentences, val_labels),\n",
    "                              callbacks=[create_tensorboard_callback(SAVE_DIR, \"10_percent_tf_hub_sentence_encoder\")])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "24/24 [==============================] - 0s 3ms/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'accuracy': 0.7808398950131233,\n",
       " 'precision': 0.7861251265484451,\n",
       " 'recall': 0.7808398950131233,\n",
       " 'f1': 0.7775280856169867,\n",
       " 'auc': 0.7717391304347825}"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model7_pred_probs = model7.predict(val_sentences)\n",
    "model7_preds = tf.squeeze(tf.round(model7_pred_probs))\n",
    "model7_results = calculate_results(val_labels,\n",
    "                                model7_preds)\n",
    "model7_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Baseline accuracy: 0.79, New accuracy: 0.78, Difference: -0.01\n",
      "Baseline precision: 0.81, New precision: 0.79, Difference: -0.03\n",
      "Baseline recall: 0.79, New recall: 0.78, Difference: -0.01\n",
      "Baseline f1: 0.79, New f1: 0.78, Difference: -0.01\n",
      "Baseline auc: 0.78, New auc: 0.77, Difference: -0.01\n"
     ]
    }
   ],
   "source": [
    "compare_baseline_to_new_results(baseline_results,model7_results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Comparing the performance of each of our models 对比模型效果\n",
    "\n",
    "Woah. We've come a long way! From training a baseline to several deep models.\n",
    "\n",
    "Now it's time to compare our model's results.\n",
    "\n",
    "But just before we do, it's worthwhile mentioning, this type of practice is a standard deep learning workflow. Training various different models, then comparing them to see which one performed best and continuing to train it if necessary.\n",
    "\n",
    "The important thing to note is that for all of our modelling experiments we used the same training data (except for `model_7` where we used 10% of the training data).\n",
    "\n",
    "To visualize our model's performances, let's create a pandas DataFrame we our results dictionaries and then plot it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>accuracy</th>\n",
       "      <th>precision</th>\n",
       "      <th>recall</th>\n",
       "      <th>f1</th>\n",
       "      <th>auc</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>baseline</th>\n",
       "      <td>0.792651</td>\n",
       "      <td>0.811139</td>\n",
       "      <td>0.792651</td>\n",
       "      <td>0.786219</td>\n",
       "      <td>0.779402</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>simple_dense</th>\n",
       "      <td>0.791339</td>\n",
       "      <td>0.797601</td>\n",
       "      <td>0.791339</td>\n",
       "      <td>0.788036</td>\n",
       "      <td>0.782088</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>lstm</th>\n",
       "      <td>0.543307</td>\n",
       "      <td>0.295183</td>\n",
       "      <td>0.543307</td>\n",
       "      <td>0.382533</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>gru</th>\n",
       "      <td>0.543307</td>\n",
       "      <td>0.295183</td>\n",
       "      <td>0.543307</td>\n",
       "      <td>0.382533</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>bidirectional</th>\n",
       "      <td>0.543307</td>\n",
       "      <td>0.295183</td>\n",
       "      <td>0.543307</td>\n",
       "      <td>0.382533</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>conv1d</th>\n",
       "      <td>0.543307</td>\n",
       "      <td>0.295183</td>\n",
       "      <td>0.543307</td>\n",
       "      <td>0.382533</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>tf_hub_sentence_encoder</th>\n",
       "      <td>0.821522</td>\n",
       "      <td>0.826025</td>\n",
       "      <td>0.821522</td>\n",
       "      <td>0.819480</td>\n",
       "      <td>0.813989</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>tf_hub_10_percent_data</th>\n",
       "      <td>0.780840</td>\n",
       "      <td>0.786125</td>\n",
       "      <td>0.780840</td>\n",
       "      <td>0.777528</td>\n",
       "      <td>0.771739</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                         accuracy  precision    recall        f1       auc\n",
       "baseline                 0.792651   0.811139  0.792651  0.786219  0.779402\n",
       "simple_dense             0.791339   0.797601  0.791339  0.788036  0.782088\n",
       "lstm                     0.543307   0.295183  0.543307  0.382533  0.500000\n",
       "gru                      0.543307   0.295183  0.543307  0.382533  0.500000\n",
       "bidirectional            0.543307   0.295183  0.543307  0.382533  0.500000\n",
       "conv1d                   0.543307   0.295183  0.543307  0.382533  0.500000\n",
       "tf_hub_sentence_encoder  0.821522   0.826025  0.821522  0.819480  0.813989\n",
       "tf_hub_10_percent_data   0.780840   0.786125  0.780840  0.777528  0.771739"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_model_results = pd.DataFrame({\"baseline\": baseline_results,\n",
    "                                  \"simple_dense\": model1_results,\n",
    "                                  \"lstm\": model2_results,\n",
    "                                  \"gru\": model3_results,\n",
    "                                  \"bidirectional\": model4_results,\n",
    "                                  \"conv1d\": model5_results,\n",
    "                                  \"tf_hub_sentence_encoder\": model6_results,\n",
    "                                  \"tf_hub_10_percent_data\": model7_results})\n",
    "all_model_results = all_model_results.transpose()\n",
    "all_model_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Axes: >"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1200x800 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "all_model_results.plot(kind='bar', figsize=(12,8))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Axes: >"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAiMAAAJACAYAAABfSmZFAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjkuMiwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8hTgPZAAAACXBIWXMAAA9hAAAPYQGoP6dpAABTGUlEQVR4nO3deVyVZeL///cBZVEBcyM0RMxcEJcCzSU1Mynz15jW6GS5wpRDuZFZZmk6Jq1GZW6jpqaplW1TVlKpqViNiFq5LwkpuCa4gsL9+8OP59vhoAlyuODwej4e5zGe69xH3mdCeXvd133dNsuyLAEAABjiYToAAAAo3ygjAADAKMoIAAAwijICAACMoowAAACjKCMAAMAoyggAADCqgukAVyMvL08HDx6Un5+fbDab6TgAAOAqWJalkydPqnbt2vLwuPz8R5koIwcPHlRwcLDpGAAAoAjS0tJ0ww03XPb1MlFG/Pz8JF38MP7+/obTAACAq5GVlaXg4GD7z/HLKRNl5NKpGX9/f8oIAABlzF8tsWABKwAAMIoyAgAAjKKMAAAAoygjAADAKMoIAAAwijICAACMoowAAACjKCMAAMAoyggAADCKMgIAAIyijAAAAKMoIwAAwCjKCAAAMIoyAgAAjKKMAAAAoyqYDlAS6j39hbGv/duL3Y19bQAAygJmRgAAgFGUEQAAYBRlBAAAGFWkMjJt2jSFhobKx8dHERERWrNmzRWPX7RokVq0aKFKlSopKChIgwYN0rFjx4oUGAAAuJdCl5GlS5dqxIgRGjt2rFJSUtShQwd169ZNqampBR6/du1a9e/fX9HR0fr111/1wQcf6H//+59iYmKuOTwAACj7Cl1GpkyZoujoaMXExKhJkyZKSEhQcHCwpk+fXuDxP/zwg+rVq6dhw4YpNDRUt912mx599FFt2LDhmsMDAICyr1BlJCcnR8nJyYqKinIYj4qKUlJSUoHvadeunX7//XctX75clmXp0KFD+vDDD9W9++Uvec3OzlZWVpbDAwAAuKdClZGjR48qNzdXgYGBDuOBgYHKyMgo8D3t2rXTokWL1KdPH3l5een6669X1apV9dZbb13268THxysgIMD+CA4OLkxMAABQhhRpAavNZnN4blmW09glW7du1bBhwzRu3DglJyfrq6++0r59+zRkyJDL/v5jxoxRZmam/ZGWllaUmAAAoAwo1A6sNWrUkKenp9MsyOHDh51mSy6Jj49X+/bt9eSTT0qSmjdvrsqVK6tDhw6aNGmSgoKCnN7j7e0tb2/vwkQDAABlVKFmRry8vBQREaHExESH8cTERLVr167A95w5c0YeHo5fxtPTU9LFGRUAAFC+Ffo0TVxcnGbPnq25c+dq27ZtGjlypFJTU+2nXcaMGaP+/fvbj7/33nv10Ucfafr06dq7d6/WrVunYcOGqXXr1qpdu3bxfRIAAFAmFfpGeX369NGxY8c0ceJEpaenKzw8XMuXL1dISIgkKT093WHPkYEDB+rkyZOaOnWqnnjiCVWtWlV33HGHXnrppeL7FCgQNwgEAJQFNqsMnCvJyspSQECAMjMz5e/vX+j3l9cfynzukkcJA4D/52p/fnNvGgAAYBRlBAAAGEUZAQAARhV6ASuA0om1MgDKKmZGAACAUZQRAABgFGUEAAAYRRkBAABGUUYAAIBRlBEAAGAUZQQAABhFGQEAAEZRRgAAgFGUEQAAYBRlBAAAGEUZAQAARlFGAACAUZQRAABgFGUEAAAYRRkBAABGUUYAAIBRlBEAAGAUZQQAABhFGQEAAEZRRgAAgFGUEQAAYBRlBAAAGEUZAQAARlFGAACAUZQRAABgFGUEAAAYRRkBAABGFamMTJs2TaGhofLx8VFERITWrFlz2WMHDhwom83m9GjatGmRQwMAAPdR6DKydOlSjRgxQmPHjlVKSoo6dOigbt26KTU1tcDj33jjDaWnp9sfaWlpqlatmv7+979fc3gAAFD2FbqMTJkyRdHR0YqJiVGTJk2UkJCg4OBgTZ8+vcDjAwICdP3119sfGzZs0B9//KFBgwZdc3gAAFD2FaqM5OTkKDk5WVFRUQ7jUVFRSkpKuqrfY86cObrzzjsVEhJy2WOys7OVlZXl8AAAAO6pUGXk6NGjys3NVWBgoMN4YGCgMjIy/vL96enp+vLLLxUTE3PF4+Lj4xUQEGB/BAcHFyYmAAAoQ4q0gNVmszk8tyzLaawg8+bNU9WqVXXfffdd8bgxY8YoMzPT/khLSytKTAAAUAZUKMzBNWrUkKenp9MsyOHDh51mS/KzLEtz585Vv3795OXldcVjvb295e3tXZhoAACgjCrUzIiXl5ciIiKUmJjoMJ6YmKh27dpd8b2rV6/W7t27FR0dXfiUAADAbRVqZkSS4uLi1K9fP0VGRqpt27aaNWuWUlNTNWTIEEkXT7EcOHBACxYscHjfnDlzdOuttyo8PLx4kgMAALdQ6DLSp08fHTt2TBMnTlR6errCw8O1fPly+9Ux6enpTnuOZGZmatmyZXrjjTeKJzUAAHAbhS4jkhQbG6vY2NgCX5s3b57TWEBAgM6cOVOULwUAANwc96YBAABGUUYAAIBRlBEAAGAUZQQAABhFGQEAAEZRRgAAgFGUEQAAYBRlBAAAGEUZAQAARlFGAACAUZQRAABgFGUEAAAYRRkBAABGUUYAAIBRlBEAAGAUZQQAABhFGQEAAEZRRgAAgFGUEQAAYBRlBAAAGEUZAQAARlFGAACAUZQRAABgFGUEAAAYRRkBAABGUUYAAIBRlBEAAGAUZQQAABhFGQEAAEZRRgAAgFGUEQAAYBRlBAAAGFWkMjJt2jSFhobKx8dHERERWrNmzRWPz87O1tixYxUSEiJvb2/deOONmjt3bpECAwAA91KhsG9YunSpRowYoWnTpql9+/aaOXOmunXrpq1bt6pu3boFvqd37946dOiQ5syZowYNGujw4cO6cOHCNYcHAABlX6HLyJQpUxQdHa2YmBhJUkJCgr7++mtNnz5d8fHxTsd/9dVXWr16tfbu3atq1apJkurVq3dtqQEAgNso1GmanJwcJScnKyoqymE8KipKSUlJBb7ns88+U2RkpF5++WXVqVNHDRs21KhRo3T27NnLfp3s7GxlZWU5PAAAgHsq1MzI0aNHlZubq8DAQIfxwMBAZWRkFPievXv3au3atfLx8dHHH3+so0ePKjY2VsePH7/supH4+HhNmDChMNEAAEAZVaQFrDabzeG5ZVlOY5fk5eXJZrNp0aJFat26te655x5NmTJF8+bNu+zsyJgxY5SZmWl/pKWlFSUmAAAoAwo1M1KjRg15eno6zYIcPnzYabbkkqCgINWpU0cBAQH2sSZNmsiyLP3++++66aabnN7j7e0tb2/vwkQDAABlVKFmRry8vBQREaHExESH8cTERLVr167A97Rv314HDx7UqVOn7GM7d+6Uh4eHbrjhhiJEBgAA7qTQp2ni4uI0e/ZszZ07V9u2bdPIkSOVmpqqIUOGSLp4iqV///724/v27avq1atr0KBB2rp1q77//ns9+eSTGjx4sHx9fYvvkwAAgDKp0Jf29unTR8eOHdPEiROVnp6u8PBwLV++XCEhIZKk9PR0paam2o+vUqWKEhMTNXToUEVGRqp69erq3bu3Jk2aVHyfAgAAlFmFLiOSFBsbq9jY2AJfmzdvntNY48aNnU7tAAAASNybBgAAGEYZAQAARlFGAACAUZQRAABgFGUEAAAYRRkBAABGUUYAAIBRlBEAAGAUZQQAABhFGQEAAEZRRgAAgFGUEQAAYBRlBAAAGEUZAQAARlFGAACAUZQRAABgFGUEAAAYRRkBAABGUUYAAIBRlBEAAGAUZQQAABhFGQEAAEZRRgAAgFGUEQAAYBRlBAAAGEUZAQAARlFGAACAUZQRAABgFGUEAAAYRRkBAABGUUYAAIBRlBEAAGBUkcrItGnTFBoaKh8fH0VERGjNmjWXPXbVqlWy2WxOj+3btxc5NAAAcB+FLiNLly7ViBEjNHbsWKWkpKhDhw7q1q2bUlNTr/i+HTt2KD093f646aabihwaAAC4j0KXkSlTpig6OloxMTFq0qSJEhISFBwcrOnTp1/xfbVq1dL1119vf3h6ehY5NAAAcB+FKiM5OTlKTk5WVFSUw3hUVJSSkpKu+N6bb75ZQUFB6tKli1auXHnFY7Ozs5WVleXwAAAA7qlQZeTo0aPKzc1VYGCgw3hgYKAyMjIKfE9QUJBmzZqlZcuW6aOPPlKjRo3UpUsXff/995f9OvHx8QoICLA/goODCxMTAACUIRWK8iabzebw3LIsp7FLGjVqpEaNGtmft23bVmlpaXr11VfVsWPHAt8zZswYxcXF2Z9nZWVRSAAAcFOFmhmpUaOGPD09nWZBDh8+7DRbciVt2rTRrl27Lvu6t7e3/P39HR4AAMA9FaqMeHl5KSIiQomJiQ7jiYmJateu3VX/PikpKQoKCirMlwYAAG6q0Kdp4uLi1K9fP0VGRqpt27aaNWuWUlNTNWTIEEkXT7EcOHBACxYskCQlJCSoXr16atq0qXJycrRw4UItW7ZMy5YtK95PAgAAyqRCl5E+ffro2LFjmjhxotLT0xUeHq7ly5crJCREkpSenu6w50hOTo5GjRqlAwcOyNfXV02bNtUXX3yhe+65p/g+BQAAKLOKtIA1NjZWsbGxBb42b948h+ejR4/W6NGji/JlAABAOcC9aQAAgFGUEQAAYBRlBAAAGEUZAQAARlFGAACAUZQRAABgFGUEAAAYRRkBAABGUUYAAIBRlBEAAGAUZQQAABhFGQEAAEZRRgAAgFGUEQAAYBRlBAAAGEUZAQAARlFGAACAUZQRAABgFGUEAAAYRRkBAABGUUYAAIBRlBEAAGAUZQQAABhFGQEAAEZRRgAAgFGUEQAAYBRlBAAAGEUZAQAARlFGAACAUZQRAABgFGUEAAAYRRkBAABGFamMTJs2TaGhofLx8VFERITWrFlzVe9bt26dKlSooJYtWxblywIAADdU6DKydOlSjRgxQmPHjlVKSoo6dOigbt26KTU19Yrvy8zMVP/+/dWlS5cihwUAAO6n0GVkypQpio6OVkxMjJo0aaKEhAQFBwdr+vTpV3zfo48+qr59+6pt27ZFDgsAANxPocpITk6OkpOTFRUV5TAeFRWlpKSky77vnXfe0Z49ezR+/Pir+jrZ2dnKyspyeAAAAPdUqDJy9OhR5ebmKjAw0GE8MDBQGRkZBb5n165devrpp7Vo0SJVqFDhqr5OfHy8AgIC7I/g4ODCxAQAAGVIkRaw2mw2h+eWZTmNSVJubq769u2rCRMmqGHDhlf9+48ZM0aZmZn2R1paWlFiAgCAMuDqpir+T40aNeTp6ek0C3L48GGn2RJJOnnypDZs2KCUlBQ9/vjjkqS8vDxZlqUKFSpoxYoVuuOOO5ze5+3tLW9v78JEAwAAZVShZka8vLwUERGhxMREh/HExES1a9fO6Xh/f3/9/PPP2rRpk/0xZMgQNWrUSJs2bdKtt956bekBAECZV6iZEUmKi4tTv379FBkZqbZt22rWrFlKTU3VkCFDJF08xXLgwAEtWLBAHh4eCg8Pd3h/rVq15OPj4zQOAADKp0KXkT59+ujYsWOaOHGi0tPTFR4eruXLlyskJESSlJ6e/pd7jgAAAFxS6DIiSbGxsYqNjS3wtXnz5l3xvc8//7yef/75onxZAADghopURgCgtKj39BfGvvZvL3Y39rX53CWPz+063CgPAAAYRRkBAABGUUYAAIBRlBEAAGAUZQQAABhFGQEAAEZRRgAAgFGUEQAAYBRlBAAAGEUZAQAARlFGAACAUZQRAABgFGUEAAAYRRkBAABGUUYAAIBRlBEAAGAUZQQAABhFGQEAAEZRRgAAgFGUEQAAYBRlBAAAGEUZAQAARlFGAACAUZQRAABgFGUEAAAYRRkBAABGUUYAAIBRlBEAAGAUZQQAABhFGQEAAEZRRgAAgFFFKiPTpk1TaGiofHx8FBERoTVr1lz22LVr16p9+/aqXr26fH191bhxY73++utFDgwAANxLhcK+YenSpRoxYoSmTZum9u3ba+bMmerWrZu2bt2qunXrOh1fuXJlPf7442revLkqV66stWvX6tFHH1XlypX1yCOPFMuHAAAAZVehZ0amTJmi6OhoxcTEqEmTJkpISFBwcLCmT59e4PE333yzHnzwQTVt2lT16tXTww8/rLvuuuuKsykAAKD8KFQZycnJUXJysqKiohzGo6KilJSUdFW/R0pKipKSktSpU6fLHpOdna2srCyHBwAAcE+FKiNHjx5Vbm6uAgMDHcYDAwOVkZFxxffecMMN8vb2VmRkpB577DHFxMRc9tj4+HgFBATYH8HBwYWJCQAAypAiLWC12WwOzy3LchrLb82aNdqwYYNmzJihhIQELV68+LLHjhkzRpmZmfZHWlpaUWICAIAyoFALWGvUqCFPT0+nWZDDhw87zZbkFxoaKklq1qyZDh06pOeff14PPvhggcd6e3vL29u7MNEAAEAZVaiZES8vL0VERCgxMdFhPDExUe3atbvq38eyLGVnZxfmSwMAADdV6Et74+Li1K9fP0VGRqpt27aaNWuWUlNTNWTIEEkXT7EcOHBACxYskCS9/fbbqlu3rho3bizp4r4jr776qoYOHVqMHwMAAJRVhS4jffr00bFjxzRx4kSlp6crPDxcy5cvV0hIiCQpPT1dqamp9uPz8vI0ZswY7du3TxUqVNCNN96oF198UY8++mjxfQoAAFBmFbqMSFJsbKxiY2MLfG3evHkOz4cOHcosCAAAuCzuTQMAAIyijAAAAKMoIwAAwCjKCAAAMIoyAgAAjKKMAAAAoygjAADAKMoIAAAwijICAACMoowAAACjKCMAAMAoyggAADCKMgIAAIyijAAAAKMoIwAAwCjKCAAAMIoyAgAAjKKMAAAAoygjAADAKMoIAAAwijICAACMoowAAACjKCMAAMAoyggAADCKMgIAAIyijAAAAKMoIwAAwCjKCAAAMIoyAgAAjKKMAAAAoygjAADAKMoIAAAwqkhlZNq0aQoNDZWPj48iIiK0Zs2ayx770UcfqWvXrqpZs6b8/f3Vtm1bff3110UODAAA3Euhy8jSpUs1YsQIjR07VikpKerQoYO6deum1NTUAo///vvv1bVrVy1fvlzJycnq3Lmz7r33XqWkpFxzeAAAUPYVuoxMmTJF0dHRiomJUZMmTZSQkKDg4GBNnz69wOMTEhI0evRotWrVSjfddJMmT56sm266Sf/973+vOTwAACj7ClVGcnJylJycrKioKIfxqKgoJSUlXdXvkZeXp5MnT6patWqXPSY7O1tZWVkODwAA4J4KVUaOHj2q3NxcBQYGOowHBgYqIyPjqn6P1157TadPn1bv3r0ve0x8fLwCAgLsj+Dg4MLEBAAAZUiRFrDabDaH55ZlOY0VZPHixXr++ee1dOlS1apV67LHjRkzRpmZmfZHWlpaUWICAIAyoEJhDq5Ro4Y8PT2dZkEOHz7sNFuS39KlSxUdHa0PPvhAd9555xWP9fb2lre3d2GiAQCAMqpQMyNeXl6KiIhQYmKiw3hiYqLatWt32fctXrxYAwcO1Hvvvafu3bsXLSkAAHBLhZoZkaS4uDj169dPkZGRatu2rWbNmqXU1FQNGTJE0sVTLAcOHNCCBQskXSwi/fv31xtvvKE2bdrYZ1V8fX0VEBBQjB8FAACURYUuI3369NGxY8c0ceJEpaenKzw8XMuXL1dISIgkKT093WHPkZkzZ+rChQt67LHH9Nhjj9nHBwwYoHnz5l37JwAAAGVaocuIJMXGxio2NrbA1/IXjFWrVhXlSwAAgHKCe9MAAACjKCMAAMAoyggAADCKMgIAAIyijAAAAKMoIwAAwCjKCAAAMIoyAgAAjKKMAAAAoygjAADAKMoIAAAwijICAACMoowAAACjKCMAAMAoyggAADCKMgIAAIyijAAAAKMoIwAAwCjKCAAAMIoyAgAAjKKMAAAAoygjAADAKMoIAAAwijICAACMoowAAACjKCMAAMAoyggAADCKMgIAAIyijAAAAKMoIwAAwCjKCAAAMIoyAgAAjCpSGZk2bZpCQ0Pl4+OjiIgIrVmz5rLHpqenq2/fvmrUqJE8PDw0YsSIomYFAABuqNBlZOnSpRoxYoTGjh2rlJQUdejQQd26dVNqamqBx2dnZ6tmzZoaO3asWrRocc2BAQCAeyl0GZkyZYqio6MVExOjJk2aKCEhQcHBwZo+fXqBx9erV09vvPGG+vfvr4CAgGsODAAA3EuhykhOTo6Sk5MVFRXlMB4VFaWkpKRiC5Wdna2srCyHBwAAcE+FKiNHjx5Vbm6uAgMDHcYDAwOVkZFRbKHi4+MVEBBgfwQHBxfb7w0AAEqXIi1gtdlsDs8ty3IauxZjxoxRZmam/ZGWllZsvzcAAChdKhTm4Bo1asjT09NpFuTw4cNOsyXXwtvbW97e3sX2+wEAgNKrUDMjXl5eioiIUGJiosN4YmKi2rVrV6zBAABA+VComRFJiouLU79+/RQZGam2bdtq1qxZSk1N1ZAhQyRdPMVy4MABLViwwP6eTZs2SZJOnTqlI0eOaNOmTfLy8lJYWFjxfAoAAFBmFbqM9OnTR8eOHdPEiROVnp6u8PBwLV++XCEhIZIubnKWf8+Rm2++2f7r5ORkvffeewoJCdFvv/12bekBAECZV+gyIkmxsbGKjY0t8LV58+Y5jVmWVZQvAwAAygHuTQMAAIyijAAAAKMoIwAAwCjKCAAAMIoyAgAAjKKMAAAAoygjAADAKMoIAAAwijICAACMoowAAACjKCMAAMAoyggAADCKMgIAAIyijAAAAKMoIwAAwCjKCAAAMIoyAgAAjKKMAAAAoygjAADAKMoIAAAwijICAACMoowAAACjKCMAAMAoyggAADCKMgIAAIyijAAAAKMoIwAAwCjKCAAAMIoyAgAAjKKMAAAAoygjAADAKMoIAAAwqkhlZNq0aQoNDZWPj48iIiK0Zs2aKx6/evVqRUREyMfHR/Xr19eMGTOKFBYAALifQpeRpUuXasSIERo7dqxSUlLUoUMHdevWTampqQUev2/fPt1zzz3q0KGDUlJS9Mwzz2jYsGFatmzZNYcHAABlX6HLyJQpUxQdHa2YmBg1adJECQkJCg4O1vTp0ws8fsaMGapbt64SEhLUpEkTxcTEaPDgwXr11VevOTwAACj7KhTm4JycHCUnJ+vpp592GI+KilJSUlKB71m/fr2ioqIcxu666y7NmTNH58+fV8WKFZ3ek52drezsbPvzzMxMSVJWVlZh4trlZZ8p0vuKQ1EzFwc+d8njc5c8PnfJ43OXvLL6uS+917KsKx5XqDJy9OhR5ebmKjAw0GE8MDBQGRkZBb4nIyOjwOMvXLigo0ePKigoyOk98fHxmjBhgtN4cHBwYeKWCgEJphOYwecuX/jc5Qufu3wpjs998uRJBQQEXPb1QpWRS2w2m8Nzy7Kcxv7q+ILGLxkzZozi4uLsz/Py8nT8+HFVr179il/HFbKyshQcHKy0tDT5+/uX6Nc2ic/N5y4P+Nx87vLA5Oe2LEsnT55U7dq1r3hcocpIjRo15Onp6TQLcvjwYafZj0uuv/76Ao+vUKGCqlevXuB7vL295e3t7TBWtWrVwkQtdv7+/uXqm/cSPnf5wucuX/jc5Yupz32lGZFLCrWA1cvLSxEREUpMTHQYT0xMVLt27Qp8T9u2bZ2OX7FihSIjIwtcLwIAAMqXQl9NExcXp9mzZ2vu3Lnatm2bRo4cqdTUVA0ZMkTSxVMs/fv3tx8/ZMgQ7d+/X3Fxcdq2bZvmzp2rOXPmaNSoUcX3KQAAQJlV6DUjffr00bFjxzRx4kSlp6crPDxcy5cvV0hIiCQpPT3dYc+R0NBQLV++XCNHjtTbb7+t2rVr680339T9999ffJ/Chby9vTV+/Hin00bujs/N5y4P+Nx87vKgLHxum/VX19sAAAC4EPemAQAARlFGAACAUZQRAABgFGUEAAAYRRkBAABGUUb+5Pz58xo0aJD27t1rOgoAAOUGl/bmU7VqVW3cuFH169c3HcWINWvWaObMmdqzZ48+/PBD1alTR++++65CQ0N12223mY7nMhcuXNCqVau0Z88e9e3bV35+fjp48KD8/f1VpUoV0/Fc5syZM0pNTVVOTo7DePPmzQ0lcr0/74NUkLp165ZQErjKn+9t9lemTJniwiS4WkW6UZ4769mzpz755JNCfTO7i2XLlqlfv3566KGHlJKSouzsbEkX77Y4efJkLV++3HBC19i/f7/uvvtupaamKjs7W127dpWfn59efvllnTt3TjNmzDAdsdgdOXJEgwYN0pdfflng67m5uSWcqOTUq1fvijfcdNfPfu7cOb311ltauXKlDh8+rLy8PIfXN27caChZ8UtJSXF4npycrNzcXDVq1EiStHPnTnl6eioiIsJEPBSAMpJPgwYN9O9//1tJSUmKiIhQ5cqVHV4fNmyYoWSuN2nSJM2YMUP9+/fXkiVL7OPt2rXTxIkTDSZzreHDhysyMlKbN292uHljz549FRMTYzCZ64wYMUJ//PGHfvjhB3Xu3Fkff/yxDh06pEmTJum1114zHc+l8v+gOn/+vFJSUjRlyhS98MILhlK53uDBg5WYmKgHHnhArVu3LvE7oJeklStX2n89ZcoU+fn5af78+bruuuskSX/88YcGDRqkDh06mIroEtddd91V/3c9fvy4i9MUDqdp8gkNDb3sazabza3Xk1SqVElbt25VvXr15Ofnp82bN6t+/frau3evwsLCdO7cOdMRXaJGjRpat26dGjVq5PC5f/vtN4WFhenMmTOmIxa7oKAgffrpp2rdurX8/f21YcMGNWzYUJ999plefvllrV271nTEEvfFF1/olVde0apVq0xHcYmAgAAtX75c7du3Nx2lRNWpU0crVqxQ06ZNHcZ/+eUXRUVF6eDBg4aSFb/58+df9bEDBgxwYZLCY2Ykn3379pmOYExQUJB2796tevXqOYyvXbvWrdfQ5OXlFTg1//vvv8vPz89AItc7ffq0atWqJUmqVq2ajhw5ooYNG6pZs2ZuNV1fGA0bNtT//vc/0zFcpk6dOm77/XwlWVlZOnTokFMZOXz4sE6ePGkolWuUtoJRGFxNcxk5OTnasWOHLly4YDpKiXn00Uc1fPhw/fjjj7LZbDp48KAWLVqkUaNGKTY21nQ8l+natasSEhLsz202m06dOqXx48frnnvuMRfMhRo1aqQdO3ZIklq2bKmZM2fqwIEDmjFjhoKCggync62srCyHR2ZmprZv367nnntON910k+l4LvPaa6/pqaee0v79+01HKVE9e/bUoEGD9OGHH+r333/X77//rg8//FDR0dHq1auX6Xgl4uzZs07f96WOBQenT5+2Bg8ebHl6elqenp7Wnj17LMuyrKFDh1rx8fGG07neM888Y/n6+lo2m82y2WyWj4+P9eyzz5qO5VIHDhywGjZsaDVp0sSqUKGC1aZNG6t69epWo0aNrEOHDpmO5xILFy603nnnHcuyLGvjxo1WzZo1LQ8PD8vHx8dasmSJ2XAuZrPZLA8PD4eHzWaz6tatayUlJZmO5zKHDx+2br/9dsvDw8OqUqWKdd111zk83NXp06etf/3rX5a3t7f9v7eXl5f1r3/9yzp16pTpeC5z6tQp67HHHrP/2c7/KG1YM5LP8OHDtW7dOiUkJOjuu+/Wli1bVL9+fX322WcaP3680+I3d3TmzBlt3bpVeXl5CgsLc+tLWy85e/asFi9erI0bNyovL0+33HKLHnroIfn6+pqOViLOnDmj7du3q27duqpRo4bpOC61evVqh+ceHh6qWbOmGjRooAoV3PfM9Z133qnU1FRFR0crMDDQaaFjWZ7ivxqnT5/Wnj17ZFmWGjRo4HRxgrt57LHHtHLlSk2cOFH9+/fX22+/rQMHDmjmzJl68cUX9dBDD5mO6MhwGSp16tata61fv96yLMuqUqWKfWZk165dlp+fn8loJS4zM9P6+OOPra1bt5qOgmI2YcIE6/Tp007jZ86csSZMmGAgUcnIycmxBg4caP9zXZ74+vpamzZtMh0DJSQ4ONhauXKlZVmW5efnZ+3atcuyLMtasGCB1a1bN4PJCsaakXyOHDliX9j3Z6dPn3brS+EkqXfv3po6daqkizMFrVq1Uu/evdW8eXMtW7bMcDrX2rlzp2bNmqVJkyZp4sSJDg93NGHCBJ06dcpp/MyZM5owYYKBRCWjYsWK+vjjj03HMKJx48Y6e/as6Rilxp49e3THHXeYjuEyx48ft18d6u/vb7+U97bbbtP3339vMlqBKCP5tGrVSl988YX9+aUC8p///Edt27Y1FatEfP/99/br7j/++GPl5eXpxIkTevPNNzVp0iTD6VznP//5j8LCwjRu3Dh9+OGH+vjjj+2PTz75xHQ8l7Asq8ByvXnzZlWrVs1AopJzaWPD8ubFF1/UE088oVWrVunYsWOlf0Gji506dcrplJ07ubQ9gSSFhYXp/ffflyT997//VdWqVc0Fuwz3PUFaRPHx8br77ru1detWXbhwQW+88YZ+/fVXrV+/3q2/cSUpMzPT/oPoq6++0v33369KlSqpe/fuevLJJw2nc51JkybphRde0FNPPWU6istd2hTJZrOpYcOGDoUkNzdXp06d0pAhQwwmdL3yurHh3XffLUnq0qWLw/ilYupuO8+++eabV3z9wIEDJZTEjEGDBmnz5s3q1KmTxowZo+7du+utt97ShQsXSuUW+CxgLcDPP/+sV199VcnJyfbFjE899ZSaNWtmOppLNWzYUJMmTVL37t0VGhqqJUuW6I477tDmzZvVpUsXHT161HREl/D399emTZvcei+VS+bPny/LsjR48GAlJCQoICDA/pqXl5fq1avn9jOA5XVjw7/6x1SnTp1KKEnJ8PDwUFBQkLy8vAp8PScnRxkZGW5Xwi4nNTVVGzZs0I033qgWLVqYjuOEMgK7adOmafjw4apSpYpCQkK0ceNGeXh46K233tJHH33ksMWyO4mOjlarVq3cfkbgz1avXq127dqpYsWKpqOghKSmpio4ONjp9JxlWUpLS3O7GwSGhobqpZdeUu/evQt8fdOmTYqIiCg3ZaS04zSNVKjzpf7+/i5MYlZsbKxat26ttLQ0de3aVR4eF5cU1a9f363XjDRo0EDPPfecfvjhBzVr1szpB7Q7Ttv/+V/BZ8+e1fnz5x1ed+fv88vdBNNms8nHx0cNGjRQjx493G7tTGhoqNLT050W6F9a6OhuP5QjIiKUnJx82TJis9nk7v8W//bbb/Xtt98WeGPEuXPnGkpVMGZGdHE672qvlHG3P7Aon9P2Z86c0ejRo/X+++/r2LFjTq+78/d5586dtXHjRvtdXC3L0q5du+Tp6anGjRtrx44dstlsWrt2rcLCwkzHLTYeHh46dOiQatas6TC+f/9+hYWF6fTp04aSucbWrVt15swZRUZGFvj6+fPndfDgQYWEhJRwspIxYcIETZw4UZGRkQoKCnL6GVfaripjZkSOd3j87bff9PTTT2vgwIH2c+fr16/X/PnzFR8fbypiicjNzdW8efMu26S/++47Q8lcqzzej+jJJ5/UypUrNW3atAI3RHJnl2Y93nnnHfsMUFZWlqKjo3Xbbbfpn//8p/r27auRI0fq66+/Npz22l2aCbLZbHruuedUqVIl+2u5ubn68ccf1bJlS0PpXOevimTFihXdtohI0owZMzRv3jz169fPdJSrwsxIPl26dFFMTIwefPBBh/H33ntPs2bNcts7ekrS448/rnnz5ql79+4FNunXX3/dUDIUt7p162rBggW6/fbb5e/vr40bN6pBgwZ69913tXjxYi1fvtx0RJepU6eOEhMTnX5Y/frrr4qKitKBAwe0ceNGRUVFucWi7c6dO0u6uE6obdu2Dgs6Ly1aHjVqlNvel2fChAl6+OGHdeONN5qOUqKqV6+un376qcx8bspIPpUqVdLmzZud/mDu3LlTLVu2dMvbyV9So0YNLViwwG1vDvdncXFx+ve//63KlStfdg3BJaXxMrhrVaVKFf36668KCQnRDTfcoI8++kitW7fWvn371KxZswI3RHMXVapU0eeff67bb7/dYXzVqlW69957dfLkSe3du1ctW7Z0q/03Bg0apDfeeMOt1wMVpHnz5vr111/VqlUrPfzww+rTp4/TqSp39NRTT6lKlSp67rnnTEe5KpymySc4OFgzZszQa6+95jA+c+ZMBQcHG0pVMry8vNSgQQPTMUpESkqKfdHmle435K677l7aECkkJMS+IVLr1q1L7YZIxalHjx4aPHiwXnvtNbVq1Uo2m00//fSTRo0apfvuu0+S9NNPP6lhw4Zmgxazd955x+F5VlaWvvvuOzVu3FiNGzc2lMr1tmzZol9//VWLFi3SlClTFBcXpzvvvFMPP/yw7rvvPofTVu7k3LlzmjVrlr755hs1b97caWF+aftHFjMj+Sxfvlz333+/brzxRrVp00aS9MMPP2jPnj1atmyZW88avPbaa9q7d6+mTp3qtj+EcdHrr78uT09PDRs2TCtXrlT37t2Vm5tr3xBp+PDhpiO6zKlTpzRy5EgtWLBAFy5ckCRVqFBBAwYM0Ouvv67KlStr06ZNkuRWayl69+6tjh076vHHH9fZs2fVokUL/fbbb7IsS0uWLNH9999vOmKJWLdund577z198MEHOnfunFvNfv3ZpdNzBbHZbKVuDSBlpAC///67pk2bpu3bt8uyLIWFhWnIkCFuPzPSs2dPrVy5UtWqVVPTpk2dmvRHH31kKBlcrbRviOQKp06d0t69e2VZlm688Ua3vzv19ddfr6+//lotWrTQe++9p/Hjx2vz5s2aP3++Zs2aVS7uSC5d3F9k4cKFWrJkiY4dO8b9ekoJygjsBg0adMXX80/zlmW9evW66mMpYXAHvr6+2rlzp4KDg9W/f3/Vrl1bL774olJTUxUWFubW64T27dun9957T4sWLdLOnTvVsWNH9e3bV3//+98ddiF2V7///rtsNpvq1KljOsplsWakACdOnNCcOXO0bds22Ww2hYWFafDgwW7/TetOZeOvuPt/y4L81b06/swdN3or74KDg7V+/XpVq1ZNX331lZYsWSJJ+uOPP+Tj42M4neu0bdtWP/30k5o1a6ZBgwapb9++pfqHcnHJy8vTpEmT9Nprr9mLpp+fn5544gmNHTvWvqllacHMSD4bNmzQXXfdJV9fX7Vu3VqWZWnDhg06e/asVqxYoVtuucV0RJe6cOGCVq1apT179qhv377y8/PTwYMH5e/v7/bT2O4u/+ZuR44c0ZkzZ+wLVk+cOKFKlSqpVq1abrnRW3n359s91K1bVykpKeXidg/PPPOMHnroITVt2tR0lBI1ZswYzZkzRxMmTFD79u1lWZbWrVun559/Xv/85z/1wgsvmI7oyIKD2267zRo4cKB1/vx5+9j58+etAQMGWB06dDCYzPV+++03q3HjxlalSpUsT09Pa8+ePZZlWdbw4cOtRx991HA61zp//ryVmJhozZgxw8rKyrIsy7IOHDhgnTx50nAy11i0aJHVvn17a/v27fax7du3Wx06dLAWLlxoMBlcacOGDdZHH31knTp1yj72+eefW+vWrTOYCq4QFBRkffrpp07jn3zyiVW7dm0Dia6MmZF8fH19lZKS4nSp29atWxUZGenW+4zcd9998vPz05w5c1S9enVt3rxZ9evX1+rVqxUTE6Ndu3aZjugS+/fv1913363U1FRlZ2dr586dql+/vkaMGKFz585pxowZpiMWuxtvvFEffvihbr75Zofx5ORkPfDAA+VyV1p39Fd76PxZabvUs7iU152lfXx8tGXLFqdL1Hfs2KGWLVuWuoW7rBnJx9/fX6mpqU5lJC0tTX5+foZSlYy1a9dq3bp1TrfcDgkJ0YEDBwylcr3hw4crMjJSmzdvVvXq1e3jPXv2VExMjMFkrpOenu50czzp4l/chw4dMpAIrnC1V8i486X8w4cPt+8sHR4e7taf9c9atGihqVOnOq0Vmzp1aqm8Yo4ykk+fPn0UHR2tV199Ve3atbPfMOvJJ5902iLe3eTl5RV4g7Tff//drYtYeSxhXbp00T//+U/NmTNHERERstls2rBhgx599FHdeeedpuOhmLjrOpDCWLJkid5//3233iOqIC+//LK6d++ub775Rm3btpXNZlNSUpLS0tJK5e0eStdy2lLg1VdfVa9evdS/f3/Vq1dPISEhGjhwoB544AG99NJLpuO5VNeuXZWQkGB/brPZdOrUKY0fP96t/yCXxxI2d+5c1alTR61bt5aPj4+8vb116623KigoSLNnzzYdDyg25Wln6T/r1KmTdu7cqZ49e+rEiRM6fvy4evXqpR07dqhDhw6m4zlhzchlnDlzRnv27JFlWWrQoIHbbhn8ZwcPHlTnzp3l6empXbt2KTIyUrt27VKNGjX0/fffq1atWqYjukSfPn0UEBCgWbNmyc/PT1u2bFHNmjXVo0cP1a1b160ved61a5e2bdsmy7LUpEkTt9sCHWBn6bKBMpJPZmamcnNzVa1aNYfx48ePq0KFCm5/k6mzZ89q8eLF2rhxo/Ly8nTLLbfooYcekq+vr+loLlNeS9jV8Pf316ZNm1S/fn3TUYAiKU87S2/ZskXh4eHy8PDQli1brnhs8+bNSyjV1aGM5NOtWzfde++9io2NdRifMWOGPvvss1J5rg3X7uzZs1qyZImSk5PLTQm7Gn5+fvarqoCyqDztLO3h4aGMjAzVqlVLHh4estlsKuhHvM1mK/DUtEmUkXyqVaumdevWqUmTJg7j27dvV/v27XXs2DFDyVzjs88+u+pj//a3v7kwCUojyghQduzfv19169aVzWbT/v37r3hsSEhICaW6OlxNk092drb9Tp5/dv78+VJ3XXZxuHTL9EsKatKXzrOWtiZdXObPn68aNWqoe/fukqTRo0dr1qxZCgsL0+LFi0vdH1oAhXfkyBHt2LFDNptNDRs2VM2aNU1HKnZ//rtq//79ateunSpUcPwxf+HCBSUlJZW6v9e4miafVq1aadasWU7jM2bMUEREhIFErpWXl2d/rFixQi1bttSXX36pEydOKDMzU19++aVuueUWffXVV6ajuszkyZPtp2PWr1+vqVOn6uWXX1aNGjU0cuRIw+kAXIvTp09r8ODBCgoKUseOHdWhQwfVrl1b0dHRbr2JZefOnXX8+HGn8czMTHXu3NlAoitjZiSfF154QXfeeac2b96sLl26SJK+/fZb/e9//9OKFSsMp3OtESNGaMaMGbrtttvsY3fddZcqVaqkRx55RNu2bTOYznXS0tLsl/598skneuCBB/TII4+offv2uv32282GM4yrD1DWxcXFafXq1frvf/+r9u3bS7q4t9CwYcP0xBNPaPr06YYTuoZlWQX++T127JgqV65sINGVUUbyad++vdavX69XXnlF77//vnx9fdW8eXPNmTNHN910k+l4LrVnz54C72YbEBCg3377reQDlZAqVaro2LFjqlu3rlasWGGfDfHx8XHLU3OFwZIylHXLli3Thx9+6PAPi3vuuUe+vr7q3bu325WRXr16Sbr4D4mBAwfK29vb/lpubq62bNmidu3amYp3WZSRArRs2VKLFi0yHaPEtWrVSiNGjNDChQsVFBQkScrIyNATTzyh1q1bG07nOl27dlVMTIxuvvlm7dy507525Ndff1W9evXMhnORiRMnatSoUU7755w9e1avvPKKxo0bJ0n68ssvy8Xt1uG+zpw5o8DAQKfxWrVqueVpmkv/oLQsS35+fg5XBHp5ealNmzb65z//aSreZXE1TQHy8vK0e/fuAm+q1LFjR0OpXG/37t3q2bOnduzYobp160qSUlNT1bBhQ33yySduu4vhiRMn9OyzzyotLU3/+te/dPfdd0uSxo8fLy8vL40dO9ZwwuLn6emp9PR0pz1Ujh07plq1arntYmWUP126dFH16tW1YMEC+fj4SLpYugcMGKDjx4/rm2++MZzQNSZMmKAnn3yyzGzYSRnJ54cfflDfvn21f//+Aq8qcfe/pC3LUmJiorZv3y7LshQWFqY777yTtQNuxsPDQ4cOHXK6ouC7775Tnz59dOTIEUPJgOL1888/q1u3bjp37pxatGghm82mTZs2ydvbWytWrFDTpk1NR3SJffv26cKFC07LC3bt2qWKFSuWullfykg+LVu2VMOGDTVhwgQFBQU5/RAuaE1FedOsWTMtX75cwcHBpqMUqzNnzig1NVU5OTkO46Vtp8Jrcd1118lmsykzM1P+/v4O39+5ubk6deqUhgwZorfffttgSqB4nT17VgsXLnT4R5a7b2rYqVMnDR48WAMGDHAYX7hwoWbPnq1Vq1aZCXYZlJF8KleurM2bN7vtKYni4G4bYR05ckQDBw687OXL7jQbNn/+fFmWpcGDByshIcGhXHt5ealevXpq27atwYRA8YqPj1dgYKAGDx7sMD537lwdOXJETz31lKFkruXv76+NGzc6/SzbvXu3IiMjdeLECTPBLoMFrPnceuut2r17N2WkHBkxYoROnDihH374QZ07d9bHH3+sQ4cOadKkSXrttddMxytWl/6VFBoaqnbt2jndpwNwNzNnztR7773nNN60aVP94x//cNsyYrPZdPLkSafxS/dfK20oI/kMHTpUTzzxhDIyMtSsWTOnv6zdacoeF3333Xf69NNP1apVK3l4eCgkJERdu3aVv7+/4uPj7VfXuJNOnTopLy9PO3fuLHcLtVG+ZGRk2K8O/LOaNWsqPT3dQKKS0aFDB8XHx2vx4sXy9PSUdHGWNz4+3mEvqdKCMpLP/fffL0kOU3qXtkgvDwtYy6PTp0/bryqpVq2ajhw5ooYNG6pZs2bauHGj4XSuUd4XaqP8CA4O1rp16xQaGuowvm7dOtWuXdtQKtd7+eWX1bFjRzVq1EgdOnSQJK1Zs0ZZWVn67rvvDKdzRhnJZ9++faYjoIQ1atRIO3bsUL169dSyZUvNnDlT9erV04wZMwr8F5U7GDJkiCIjI/XFF18UuFAbcBcxMTEaMWKEzp8/rzvuuEPSxV21R48erSeeeMJwOtcJCwvTli1bNHXqVG3evFm+vr7q37+/Hn/8cVWrVs10PCcsYEWhudsC1kWLFun8+fMaOHCgUlJSdNddd+no0aPy8vLS/Pnz1adPH9MRix0LtVFeWJalp59+Wm+++ab9SjkfHx899dRT9s39YB5lpADvvvuuZsyYoX379mn9+vUKCQlRQkKCQkND1aNHD9PxSsS5c+fsGwTl995776lHjx6l8v4G18qyLJ09e1bbt29X3bp1VaNGDdORXOKOO+7Q6NGj7Ru8Ae7u1KlT2rZtm3x9fXXTTTc5bJPurtasWaOZM2dq7969+uCDD1SnTh29++67Cg0NLXXrRrhrbz7Tp09XXFyc7rnnHp04ccJ+7rxq1apKSEgwG87F8vLy9O9//1t16tRRlSpVtHfvXknSc889pzlz5tiP69u3r9sVkTlz5ig8PFw+Pj667rrr1L9/f33yySemY7nMpYXa8+bNU3JysrZs2eLwANxNlSpV1KpVK4WHh5eLIrJs2TLddddd8vX11caNG5WdnS1JOnnypCZPnmw4XQEsOGjSpIn18ccfW5ZlWVWqVLH27NljWZZl/fzzz1b16tUNJnO9CRMmWPXr17cWLlxo+fr62j/70qVLrTZt2hhO5zrPPvusVblyZevpp5+2Pv30U+vTTz+1nn76aatKlSrW2LFjTcdzCZvN5vTw8PCw/y+Asq1ly5bW/PnzLcty/FmWkpJiBQYGmoxWIBaw5rNv3z7dfPPNTuPe3t46ffq0gUQlZ8GCBZo1a5a6dOmiIUOG2MebN2+u7du3G0zmWtOnT9d//vMfPfjgg/axv/3tb2revLmGDh2qSZMmGUznGizUBtzbjh07CrxE39/fv9RteCZxNY2T0NBQbdq0SSEhIQ7jX375pcLCwgylKhkHDhwocEFjXl6ezp8/byBRycjNzVVkZKTTeEREhC5cuGAgkevl//4G4F6CgoK0e/dup3vQrF27tlRefMCakXyefPJJPfbYY1q6dKksy9JPP/2kF154Qc8884yefPJJ0/FcqmnTplqzZo3T+AcffFDgbJG7ePjhhzV9+nSn8VmzZumhhx4ykKhkvPvuu2rfvr1q166t/fv3S5ISEhL06aefGk4G4Fo9+uijGj58uH788UfZbDYdPHhQixYt0qhRoxQbG2s6nhNmRvIZNGiQLly4oNGjR+vMmTPq27ev6tSpozfeeEP/+Mc/TMdzqfHjx6tfv346cOCA8vLy9NFHH2nHjh1asGCBPv/8c9PxilVcXJz91zabTbNnz9aKFSvUpk0bSRc3BUtLS1P//v1NRXSp6dOna9y4cRoxYoReeOEFp4Xa5eWqMcBdjR49WpmZmercubPOnTunjh07ytvbW6NGjdLjjz9uOp4TLu29gqNHjyovL8++O+efrVu3TpGRkW63Kvvrr7/W5MmTlZycrLy8PN1yyy0aN26coqKiTEcrVp07d76q42w2W6ncrfBahYWFafLkybrvvvsc9o355ZdfdPvtt+vo0aOmIwIootzcXK1du1bNmjWTj4+Ptm7dqry8PIWFhalKlSqm4xWIMlJE/v7+2rRpU6k89wb8FV9fX23fvl0hISEOZWTXrl1q3ry5zp49azoigGvg4+Ojbdu2OW2DX1qxZqSI6HAoyy4t1M6vPCzUBsqDZs2a2feKKgtYM1LOXXfddVd9X5Ljx4+7OA1KyqWF2ufOnbMv1F68eLHi4+M1e/Zs0/EAXKMXXnhBo0aN0r///W9FREQ4bVTp7+9vKFnBOE1TRO5yf5b58+df9bEDBgxwYRKUtP/85z+aNGmS0tLSJEl16tTR888/r+joaMPJAFwrD4//d+Ljz//gtErpHegpI0XkLmUEuNJCbQBl0+rVq6/4eqdOnUooydXhNE0Ruest13Nzc/Xxxx9r27ZtstlsatKkiXr06KEKFfhWcSf79u3ThQsXdNNNNzncDHDXrl2qWLGi00ZJAMqW0lY2/go/YYrIHSeUfvnlF/Xo0UMZGRlq1KiRJGnnzp2qWbOmPvvsMzVr1sxwQhSXgQMHavDgwbrpppscxn/88UfNnj1bq1atMhMMQJFt2bJF4eHh8vDw+MsbXjZv3ryEUl0dTtMU4MKFC1q1apX27Nmjvn37ys/PTwcPHpS/v3+pvUa7OLRp00a1atXS/Pnzdd1110mS/vjjDw0cOFCHDx/W+vXrDSdEcfH399fGjRudtv/fvXu3IiMjS+W9KwBcmYeHhzIyMlSrVi15eHjIZrMV+A/n0rhmhJmRfPbv36+7775bqampys7OVteuXeXn56eXX35Z586d04wZM0xHdJnNmzdrw4YN9iIiXbza5oUXXlCrVq0MJkNxs9lsOnnypNN4ZmZmqftLCsDV2bdvn2rWrGn/dVlCGcln+PDhioyM1ObNm1W9enX7eM+ePRUTE2Mwmes1atRIhw4dUtOmTR3GDx8+XOAN9FB2dejQQfHx8Vq8eLE8PT0lXVwvFB8fr9tuu81wOgBF8ecbYJa1m2FSRvJZu3at1q1bJy8vL4fxkJAQHThwwFCqkjF58mQNGzZMzz//vMM9WiZOnKiXXnpJWVlZ9mNL2zXqKJyXXnpJnTp1UqNGjdShQwdJ0po1a5SVleWW298D5cFnn3121cf+7W9/c2GSwmPNSD7VqlXT2rVrFRYW5nD57tq1a3X//ffr0KFDpiO6TEHXpV/69vjz89J4vhGFd/DgQU2dOlWbN2+Wr6+vmjdvrscff1zVqlUzHQ1AEfz573BJTmtG/nwVaGn7O5yZkXy6du2qhIQEzZo1S9LF/3inTp3S+PHjdc899xhO51orV640HQEl4Pz584qKitLMmTM1efJk03EAFJO8vDz7r7/55hs99dRTmjx5stq2bSubzaakpCQ9++yzpfLPPTMj+Rw8eFCdO3eWp6endu3apcjISO3atUs1atTQ999/z8ZQcAs1a9ZUUlKS06W9ANxDeHi4ZsyY4bQGbM2aNXrkkUe0bds2Q8kKRhkpwNmzZ7VkyRIlJycrLy9Pt9xyix566CH5+vqajuZy586d05YtW3T48GGHli2VvnOMKLonnnhCFStW1Isvvmg6CgAX8PX11U8//eS0P9SWLVt06623lro7c1NGYPfVV1+pf//+Onr0qNNrrBNxL0OHDtWCBQvUoEEDRUZGOt1Ea8qUKYaSASgOHTt2VMWKFbVw4UIFBQVJkjIyMtSvXz/l5OT85XbxJY0ykk98fLwCAwM1ePBgh/G5c+fqyJEjeuqppwwlc70GDRrorrvu0rhx4xQYGGg6Dlyoc+fOl33NZrNxRQ1Qxu3evVs9e/bUjh07VLduXUlSamqqGjZsqE8++aTUbddAGcmnXr16eu+999SuXTuH8R9//FH/+Mc/ytxGMoXh7++vlJQU3XjjjaajAACukWVZSkxM1Pbt22VZlsLCwnTnnXeWynurcTVNPhkZGfYprT+rWbOm0tPTDSQqOQ888IBWrVpFGSlHdu/erT179qhjx47y9fW1X7oNoOyz2WyKiopSVFSU6Sh/iTKST3BwsNatW6fQ0FCH8XXr1ql27dqGUpWMqVOn6u9//7vWrFmjZs2aqWLFig6vDxs2zFAyFLdjx46pd+/eWrlypWw2m3bt2qX69esrJiZGVatW1WuvvWY6IoBCevPNN/XII4/Ix8dHb7755hWPLW1/n3OaJp+XXnpJr7zyil555RXdcccdkqRvv/1Wo0eP1hNPPKExY8YYTug6s2fP1pAhQ+Tr66vq1as7/AvZZrNp7969BtOhOPXv31+HDx/W7Nmz1aRJE/vmfitWrNDIkSP166+/mo4IoJBCQ0O1YcMGVa9e3ekf1H9WGv8+p4zkY1mWnn76ab355pvKycmRJPn4+Oipp57SuHHjDKdzreuvv17Dhg3T008/7bSTH9zL9ddfr6+//lotWrRw2Gl43759atasmU6dOmU6IoBikn8n7dKInzj52Gw2vfTSSzpy5Ih++OEHbd68WcePH3f7IiJJOTk56tOnD0WkHDh9+rQqVarkNH706FF5e3sbSASguM2ZM0fh4eHy8fGRj4+PwsPDNXv2bNOxCsRPncuoUqWKWrVqpfDw8HLzl/OAAQO0dOlS0zFQAjp27KgFCxbYn9tsNuXl5emVV1654mW/AMqG5557TsOHD9e9996rDz74QB988IHuvfdejRw5Us8++6zpeE44TZPP6dOn9eKLL+rbb78tcBfS0naerTgNGzZMCxYsUIsWLdS8eXOnBaxshOU+tm7dqttvv10RERH67rvv9Le//U2//vqrjh8/rnXr1nFFFVDG1ahRQ2+99ZYefPBBh/HFixdr6NChBW5uaRJX0+QTExOj1atXq1+/fgoKCirV59iK288//6ybb75ZkvTLL784vFae/n8oD8LCwrRlyxZNnz5dnp6eOn36tHr16qXHHnuswEvbAZQtubm5ioyMdBqPiIjQhQsXDCS6MmZG8qlataq++OILtW/f3nQUAACKZOjQoapYsaLTjPaoUaN09uxZvf3224aSFYyZkXyuu+46VatWzXQMwOX++OMPzZkzR9u2bZPNZlOTJk00aNAgvv+BMiouLs7+a5vNptmzZ2vFihVq06aNJOmHH35QWlqa+vfvbyriZTEzks/ChQv16aefav78+QVebeBuevXqpXnz5snf31+9evW64rEfffRRCaWCq61evVo9evSQv7+/fSo3OTlZJ06c0GeffaZOnToZTgigsK528XlpvP8UMyP5vPbaa9qzZ48CAwNVr149p0WcGzduNJTMNQICAuzrQQICAgynQUl57LHH1Lt3b/uaEeniOebY2Fg99thjTmuGAJR+K1euNB2hyJgZyWfChAlXfH38+PEllKTknT17Vnl5efbbyf/222/65JNP1KRJE911112G06E4+fr6atOmTWrUqJHD+I4dO9SyZUudPXvWUDIA5REzI/m4c9n4Kz169FCvXr00ZMgQnThxQm3atFHFihV19OhRTZkyRf/6179MR0QxueWWW7Rt2zanMrJt2za1bNnSTCgA5RZlpAAnTpzQhx9+qD179ujJJ59UtWrVtHHjRgUGBqpOnTqm47nMxo0b9frrr0uSPvzwQwUGBiolJUXLli3TuHHjKCNuZNiwYRo+fLh2797tsLjt7bff1osvvqgtW7bYj23evLmpmADKCU7T5LNlyxbdeeedCggI0G+//aYdO3aofv36eu6557R//36HXSvdTaVKlbR9+3bVrVtXvXv3VtOmTTV+/HilpaWpUaNGOnPmjOmIKCZ/teW/zWaTZVmy2WzKzc0toVQAyitmRvKJi4vTwIED9fLLL8vPz88+3q1bN/Xt29dgMtdr0KCBPvnkE/Xs2VNff/21Ro4cKUk6fPiw/P39DadDcdq3b5/pCABgRxnJ53//+59mzpzpNF6nTh1lZGQYSFRyxo0bp759+2rkyJHq0qWL2rZtK0lasWKFfWdWuIeQkJCrOq579+6aPXs2u7ICcCnKSD4+Pj7KyspyGt+xY4dq1qxpIFHJeeCBB3TbbbcpPT1dLVq0sI936dJFPXv2NJgMpnz//fdcWQPA5bhrbz49evTQxIkTdf78eUkXz52npqbq6aef1v333284netdf/31uvnmmx3WFLRu3VqNGzc2mAoA4M4oI/m8+uqrOnLkiGrVqqWzZ8+qU6dOatCggfz8/PTCCy+YjgcAgNvhNE0+/v7+Wrt2rb777jtt3LhReXl5uuWWW3TnnXeajgYAgFvi0t58FixYoD59+sjb29thPCcnR0uWLCmVNxgCXMXPz0+bN29W/fr1TUcB4MYoI/l4enoqPT1dtWrVchg/duyYatWqxZ4LKFcoIwBKAmtG8rm00VN+v//+OzeSQ7nzzDPPqFq1aqZjAHBzzIz8n5tvvlk2m02bN29W06ZNVaHC/1tOk5ubq3379unuu+/W+++/bzAlcO0sy9I333yjpKQkZWRkyGazKTAwUO3bt1eXLl0KLOMA4EosYP0/9913nyRp06ZNuuuuu1SlShX7a15eXqpXr165uLQX7u3AgQP6//6//08///yzwsPDFRgYKMuylJSUpH//+99q0aKFPvvsM7e+BxOA0oeZkXzmz5+vPn36yMfHx3QUoNj16NFDp06d0sKFC512VU1PT9fDDz8sPz8/ffLJJ2YCAiiXKCOXkZOTo8OHDysvL89hvG7duoYSAdeuSpUqWrduncMOu3+WkpKiDh066NSpUyWcDEB5xmmafHbt2qXBgwcrKSnJYZw7mMId+Pr66vjx45d9/Y8//pCvr28JJgIAyoiTgQMHqkKFCvr8888VFBTEYj64lX/84x8aMGCApkyZoq5du9qvEMvMzFRiYqKeeOIJt787NYDSh9M0+VSuXFnJycnciwVuKScnR8OHD9fcuXN14cIFeXl52ccrVKig6OhoJSQk2McBoCRQRvJp1aqVXn/9dd12222mowAuk5WVpeTkZGVkZEi6eIPEiIgI+fv7G04GoDyijOTz3Xff6dlnn9XkyZPVrFkzVaxY0eF1/rIGAKB4UUby8fC4uClt/rUiLGBFeXDo0CHNnDlT48aNMx0FQDlCGcln9erVV3y9U6dOJZQEKHmbN2/WLbfcQukGUKK4miYfygbc2ZYtW674+o4dO0ooCQD8P8yMFGDNmjWaOXOm9u7dqw8++EB16tTRu+++q9DQUBa2okzz8PCQzWZTQX/sL41zOhJASeOuvfksW7ZMd911l3x9fbVx40ZlZ2dLkk6ePKnJkycbTgdcm+rVq+s///mP9u3b5/TYu3evPv/8c9MRAZRDnKbJZ9KkSZoxY4b69++vJUuW2MfbtWuniRMnGkwGXLuIiAgdPHhQISEhBb5+4sSJAmdNAMCVKCP57NixQx07dnQa9/f314kTJ0o+EFCMHn30UZ0+ffqyr9etW1fvvPNOCSYCAMqIk6CgIO3evVv16tVzGF+7dq3q169vJhRQTHr27HnF16+77joNGDCghNIAwEWsGcnn0Ucf1fDhw/Xjjz/KZrPp4MGDWrRokUaNGqXY2FjT8YAS5e/vr71795qOAcDNMTOSz+jRo5WZmanOnTvr3Llz6tixo7y9vTVq1Cg9/vjjpuMBJYr1IwBKApf2XsaZM2e0detW5eXlKSwsTFWqVDEdCShxfn5+2rx5M6coAbgUp2kuo1KlSoqMjFTjxo31zTffaNu2baYjAQDgligj+fTu3VtTp06VJJ09e1atWrVS79691bx5cy1btsxwOgAA3A9lJJ/vv/9eHTp0kCR9/PHHysvL04kTJ/Tmm29q0qRJhtMBJSv/DSMBwBUoI/lkZmaqWrVqkqSvvvpK999/vypVqqTu3btr165dhtMBJYslZQBKAmUkn+DgYK1fv16nT5/WV199paioKEnSH3/8IR8fH8PpgOJnWdZlS8eXX36pOnXqlHAiAOUNZSSfESNG6KGHHtINN9yg2rVr6/bbb5d08fRNs2bNzIYDitGcOXMUHh4uHx8f+fj4KDw8XLNnz3Y45rbbbpO3t7ehhADKCy7tLUBycrJSU1PVtWtX+yW9X3zxhapWrar27dsbTgdcu+eee06vv/66hg4dqrZt20qS1q9fr6lTp2r48OGsjwJQoigjReTv769Nmzax/wLKpBo1auitt97Sgw8+6DC+ePFiDR06VEePHjWUDEB5xGmaIqLDoSzLzc1VZGSk03hERIQuXLhgIBGA8owyApRDDz/8sKZPn+40PmvWLD300EMGEgEoz7g3DVBOxMXF2X9ts9k0e/ZsrVixQm3atJEk/fDDD0pLS1P//v1NRQRQTlFGgHIiJSXF4XlERIQkac+ePZKkmjVrqmbNmvr1119LPBuA8o0yUkTsTImyZuXKlaYjAECBWDNSRCxgBQCgeDAzcgWXCkdBsyDsTImyrHPnzlec3fvuu+9KMA2A8o6ZkQKwMyXcXcuWLdWiRQv7IywsTDk5Odq4cSM7DQMoccyM5HO5nSlHjhyp3377jZ0p4RZef/31Aseff/55nTp1qoTTACjv2IE1H3amRHm2e/dutW7dWsePHzcdBUA5wmmafNiZEuXZ+vXruTs1gBLHaZp8Lu1MOWXKFIdxdqaEO+nVq5fDc8uylJ6erg0bNui5554zlApAeUUZETtTovwJCAhweO7h4aFGjRpp4sSJioqKMpQKQHnFmhFdvMzxathsNi55BACgmFFGgHIsJydHhw8fVl5ensN43bp1DSUCUB5xmgYoh3bu3Kno6GglJSU5jFuWJZvNptzcXEPJAJRHlJF82JkS5cGgQYNUoUIFff755woKCuJeSwCMoozk07JlS4fn58+f16ZNm/TLL79owIABZkIBxWzTpk1KTk5W48aNTUcBAMpIfuxMifIgLCyMDfwAlBosYL1K7EyJsi4rK8v+6w0bNujZZ5/V5MmT1axZM1WsWNHhWH9//5KOB6AcY2bkKrEzJcq6qlWrOqwNsSxLXbp0cTiGBawATKCM5MPOlHBXK1euNB0BAArEaZp8Bg0a5PDcw8NDNWvW1B133MHOlCh3YmNjNXHiRNWoUcN0FABujDIC4LL8/f21adMm1a9f33QUAG6M0zSXwc6UwMXTlADgapSRfNiZEgCAkkUZyYedKQEAKFmUkXzYmRIAgJLlYTpAacPOlAAAlCzKiC7uTHnp8dJLL2n06NFatWqVjh075vDan3ewBMqaXr162b+HFyxYoOzs7L98z8MPP8xurABcjkt7dXEvkfw7U+ZfK8ICVpR1Xl5e2r9/v4KCguTp6an09HTVqlXLdCwAYM2IxM6UKB8aN26sMWPGqHPnzrIsS++///5lZz369+9fwukAlGfMjBQRO1OirElKSlJcXJz27Nmj48ePy8/Pr8CrxWw2GzeEBFCiKCNFxM6UKMs8PDx04MABBQUFOYxblqXU1FSFhIQYSgagPGIBaxHR4eCOjh8/TsEGUOIoI0A55enp6TR26tQp+fj4GEgDoDxjAStQjsTFxUm6uC5k3LhxqlSpkv213Nxc/fjjj2rZsqWhdADKK8oIUI6kpKRIunia8eeff5aXl5f9NS8vL7Vo0UKjRo0yFQ9AOUUZAcqRS5exDxo0SG+88QYbmgEoFVgzInamRPnzzjvv8P0LoNTg0l6xMyUAACZxmkbsTAkAgEnMjIidKQEAMIkykg87UwIAULJYwHqV2JkSAADXoIwUgJ0pAQAoOSxg/T/sTAkAgBmUkf/DzpQAAJjBAtZ82JkSAICSRRkBAABGsYAVAAAYRRkBAABGUUYAAIBRlBEAAGAUZQQAABhFGQEAAEZRRgAAgFH/P3WW/dArQNI9AAAAAElFTkSuQmCC",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "all_model_results.sort_values('f1', ascending=False)['f1'].plot(kind='bar')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 可以查看tensor board中的训练日志"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Combining our models (model ensembling/stacking)  多模型联合，平均/堆叠\n",
    "\n",
    "Many production systems use an **ensemble** (multiple different models combined) of models to make a prediction.\n",
    "\n",
    "The idea behind model stacking is that if several uncorrelated models agree on a prediction, then the prediction must be more robust than a prediction made by a singular model.\n",
    "\n",
    "The keyword in the sentence above is **uncorrelated**, which is another way of saying, different types of models. For example, in our case, we might combine our baseline, our bidirectional model and our TensorFlow Hub USE model.\n",
    "\n",
    "Although these models are all trained on the same data, they all have a different way of finding patterns.\n",
    "\n",
    "If we were to use three similarly trained models, such as three LSTM models, the predictions they output will likely be very similar.\n",
    "\n",
    "Think of it as trying to decide where to eat with your friends. If you all have similar tastes, you'll probably all pick the same restaurant. But if you've all got different tastes and still end up picking the same restaurant, the restaurant must be good.\n",
    "\n",
    "选择不同的模型，联合方式：平均值，多数投票，堆叠\n",
    "\n",
    "Since we're working with a classification problem, there are a few of ways we can combine our models:\n",
    "1. **Averaging** - Take the output prediction probabilities of each model for each sample, combine them and then average them.\n",
    "2. **Majority vote (mode)** - Make class predictions with each of your models on all samples, the predicted class is the one in majority. For example, if three different models predict `[1, 0, 1]` respectively, the majority class is `1`, therefore, that would be the predicted label.\n",
    "3. **Model stacking** - Take the outputs of each of your chosen models and use them as inputs to another model.\n",
    "\n",
    "> 📖 **Resource:** The above methods for model stacking/ensembling were adapted from Chapter 6 of the [Machine Learning Engineering Book](http://www.mlebook.com/wiki/doku.php) by Andriy Burkov. If you're looking to enter the field of machine learning engineering, not only building models but production-scale machine learning systems, I'd highly recommend reading it in its entirety.\n",
    "\n",
    "Again, the concept of model stacking is best seen in action.\n",
    "\n",
    "We're going to combine our baseline model (`model_0`), LSTM model (`model_2`) and our USE model trained on the full training data (`model_6`) by averaging the combined prediction probabilities of each."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(20,), dtype=float32, numpy=\n",
       "array([0., 0., 1., 0., 0., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0.], dtype=float32)>"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 三个模型的均值\n",
    "baseline_pred_probs = np.max(model0.predict_proba(val_sentences), axis=1) # get the prediction probabilities from baseline model\n",
    "combined_pred_probs = baseline_pred_probs + tf.squeeze(model2_pred_probs, axis=1) + tf.squeeze(model6_pred_probs)\n",
    "combined_preds = tf.round(combined_pred_probs/3) # average and round the prediction probabilities to get prediction classes\n",
    "combined_preds[:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'accuracy': 0.7611548556430446,\n",
       " 'precision': 0.8132090847515028,\n",
       " 'recall': 0.7611548556430446,\n",
       " 'f1': 0.7444674114407813,\n",
       " 'auc': 0.7407962685324004}"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ensemble_results = calculate_results(val_labels,combined_preds)\n",
    "ensemble_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>accuracy</th>\n",
       "      <th>precision</th>\n",
       "      <th>recall</th>\n",
       "      <th>f1</th>\n",
       "      <th>auc</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>baseline</th>\n",
       "      <td>0.792651</td>\n",
       "      <td>0.811139</td>\n",
       "      <td>0.792651</td>\n",
       "      <td>0.786219</td>\n",
       "      <td>0.779402</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>simple_dense</th>\n",
       "      <td>0.791339</td>\n",
       "      <td>0.797601</td>\n",
       "      <td>0.791339</td>\n",
       "      <td>0.788036</td>\n",
       "      <td>0.782088</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>lstm</th>\n",
       "      <td>0.543307</td>\n",
       "      <td>0.295183</td>\n",
       "      <td>0.543307</td>\n",
       "      <td>0.382533</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>gru</th>\n",
       "      <td>0.543307</td>\n",
       "      <td>0.295183</td>\n",
       "      <td>0.543307</td>\n",
       "      <td>0.382533</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>bidirectional</th>\n",
       "      <td>0.543307</td>\n",
       "      <td>0.295183</td>\n",
       "      <td>0.543307</td>\n",
       "      <td>0.382533</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>conv1d</th>\n",
       "      <td>0.543307</td>\n",
       "      <td>0.295183</td>\n",
       "      <td>0.543307</td>\n",
       "      <td>0.382533</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>tf_hub_sentence_encoder</th>\n",
       "      <td>0.821522</td>\n",
       "      <td>0.826025</td>\n",
       "      <td>0.821522</td>\n",
       "      <td>0.819480</td>\n",
       "      <td>0.813989</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>tf_hub_10_percent_data</th>\n",
       "      <td>0.780840</td>\n",
       "      <td>0.786125</td>\n",
       "      <td>0.780840</td>\n",
       "      <td>0.777528</td>\n",
       "      <td>0.771739</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ensemble_results</th>\n",
       "      <td>0.761155</td>\n",
       "      <td>0.813209</td>\n",
       "      <td>0.761155</td>\n",
       "      <td>0.744467</td>\n",
       "      <td>0.740796</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                         accuracy  precision    recall        f1       auc\n",
       "baseline                 0.792651   0.811139  0.792651  0.786219  0.779402\n",
       "simple_dense             0.791339   0.797601  0.791339  0.788036  0.782088\n",
       "lstm                     0.543307   0.295183  0.543307  0.382533  0.500000\n",
       "gru                      0.543307   0.295183  0.543307  0.382533  0.500000\n",
       "bidirectional            0.543307   0.295183  0.543307  0.382533  0.500000\n",
       "conv1d                   0.543307   0.295183  0.543307  0.382533  0.500000\n",
       "tf_hub_sentence_encoder  0.821522   0.826025  0.821522  0.819480  0.813989\n",
       "tf_hub_10_percent_data   0.780840   0.786125  0.780840  0.777528  0.771739\n",
       "ensemble_results         0.761155   0.813209  0.761155  0.744467  0.740796"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_model_results.loc[\"ensemble_results\"] = ensemble_results\n",
    "all_model_results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How did the stacked model go against the other models?\n",
    "\n",
    "> 🔑 **Note:** It seems many of our model's results are similar. This may mean there are some limitations to what can be learned from our data. When many of your modelling experiments return similar results, it's a good idea to revisit your data, we'll do this shortly."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Saving and loading a trained model\n",
    "\n",
    "Although training time didn't take very long, it's good practice to save your trained models to avoid having to retrain them.\n",
    "\n",
    "Saving your models also enables you to export them for use elsewhere outside of your notebooks, such as in a web application.\n",
    "\n",
    "There are two main ways of [saving a model in TensorFlow](https://www.tensorflow.org/tutorials/keras/save_and_load#save_the_entire_model):\n",
    "1. The `HDF5` format. \n",
    "2. The `SavedModel` format (default).\n",
    "\n",
    "Let's take a look at both."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "model6.save('model6.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you save a model as a `HDF5`, when loading it back in, you need to let [TensorFlow know about any custom objects you've used](https://www.tensorflow.org/tutorials/keras/save_and_load#saving_custom_objects) (e.g. components which aren't built from pure TensorFlow, such as TensorFlow Hub components).\n",
    "\n",
    "h5的格式，加载的时候，需要让tensorflow知道，使用了哪些自定义对象（比如，不是纯tensorflow的组件，如tensorflow hub的组件）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_6_USE\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " USE (KerasLayer)            (None, 512)               256797824 \n",
      "                                                                 \n",
      " dense_5 (Dense)             (None, 64)                32832     \n",
      "                                                                 \n",
      " dense_6 (Dense)             (None, 1)                 65        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 256,830,721\n",
      "Trainable params: 32,897\n",
      "Non-trainable params: 256,797,824\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "load_model6 = tf.keras.models.load_model('model6.h5',custom_objects={'KerasLayer':hub.KerasLayer})\n",
    "load_model6.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "24/24 [==============================] - 0s 4ms/step - loss: 0.4289 - accuracy: 0.8215\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.4289405941963196, 0.8215222954750061]"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "load_model6.evaluate(val_sentences,val_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Function `_wrapped_model` contains input name(s) USE_input with unsupported characters which will be renamed to use_input in the SavedModel.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: model6_SavedModel_format/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: model6_SavedModel_format/assets\n"
     ]
    }
   ],
   "source": [
    "model6.save('model6_SavedModel_format')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you use SavedModel format (default), you can reload your model without specifying custom objects using the [`tensorflow.keras.models.load_model()`](https://www.tensorflow.org/tutorials/keras/save_and_load) function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<keras.engine.sequential.Sequential at 0x7fd771ee0640>"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# savedmodel format 格式，可以直接加载使用\n",
    "load_model6_SavedModel = tf.keras.models.load_model(\"model6_SavedModel_format\")\n",
    "load_model6_SavedModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "24/24 [==============================] - 0s 4ms/step - loss: 0.4289 - accuracy: 0.8215\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.4289405941963196, 0.8215222954750061]"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "load_model6_SavedModel.evaluate(val_sentences,val_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see saving and loading our model with either format results in the same performance.\n",
    "\n",
    "> 🤔 **Question:** Should you used the `SavedModel` format or `HDF5` format?\n",
    "\n",
    "For most use cases, the `SavedModel` format will suffice. However, this is a TensorFlow specific standard. If you need a more general-purpose data standard, `HDF5` might be better. For more, check out the [TensorFlow documentation on saving and loading models](https://www.tensorflow.org/tutorials/keras/save_and_load)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Finding the most wrong examples  错的最离谱的样本\n",
    "\n",
    "We mentioned before that if many of our modelling experiments are returning similar results, despite using different kinds of models, it's a good idea to return to the data and inspect why this might be.\n",
    "\n",
    "One of the best ways to inspect your data is to sort your model's predictions and find the samples it got *most* wrong, meaning, what predictions had a high prediction probability but turned out to be wrong.\n",
    "\n",
    "Once again, visualization is your friend. Visualize, visualize, visualize.\n",
    "\n",
    "To make things visual, let's take our best performing model's prediction probabilities and classes along with the validation samples (text and ground truth labels) and combine them in a pandas DataFrame.\n",
    "\n",
    "* If our best model still isn't perfect, what examples is it getting wrong? \n",
    "* Which ones are the *most* wrong?\n",
    "* Are there some labels which are wrong? E.g. the model gets it right but the ground truth label doesn't reflect this"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>target</th>\n",
       "      <th>pred</th>\n",
       "      <th>pred_prob</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>DFR EP016 Monthly Meltdown - On Dnbheaven 2015...</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.132109</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>FedEx no longer to transport bioterror germs i...</td>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.749391</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Gunmen kill four in El Salvador bus attack: Su...</td>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.995240</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>@camilacabello97 Internally and externally scr...</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.228840</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Radiation emergency #preparedness starts with ...</td>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.654380</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text  target  pred  pred_prob\n",
       "0  DFR EP016 Monthly Meltdown - On Dnbheaven 2015...       0   0.0   0.132109\n",
       "1  FedEx no longer to transport bioterror germs i...       0   1.0   0.749391\n",
       "2  Gunmen kill four in El Salvador bus attack: Su...       1   1.0   0.995240\n",
       "3  @camilacabello97 Internally and externally scr...       1   0.0   0.228840\n",
       "4  Radiation emergency #preparedness starts with ...       1   1.0   0.654380"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val_df = pd.DataFrame({\"text\":val_sentences,\n",
    "                       \"target\":val_labels,\n",
    "                       \"pred\":model6_preds,\n",
    "                       \"pred_prob\":tf.squeeze(model6_pred_probs)\n",
    "                       })\n",
    "val_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>target</th>\n",
       "      <th>pred</th>\n",
       "      <th>pred_prob</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>? High Skies - Burning Buildings ? http://t.co...</td>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.904628</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>628</th>\n",
       "      <td>@noah_anyname That's where the concentration c...</td>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.896122</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>381</th>\n",
       "      <td>Deaths 3 http://t.co/nApviyGKYK</td>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.873395</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>759</th>\n",
       "      <td>FedEx will no longer transport bioterror patho...</td>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.867095</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>109</th>\n",
       "      <td>[55436] 1950 LIONEL TRAINS SMOKE LOCOMOTIVES W...</td>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.865158</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>251</th>\n",
       "      <td>@AshGhebranious civil rights continued in the ...</td>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.842524</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49</th>\n",
       "      <td>@madonnamking RSPCA site multiple 7 story high...</td>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.832282</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>474</th>\n",
       "      <td>My phone looks like it was in a car ship airpl...</td>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.814265</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>209</th>\n",
       "      <td>Ashes 2015: AustraliaÛªs collapse at Trent Br...</td>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.809073</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>698</th>\n",
       "      <td>åÈMGN-AFRICAå¨ pin:263789F4 åÈ Correction: Ten...</td>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.768614</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                  text  target  pred  \\\n",
       "31   ? High Skies - Burning Buildings ? http://t.co...       0   1.0   \n",
       "628  @noah_anyname That's where the concentration c...       0   1.0   \n",
       "381                    Deaths 3 http://t.co/nApviyGKYK       0   1.0   \n",
       "759  FedEx will no longer transport bioterror patho...       0   1.0   \n",
       "109  [55436] 1950 LIONEL TRAINS SMOKE LOCOMOTIVES W...       0   1.0   \n",
       "251  @AshGhebranious civil rights continued in the ...       0   1.0   \n",
       "49   @madonnamking RSPCA site multiple 7 story high...       0   1.0   \n",
       "474  My phone looks like it was in a car ship airpl...       0   1.0   \n",
       "209  Ashes 2015: AustraliaÛªs collapse at Trent Br...       0   1.0   \n",
       "698  åÈMGN-AFRICAå¨ pin:263789F4 åÈ Correction: Ten...       0   1.0   \n",
       "\n",
       "     pred_prob  \n",
       "31    0.904628  \n",
       "628   0.896122  \n",
       "381   0.873395  \n",
       "759   0.867095  \n",
       "109   0.865158  \n",
       "251   0.842524  \n",
       "49    0.832282  \n",
       "474   0.814265  \n",
       "209   0.809073  \n",
       "698   0.768614  "
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "most_wrong = val_df[val_df[\"target\"] != val_df[\"pred\"]].sort_values(\"pred_prob\", ascending=False)\n",
    "most_wrong[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Target: 0, Pred: 1, Prob: 0.9046280384063721\n",
      "Text:\n",
      "? High Skies - Burning Buildings ? http://t.co/uVq41i3Kx2 #nowplaying\n",
      "\n",
      "----\n",
      "\n",
      "Target: 0, Pred: 1, Prob: 0.8961215019226074\n",
      "Text:\n",
      "@noah_anyname That's where the concentration camps and mass murder come in. \n",
      " \n",
      "EVERY. FUCKING. TIME.\n",
      "\n",
      "----\n",
      "\n",
      "Target: 0, Pred: 1, Prob: 0.8733946681022644\n",
      "Text:\n",
      "Deaths 3 http://t.co/nApviyGKYK\n",
      "\n",
      "----\n",
      "\n",
      "Target: 0, Pred: 1, Prob: 0.867094874382019\n",
      "Text:\n",
      "FedEx will no longer transport bioterror pathogens in wake of anthrax lab mishaps http://t.co/lHpgxc4b8J\n",
      "\n",
      "----\n",
      "\n",
      "Target: 0, Pred: 1, Prob: 0.8651583194732666\n",
      "Text:\n",
      "[55436] 1950 LIONEL TRAINS SMOKE LOCOMOTIVES WITH MAGNE-TRACTION INSTRUCTIONS http://t.co/xEZBs3sq0y http://t.co/C2x0QoKGlY\n",
      "\n",
      "----\n",
      "\n",
      "Target: 0, Pred: 1, Prob: 0.8425237536430359\n",
      "Text:\n",
      "@AshGhebranious civil rights continued in the 60s. And what about trans-generational trauma? if anything we should listen to the Americans.\n",
      "\n",
      "----\n",
      "\n",
      "Target: 0, Pred: 1, Prob: 0.8322820067405701\n",
      "Text:\n",
      "@madonnamking RSPCA site multiple 7 story high rise buildings next to low density character residential in an area that floods\n",
      "\n",
      "----\n",
      "\n",
      "Target: 0, Pred: 1, Prob: 0.8142651319503784\n",
      "Text:\n",
      "My phone looks like it was in a car ship airplane accident. Terrible\n",
      "\n",
      "----\n",
      "\n",
      "Target: 0, Pred: 1, Prob: 0.8090731501579285\n",
      "Text:\n",
      "Ashes 2015: AustraliaÛªs collapse at Trent Bridge among worst in history: England bundled out Australia for 60 ... http://t.co/t5TrhjUAU0\n",
      "\n",
      "----\n",
      "\n",
      "Target: 0, Pred: 1, Prob: 0.7686136364936829\n",
      "Text:\n",
      "åÈMGN-AFRICAå¨ pin:263789F4 åÈ Correction: Tent Collapse Story: Correction: Tent Collapse story åÈ http://t.co/fDJUYvZMrv @wizkidayo\n",
      "\n",
      "----\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for row in most_wrong[:10].itertuples(): # loop through the top 10 rows (change the index to view different rows)\n",
    "  _, text, target, pred, prob = row\n",
    "  print(f\"Target: {target}, Pred: {int(pred)}, Prob: {prob}\")\n",
    "  print(f\"Text:\\n{text}\\n\")\n",
    "  print(\"----\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Target: 1, Pred: 0, Prob: 0.05307368189096451\n",
      "Text:\n",
      "VICTORINOX SWISS ARMY DATE WOMEN'S RUBBER MOP WATCH 241487 http://t.co/yFy3nkkcoH http://t.co/KNEhVvOHVK\n",
      "\n",
      "----\n",
      "\n",
      "Target: 1, Pred: 0, Prob: 0.04697379842400551\n",
      "Text:\n",
      "Reddit Will Now QuarantineÛ_ http://t.co/pkUAMXw6pm #onlinecommunities #reddit #amageddon #freespeech #Business http://t.co/PAWvNJ4sAP\n",
      "\n",
      "----\n",
      "\n",
      "Target: 1, Pred: 0, Prob: 0.04614143446087837\n",
      "Text:\n",
      "@SoonerMagic_ I mean I'm a fan but I don't need a girl sounding off like a damn siren\n",
      "\n",
      "----\n",
      "\n",
      "Target: 1, Pred: 0, Prob: 0.04559875652194023\n",
      "Text:\n",
      "New post from @darkreading http://t.co/8eIJDXApnp New SMB Relay Attack Steals User Credentials Over Internet\n",
      "\n",
      "----\n",
      "\n",
      "Target: 1, Pred: 0, Prob: 0.04295254126191139\n",
      "Text:\n",
      "@DavidVonderhaar At least you were sincere ??\n",
      "\n",
      "----\n",
      "\n",
      "Target: 1, Pred: 0, Prob: 0.0395207479596138\n",
      "Text:\n",
      "You can never escape me. Bullets don't harm me. Nothing harms me. But I know pain. I know pain. Sometimes I share it. With someone like you.\n",
      "\n",
      "----\n",
      "\n",
      "Target: 1, Pred: 0, Prob: 0.03900395706295967\n",
      "Text:\n",
      "going to redo my nails and watch behind the scenes of desolation of smaug ayyy\n",
      "\n",
      "----\n",
      "\n",
      "Target: 1, Pred: 0, Prob: 0.03155232593417168\n",
      "Text:\n",
      "I get to smoke my shit in peace\n",
      "\n",
      "----\n",
      "\n",
      "Target: 1, Pred: 0, Prob: 0.028146617114543915\n",
      "Text:\n",
      "Ron &amp; Fez - Dave's High School Crush https://t.co/aN3W16c8F6 via @YouTube\n",
      "\n",
      "----\n",
      "\n",
      "Target: 1, Pred: 0, Prob: 0.020339958369731903\n",
      "Text:\n",
      "Why are you deluged with low self-image? Take the quiz: http://t.co/XsPqdOrIqj http://t.co/CQYvFR4UCy\n",
      "\n",
      "----\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Check the most wrong false negatives (model predicted 0 when should've predict 1)\n",
    "# 预测为0的，但实际为1，错的最离谱的样本\n",
    "for row in most_wrong[-10:].itertuples():\n",
    "  _, text, target, pred, prob = row\n",
    "  print(f\"Target: {target}, Pred: {int(pred)}, Prob: {prob}\")\n",
    "  print(f\"Text:\\n{text}\\n\")\n",
    "  print(\"----\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Making predictions on the test dataset  测试集预测效果\n",
    "\n",
    "Alright we've seen how our model's perform on the validation set.\n",
    "\n",
    "But how about the test dataset?\n",
    "\n",
    "We don't have labels for the test dataset so we're going to have to make some predictions and inspect them for ourselves.\n",
    "\n",
    "Let's write some code to make predictions on random samples from the test dataset and visualize them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 33ms/step\n",
      "Pred: 0, Prob: 0.32714521884918213\n",
      "Text:\n",
      "@lordjewcup @helloimivan wounded warrior\n",
      "\n",
      "----\n",
      "\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "Pred: 1, Prob: 0.9390918016433716\n",
      "Text:\n",
      "Striking views of Super Typhoon Soudelor as it tracks toward Taiwan China http://t.co/hsjp6Hoffe\n",
      "\n",
      "----\n",
      "\n",
      "1/1 [==============================] - 0s 30ms/step\n",
      "Pred: 1, Prob: 0.9940610527992249\n",
      "Text:\n",
      "BBC News - MH370: Reunion debris is from missing Malaysia flight http://t.co/bze47fzKUd\n",
      "\n",
      "----\n",
      "\n",
      "1/1 [==============================] - 0s 29ms/step\n",
      "Pred: 0, Prob: 0.08041217923164368\n",
      "Text:\n",
      "@FollowerOfDole 'Give me your lunch money ner-' *flattened by falling quarter*\n",
      "\n",
      "----\n",
      "\n",
      "1/1 [==============================] - 0s 29ms/step\n",
      "Pred: 0, Prob: 0.02462243102490902\n",
      "Text:\n",
      "Body bagging that I think it's time to bring bags out\n",
      "\n",
      "----\n",
      "\n",
      "1/1 [==============================] - 0s 29ms/step\n",
      "Pred: 1, Prob: 0.7288753986358643\n",
      "Text:\n",
      "@MalikChaimaa I hope Zayn gets blown up in a drone attack whilst visiting family in Pakistan ??\n",
      "\n",
      "----\n",
      "\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "Pred: 0, Prob: 0.10979513078927994\n",
      "Text:\n",
      "Too bloody hot in my room\n",
      "\n",
      "----\n",
      "\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "Pred: 0, Prob: 0.10128657519817352\n",
      "Text:\n",
      "I liked a @YouTube video from @prosyndicate http://t.co/mLvBg6sEka Minecraft: Hunting OpTic - Emergency Exit Plan! (Episode 13)\n",
      "\n",
      "----\n",
      "\n",
      "1/1 [==============================] - 0s 32ms/step\n",
      "Pred: 0, Prob: 0.08577031642198563\n",
      "Text:\n",
      "@Chelsea_Rosette ah bless ya. Not too bad. My life's been a bit of a whirlwind the last two weeks! Got a cold too that won't shift ?? #sexy\n",
      "\n",
      "----\n",
      "\n",
      "1/1 [==============================] - 0s 33ms/step\n",
      "Pred: 0, Prob: 0.4130004644393921\n",
      "Text:\n",
      "Society will collapse by 2040 due to catastrophic food shortages says study http://t.co/2wWZBW5lId\n",
      "\n",
      "----\n",
      "\n"
     ]
    }
   ],
   "source": [
    "test_sentences = test_df[\"text\"].tolist()\n",
    "test_random = random.sample(test_sentences, 10)\n",
    "for sentence in test_random:\n",
    "    pred_prob = tf.squeeze(model6.predict([sentence]))\n",
    "    pred = tf.round(pred_prob)\n",
    "    print(f\"Pred: {int(pred)}, Prob: {pred_prob}\")\n",
    "    print(f\"Text:\\n{sentence}\\n\")\n",
    "    print(\"----\\n\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predicting on Tweets from the wild 预测原生的tweet\n",
    "\n",
    "How about we find some Tweets and use our model to predict whether or not they're about a diaster or not?\n",
    "\n",
    "To start, let's take one of my own [Tweets on living life like an ensemble model](https://twitter.com/mrdbourke/status/1313649328351662082). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "daniels_tweet = \"Life like an ensemble: take the best choices from others and make your own\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_on_sentence(model, sentence):\n",
    "  \"\"\"\n",
    "  Uses model to make a prediction on sentence.\n",
    "\n",
    "  Returns the sentence, the predicted label and the prediction probability.\n",
    "  \"\"\"\n",
    "  pred_prob = model.predict([sentence])\n",
    "  pred_label = tf.squeeze(tf.round(pred_prob)).numpy()\n",
    "  print(f\"Pred: {pred_label}\", \"(real disaster)\" if pred_label > 0 else \"(not real disaster)\", f\"Prob: {pred_prob[0][0]}\")\n",
    "  print(f\"Text:\\n{sentence}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 33ms/step\n",
      "Pred: 0.0 (not real disaster) Prob: 0.055655840784311295\n",
      "Text:\n",
      "Life like an ensemble: take the best choices from others and make your own\n"
     ]
    }
   ],
   "source": [
    "predict_on_sentence(model=model6, sentence=daniels_tweet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Source - https://twitter.com/BeirutCityGuide/status/1290696551376007168\n",
    "beirut_tweet_1 = \"Reports that the smoke in Beirut sky contains nitric acid, which is toxic. Please share and refrain from stepping outside unless urgent. #Lebanon\"\n",
    "\n",
    "# Source - https://twitter.com/BeirutCityGuide/status/1290773498743476224\n",
    "beirut_tweet_2 = \"#Beirut declared a “devastated city”, two-week state of emergency officially declared. #Lebanon\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 31ms/step\n",
      "Pred: 1.0 (real disaster) Prob: 0.9816731214523315\n",
      "Text:\n",
      "Reports that the smoke in Beirut sky contains nitric acid, which is toxic. Please share and refrain from stepping outside unless urgent. #Lebanon\n",
      "1/1 [==============================] - 0s 29ms/step\n",
      "Pred: 1.0 (real disaster) Prob: 0.9849336743354797\n",
      "Text:\n",
      "#Beirut declared a “devastated city”, two-week state of emergency officially declared. #Lebanon\n"
     ]
    }
   ],
   "source": [
    "predict_on_sentence(model=model6, sentence=beirut_tweet_1)\n",
    "predict_on_sentence(model=model6, sentence=beirut_tweet_2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The speed/score tradeoff  速度和分数的权衡\n",
    "\n",
    "One of the final tests we're going to do is to find the speed/score tradeoffs between our best model and baseline model.\n",
    "\n",
    "Why is this important?\n",
    "\n",
    "Although it can be tempting to just choose the best performing model you find through experimentation, this model might not actually work in a production setting.\n",
    "\n",
    "Put it this way, imagine you're Twitter and receive 1 million Tweets per hour (this is a made up number, the actual number is much higher). And you're trying to build a diaster detection system to read Tweets and alert authorities with details about a diaster in close to real-time.\n",
    "\n",
    "Compute power isn't free so you're limited to a single compute machine for the project. On that machine, one of your models makes 10,000 predictions per second at 80% accuracy where as another one of your models (a larger model) makes 100 predictions per second at 85% accuracy.\n",
    "\n",
    "Which model do you choose?\n",
    "\n",
    "Is the second model's performance boost worth missing out on the extra capacity?\n",
    "\n",
    "Of course, there are many options you could try here, such as sending as many Tweets as possible to the first model and then sending the ones which the model is least certain of to the second model. \n",
    "\n",
    "The point here is to illustrate the best model you find through experimentation, might not be the model you end up using in production.\n",
    "\n",
    "To make this more concrete, let's write a function to take a model and a number of samples and time how long the given model takes to make predictions on those samples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "def pred_timer(model, samples):\n",
    "  \"\"\"\n",
    "  Times how long a model takes to make predictions on samples.\n",
    "  \n",
    "  Args:\n",
    "  ----\n",
    "  model = a trained model\n",
    "  sample = a list of samples\n",
    "\n",
    "  Returns:\n",
    "  ----\n",
    "  total_time = total elapsed time for model to make predictions on samples\n",
    "  time_per_pred = time in seconds per single sample\n",
    "  \"\"\"\n",
    "  start_time = time.perf_counter() # get start time\n",
    "  model.predict(samples) # make predictions\n",
    "  end_time = time.perf_counter() # get finish time\n",
    "  total_time = end_time-start_time # calculate how long predictions took to make\n",
    "  time_per_pred = total_time/len(val_sentences) # find prediction time per sample\n",
    "  return total_time, time_per_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "24/24 [==============================] - 0s 3ms/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(0.12266498431563377, 0.00016097766970555612)"
      ]
     },
     "execution_count": 117,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model6_total_pred_time, model6_time_per_pred = pred_timer(model6, val_sentences)\n",
    "model6_total_pred_time, model6_time_per_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.010715180076658726, 1.406191611110069e-05)"
      ]
     },
     "execution_count": 118,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model0_total_pred_time, model0_time_per_pred = pred_timer(model0, val_sentences)\n",
    "model0_total_pred_time, model0_time_per_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x700 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.figure(figsize=(10, 7))\n",
    "plt.scatter(model0_time_per_pred, baseline_results[\"f1\"], label=\"baseline\")\n",
    "plt.scatter(model6_time_per_pred, model6_results[\"f1\"], label=\"tf_hub_sentence_encoder\")\n",
    "plt.legend()\n",
    "plt.title(\"F1-score versus time per prediction\")\n",
    "plt.xlabel(\"Time per prediction\")\n",
    "plt.ylabel(\"F1-Score\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](https://raw.githubusercontent.com/mrdbourke/tensorflow-deep-learning/main/images/08-ideal-performance-speed-of-pred-tradeoff-highlighted.png)\n",
    "*Ideal position for speed and performance tradeoff model (fast predictions with great results).*\n",
    "\n",
    "Of course, the ideal position for each of these dots is to be in the top left of the plot (low time per prediction, high F1-score). \n",
    "\n",
    "In our case, there's a clear tradeoff for time per prediction and performance. Our best performing model takes an order of magnitude longer per prediction but only results in a few F1-score point increase.\n",
    "\n",
    "This kind of tradeoff is something you'll need to keep in mind when incorporating machine learning models into your own applications."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 🛠 Exercises\n",
    "\n",
    "1. Rebuild, compile and train `model_1`, `model_2` and `model_5` using the [Keras Sequential API](https://www.tensorflow.org/api_docs/python/tf/keras/Sequential) instead of the Functional API.\n",
    "2. Retrain the baseline model with 10% of the training data. How does perform compared to the Universal Sentence Encoder model with 10% of the training data?\n",
    "3. Try fine-tuning the TF Hub Universal Sentence Encoder model by setting `training=True` when instantiating it as a Keras layer.\n",
    "\n",
    "```\n",
    "We can use this encoding layer in place of our text_vectorizer and embedding layer\n",
    "\n",
    "sentence_encoder_layer = hub.KerasLayer(\"https://tfhub.dev/google/universal-sentence-encoder/4\",\n",
    "                                        input_shape=[],\n",
    "                                        dtype=tf.string,\n",
    "                                        trainable=True) # turn training on to fine-tune the TensorFlow Hub model\n",
    "```\n",
    "4. Retrain the best model you've got so far on the whole training set (no validation split). Then use this trained model to make predictions on the test dataset and format the predictions into the same format as the `sample_submission.csv` file from Kaggle (see the Files tab in Colab for what the `sample_submission.csv` file looks like). Once you've done this, [make a submission to the Kaggle competition](https://www.kaggle.com/c/nlp-getting-started/data), how did your model perform?\n",
    "5. Combine the ensemble predictions using the majority vote (mode), how does this perform compare to averaging the prediction probabilities of each model?\n",
    "6. Make a confusion matrix with the best performing model's predictions on the validation set and the validation ground truth labels."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 📖 Extra-curriculum \n",
    "\n",
    "To practice what you've learned, a good idea would be to spend an hour on 3 of the following (3-hours total, you could through them all if you want) and then write a blog post about what you've learned.\n",
    "\n",
    "* For an overview of the different problems within NLP and how to solve them read through: \n",
    " * [A Simple Introduction to Natural Language Processing](https://becominghuman.ai/a-simple-introduction-to-natural-language-processing-ea66a1747b32)\n",
    " * [How to solve 90% of NLP problems: a step-by-step guide](https://blog.insightdatascience.com/how-to-solve-90-of-nlp-problems-a-step-by-step-guide-fda605278e4e)\n",
    "* Go through [MIT's Recurrent Neural Networks lecture](https://youtu.be/SEnXr6v2ifU). This will be one of the greatest additions to what's happening behind the RNN model's you've been building.\n",
    "* Read through the [word embeddings page on the TensorFlow website](https://www.tensorflow.org/tutorials/text/word_embeddings). Embeddings are such a large part of NLP. We've covered them throughout this notebook but extra practice would be well worth it. A good exercise would be to write out all the code in the guide in a new notebook. \n",
    "* For more on RNN's in TensorFlow, read and reproduce [the TensorFlow RNN guide](https://www.tensorflow.org/guide/keras/rnn). We've covered many of the concepts in this guide, but it's worth writing the code again for yourself.\n",
    "* Text data doesn't always come in a nice package like the data we've downloaded. So if you're after more on preparing different text sources for being with your TensorFlow deep learning models, it's worth checking out the following:\n",
    " * [TensorFlow text loading tutorial](https://www.tensorflow.org/tutorials/load_data/text).\n",
    "  * [Reading text files with Python](https://realpython.com/read-write-files-python/) by Real Python.\n",
    "* This notebook has focused on writing NLP code. For a mathematically rich overview of how NLP with Deep Learning happens, read [Standford's Natural Language Processing with Deep Learning lecture notes Part 1](https://web.stanford.edu/class/cs224n/readings/cs224n-2019-notes01-wordvecs1.pdf).  \n",
    "  * For an even deeper dive, you could even do the whole [CS224n](http://web.stanford.edu/class/cs224n/) (Natural Language Processing with Deep Learning) course. \n",
    "* Great blog posts to read:\n",
    "  * Andrei Karpathy's [The Unreasonable Effectiveness of RNNs](https://karpathy.github.io/2015/05/21/rnn-effectiveness/) dives into generating Shakespeare text with RNNs.\n",
    "  * [Text Classification with NLP: Tf-Idf vs Word2Vec vs BERT](https://towardsdatascience.com/text-classification-with-nlp-tf-idf-vs-word2vec-vs-bert-41ff868d1794) by Mauro Di Pietro. An overview of different techniques for turning text into numbers and then classifying it.\n",
    "  * [What are word embeddings?](https://machinelearningmastery.com/what-are-word-embeddings/) by Machine Learning Mastery.\n",
    "* Other topics worth looking into:\n",
    "  * [Attention mechanisms](https://jalammar.github.io/visualizing-neural-machine-translation-mechanics-of-seq2seq-models-with-attention/). These are a foundational component of the transformer architecture and also often add improvments to deep NLP models.\n",
    "  * [Transformer architectures](http://jalammar.github.io/illustrated-transformer/). This model architecture has recently taken the NLP world by storm, achieving state of the art on many benchmarks. However, it does take a little more processing to get off the ground, the [HuggingFace Models (formerly HuggingFace Transformers) library](https://huggingface.co/models/) is probably your best quick start.\n",
    "    * And now [HuggingFace even have their own course](https://huggingface.co/course/chapter1) on how their library works! I haven't done it but anything HuggingFace makes is world-class.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torchai",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
